{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MasakhaNER Project: Named Entity Recognition for African Languages\n",
    "\n",
    "## Introduction and Problem Statement\n",
    "\n",
    "This notebook explores Named Entity Recognition (NER) for low-resource African languages using the MasakhaNER dataset. NER is a fundamental NLP task that identifies and classifies named entities in text into predefined categories such as person names, organizations, locations, and dates.\n",
    "\n",
    "The MasakhaNER dataset covers 10 African languages: Amharic, Hausa, Igbo, Kinyarwanda, Luganda, Luo, Nigerian-Pidgin, Swahili, Wolof, and Yorùbá. This project addresses the challenge of developing effective NLP tools for languages with limited digital resources.\n",
    "\n",
    "### Project Goals:\n",
    "1. Explore and analyze the MasakhaNER dataset\n",
    "2. Implement preprocessing pipelines for African languages\n",
    "3. Develop and compare different NER models\n",
    "4. Evaluate model performance across languages\n",
    "5. Identify challenges and opportunities for low-resource NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preparation\n",
    "\n",
    "We'll download the MasakhaNER dataset and load it into our environment. The dataset is structured in CoNLL format, with one token per line and entity tags in BIO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_masakhaner_dataset():\n",
    "    \"\"\"\n",
    "    Download the MasakhaNER dataset from GitHub if not already present\n",
    "    \n",
    "    Returns:\n",
    "        Path to the dataset directory\n",
    "    \"\"\"\n",
    "    # Define the URL and directory\n",
    "    url = \"https://github.com/masakhane-io/masakhane-ner/archive/refs/heads/main.zip\"\n",
    "    data_dir = \"masakhane-ner-data\"\n",
    "    \n",
    "    # Check if directory already exists\n",
    "    if os.path.exists(data_dir):\n",
    "        print(f\"Dataset directory already exists at {data_dir}\")\n",
    "        return data_dir\n",
    "    \n",
    "    # Download and extract the dataset\n",
    "    print(\"Downloading MasakhaNER dataset...\")\n",
    "    response = requests.get(url)\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "        # Extract only the data directory to save space\n",
    "        for file in zip_ref.namelist():\n",
    "            if file.startswith('masakhane-ner-main/data/'):\n",
    "                zip_ref.extract(file, '.')\n",
    "    \n",
    "    # Rename the directory for easier access\n",
    "    os.rename('masakhane-ner-main/data', data_dir)\n",
    "    \n",
    "    # Clean up\n",
    "    os.rmdir('masakhane-ner-main')\n",
    "    \n",
    "    print(f\"Dataset downloaded and extracted to {data_dir}\")\n",
    "    return data_dir\n",
    "\n",
    "def load_masakhaner_data(data_dir, languages=None, sample_size=None):\n",
    "    \"\"\"\n",
    "    Load the MasakhaNER dataset for specified languages.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to the directory containing MasakhaNER data\n",
    "        languages: List of languages to load (if None, load all available)\n",
    "        sample_size: If specified, load only this many examples per language and split\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping language codes to their respective datasets\n",
    "    \"\"\"\n",
    "    if languages is None:\n",
    "        # All 10 languages in MasakhaNER\n",
    "        languages = ['amh', 'hau', 'ibo', 'kin', 'lug', 'luo', 'pcm', 'swa', 'wol', 'yor']\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    for lang in languages:\n",
    "        lang_data = []\n",
    "        \n",
    "        # Load train, dev, and test sets\n",
    "        for split in ['train', 'dev', 'test']:\n",
    "            file_path = os.path.join(data_dir, lang, f\"{split}.txt\")\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Warning: File not found: {file_path}\")\n",
    "                continue\n",
    "                \n",
    "            current_sentence = []\n",
    "            sentences = []\n",
    "            \n",
    "            # Read the CoNLL formatted file\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    \n",
    "                    if line:\n",
    "                        # Parse CoNLL format: word, tag\n",
    "                        parts = line.split()\n",
    "                        if len(parts) >= 2:\n",
    "                            word, tag = parts[0], parts[-1]  # Last column is the NER tag\n",
    "                            current_sentence.append((word, tag))\n",
    "                    elif current_sentence:\n",
    "                        # End of sentence\n",
    "                        sentences.append(current_sentence)\n",
    "                        current_sentence = []\n",
    "            \n",
    "            # Don't forget the last sentence if file doesn't end with an empty line\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "            \n",
    "            # Sample if needed\n",
    "            if sample_size and len(sentences) > sample_size:\n",
    "                sentences = sentences[:sample_size]\n",
    "            \n",
    "            # Convert to DataFrame format\n",
    "            for sentence in sentences:\n",
    "                lang_data.append({\n",
    "                    'tokens': [word for word, _ in sentence],\n",
    "                    'tags': [tag for _, tag in sentence],\n",
    "                    'split': split\n",
    "                })\n",
    "        \n",
    "        datasets[lang] = pd.DataFrame(lang_data)\n",
    "        print(f\"Loaded {len(datasets[lang])} sentences for {lang}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Download and load the dataset\n",
    "data_dir = download_masakhaner_dataset()\n",
    "\n",
    "# For quick testing, let's use a smaller sample. Remove the sample_size parameter for the full dataset.\n",
    "# For final analysis, use all data with: datasets = load_masakhaner_data(data_dir)\n",
    "datasets = load_masakhaner_data(data_dir, sample_size=100)\n",
    "\n",
    "# Display a sample from Swahili dataset\n",
    "print(\"\\nSample from Swahili (swa) dataset:\")\n",
    "sample_row = datasets['swa'].iloc[0]\n",
    "print(f\"Tokens: {sample_row['tokens']}\")\n",
    "print(f\"Tags: {sample_row['tags']}\")\n",
    "print(f\"Split: {sample_row['split']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Now, let's explore the MasakhaNER dataset to understand its characteristics across different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(datasets):\n",
    "    \"\"\"\n",
    "    Perform exploratory analysis on the MasakhaNER dataset\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dictionary mapping language codes to their respective datasets\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of DataFrames with dataset statistics and entity distribution\n",
    "    \"\"\"\n",
    "    # Overall statistics\n",
    "    print(\"Dataset Statistics:\")\n",
    "    stats = {}\n",
    "    \n",
    "    for lang, df in datasets.items():\n",
    "        # Language name mapping for better readability\n",
    "        lang_names = {\n",
    "            'amh': 'Amharic', 'hau': 'Hausa', 'ibo': 'Igbo', \n",
    "            'kin': 'Kinyarwanda', 'lug': 'Luganda', 'luo': 'Luo',\n",
    "            'pcm': 'Nigerian-Pidgin', 'swa': 'Swahili', 'wol': 'Wolof', \n",
    "            'yor': 'Yorùbá'\n",
    "        }\n",
    "        \n",
    "        # Count sentences, tokens, and entities\n",
    "        num_sentences = len(df)\n",
    "        num_tokens = sum(df['tokens'].apply(len))\n",
    "        \n",
    "        # Count entities (non-O tags)\n",
    "        all_tags = [tag for tags in df['tags'] for tag in tags]\n",
    "        entity_tags = [tag for tag in all_tags if tag != 'O']\n",
    "        num_entities = len(entity_tags)\n",
    "        \n",
    "        # Count entity types\n",
    "        entity_types = set([tag.split('-')[1] if '-' in tag else tag for tag in entity_tags if tag != 'O'])\n",
    "        \n",
    "        # Average sentence length\n",
    "        avg_sentence_len = num_tokens / num_sentences if num_sentences > 0 else 0\n",
    "        \n",
    "        # Store statistics\n",
    "        stats[lang_names.get(lang, lang)] = {\n",
    "            'sentences': num_sentences,\n",
    "            'tokens': num_tokens,\n",
    "            'entities': num_entities,\n",
    "            'entity_density': num_entities / num_tokens if num_tokens > 0 else 0,\n",
    "            'entity_types': ', '.join(sorted(entity_types)) if entity_types else 'None',\n",
    "            'avg_sentence_len': avg_sentence_len\n",
    "        }\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats).T\n",
    "    \n",
    "    # Format the columns for better readability\n",
    "    stats_df['entity_density'] = stats_df['entity_density'].map('{:.2%}'.format)\n",
    "    stats_df['avg_sentence_len'] = stats_df['avg_sentence_len'].map('{:.1f}'.format)\n",
    "    \n",
    "    print(stats_df)\n",
    "    \n",
    "    # Visualize entity type distribution\n",
    "    entity_counts = {}\n",
    "    \n",
    "    for lang, df in datasets.items():\n",
    "        lang_name = {'amh': 'Amharic', 'hau': 'Hausa', 'ibo': 'Igbo', \n",
    "                    'kin': 'Kinyarwanda', 'lug': 'Luganda', 'luo': 'Luo',\n",
    "                    'pcm': 'Nigerian-Pidgin', 'swa': 'Swahili', 'wol': 'Wolof', \n",
    "                    'yor': 'Yorùbá'}.get(lang, lang)\n",
    "        \n",
    "        # Get all entity tags and count them by type\n",
    "        all_tag_types = []\n",
    "        for tags in df['tags']:\n",
    "            for tag in tags:\n",
    "                if tag != 'O':  # Skip non-entity tags\n",
    "                    # Extract the entity type (e.g., PER from B-PER)\n",
    "                    entity_type = tag.split('-')[1] if '-' in tag else tag\n",
    "                    all_tag_types.append(entity_type)\n",
    "        \n",
    "        entity_counts[lang_name] = Counter(all_tag_types)\n",
    "    \n",
    "    # Convert to DataFrame for easier plotting\n",
    "    entity_df = pd.DataFrame(entity_counts).fillna(0)\n",
    "    \n",
    "    # Plot entity type distribution\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    entity_df.plot(kind='bar', stacked=True)\n",
    "    plt.title('Entity Type Distribution Across Languages')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Entity Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot entity density by language\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    entity_density = pd.Series({k: float(v['entity_density'].strip('%'))/100 \n",
    "                               for k, v in stats_df['entity_density'].items()})\n",
    "    entity_density.sort_values().plot(kind='bar')\n",
    "    plt.title('Entity Density by Language')\n",
    "    plt.ylabel('Entity Density (entities/token)')\n",
    "    plt.xlabel('Language')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot average sentence length by language\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    avg_sent_len = pd.Series({k: float(v) for k, v in stats_df['avg_sentence_len'].items()})\n",
    "    avg_sent_len.sort_values().plot(kind='bar')\n",
    "    plt.title('Average Sentence Length by Language')\n",
    "    plt.ylabel('Average Tokens per Sentence')\n",
    "    plt.xlabel('Language')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return stats_df, entity_df\n",
    "\n",
    "# Run exploratory data analysis\n",
    "stats_df, entity_df = explore_dataset(datasets)\n",
    "\n",
    "# Additional analysis: Look at tag distribution for a specific language\n",
    "def analyze_entity_distribution(df, lang_name):\n",
    "    \"\"\"Analyze entity distribution for a specific language\"\"\"\n",
    "    all_tags = [tag for tags in df['tags'] for tag in tags]\n",
    "    tag_counts = Counter(all_tags)\n",
    "    \n",
    "    # Create a DataFrame for better display\n",
    "    tag_df = pd.DataFrame({\n",
    "        'Tag': list(tag_counts.keys()),\n",
    "        'Count': list(tag_counts.values())\n",
    "    })\n",
    "    tag_df = tag_df.sort_values('Count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total = tag_df['Count'].sum()\n",
    "    tag_df['Percentage'] = tag_df['Count'] / total * 100\n",
    "    \n",
    "    print(f\"\\nEntity tag distribution for {lang_name}:\")\n",
    "    print(tag_df)\n",
    "    \n",
    "    # Plot the distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = tag_df.head(10).plot(kind='bar', x='Tag', y='Count')\n",
    "    plt.title(f'Top 10 Entity Tags for {lang_name}')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Tag')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return tag_df\n",
    "\n",
    "# Analyze entity distribution for Swahili\n",
    "swahili_tag_df = analyze_entity_distribution(datasets['swa'], 'Swahili')\n",
    "\n",
    "# Analyze token distribution (how many unique tokens)\n",
    "def analyze_token_distribution(datasets):\n",
    "    \"\"\"Analyze token distributions across languages\"\"\"\n",
    "    token_stats = {}\n",
    "    \n",
    "    for lang, df in datasets.items():\n",
    "        lang_name = {'amh': 'Amharic', 'hau': 'Hausa', 'ibo': 'Igbo', \n",
    "                    'kin': 'Kinyarwanda', 'lug': 'Luganda', 'luo': 'Luo',\n",
    "                    'pcm': 'Nigerian-Pidgin', 'swa': 'Swahili', 'wol': 'Wolof', \n",
    "                    'yor': 'Yorùbá'}.get(lang, lang)\n",
    "        \n",
    "        # Get all tokens\n",
    "        all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "        unique_tokens = set(all_tokens)\n",
    "        \n",
    "        token_stats[lang_name] = {\n",
    "            'total_tokens': len(all_tokens),\n",
    "            'unique_tokens': len(unique_tokens),\n",
    "            'vocabulary_ratio': len(unique_tokens) / len(all_tokens) if all_tokens else 0\n",
    "        }\n",
    "    \n",
    "    token_stats_df = pd.DataFrame(token_stats).T\n",
    "    token_stats_df['vocabulary_ratio'] = token_stats_df['vocabulary_ratio'].map('{:.2%}'.format)\n",
    "    \n",
    "    print(\"\\nToken Distribution Statistics:\")\n",
    "    print(token_stats_df)\n",
    "    \n",
    "    # Plot vocabulary ratio by language\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    vocab_ratio = pd.Series({k: float(v.strip('%'))/100 \n",
    "                            for k, v in token_stats_df['vocabulary_ratio'].items()})\n",
    "    vocab_ratio.sort_values().plot(kind='bar')\n",
    "    plt.title('Vocabulary Ratio by Language (unique tokens/total tokens)')\n",
    "    plt.ylabel('Vocabulary Ratio')\n",
    "    plt.xlabel('Language')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return token_stats_df\n",
    "\n",
    "# Analyze token distribution\n",
    "token_stats_df = analyze_token_distribution(datasets)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
