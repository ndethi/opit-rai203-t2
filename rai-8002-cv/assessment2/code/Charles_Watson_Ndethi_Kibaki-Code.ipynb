{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment 2: Metric Learning with Oxford-IIIT Pet Dataset\n",
    "\n",
    "## Introduction and Setup\n",
    "\n",
    "This notebook implements a deep metric learning approach for the Oxford-IIIT Pet Dataset, focusing on learning an embedding space where similar pet breeds are close together and dissimilar ones are far apart. We'll explore different loss functions, evaluate the model on verification, retrieval, and few-shot classification tasks, and visualize the embedding space.\n",
    "\n",
    "### Environment Setup and Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if running in Colab (to install dependencies and set up environment)\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Install required packages\n",
    "if IN_COLAB:\n",
    "    !pip install pytorch-metric-learning\n",
    "    !pip install faiss-gpu\n",
    "    !pip install umap-learn\n",
    "    !pip install matplotlib seaborn scikit-learn tqdm\n",
    "    !pip install gradio\n",
    "    !pip install grad-cam\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import itertools  # For generating pairs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import pytorch_metric_learning\n",
    "from pytorch_metric_learning import losses, miners, distances, reducers, testers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import umap\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Data Loading and Preprocessing\n",
    "\n",
    "In this section, we'll load the Oxford-IIIT Pet Dataset, perform necessary preprocessing, and create appropriate data loaders for our metric learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to load the dataset\n",
    "def load_oxford_pets_dataset(root=\"./data\", download=True):\n",
    "    train_val_dataset = datasets.OxfordIIITPet(\n",
    "        root=root, \n",
    "        split=\"trainval\", \n",
    "        transform=train_transform, \n",
    "        download=download\n",
    "    )\n",
    "    \n",
    "    test_dataset = datasets.OxfordIIITPet(\n",
    "        root=root, \n",
    "        split=\"test\", \n",
    "        transform=eval_transform, \n",
    "        download=download\n",
    "    )\n",
    "    \n",
    "    # For evaluation, create a version of the training set with eval transforms\n",
    "    eval_train_dataset = datasets.OxfordIIITPet(\n",
    "        root=root, \n",
    "        split=\"trainval\", \n",
    "        transform=eval_transform, \n",
    "        download=False\n",
    "    )\n",
    "    \n",
    "    return train_val_dataset, test_dataset, eval_train_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset Preparation for Different Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data for training, validation and few-shot evaluation\n",
    "def prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset, num_holdout_classes=5, val_ratio=0.2):\n",
    "    # Get the class names\n",
    "    class_to_idx = train_val_dataset.class_to_idx\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    num_classes = len(class_to_idx)\n",
    "    \n",
    "    # Split classes for few-shot learning (hold out some classes for testing)\n",
    "    all_class_indices = list(range(num_classes))\n",
    "    holdout_class_indices = random.sample(all_class_indices, num_holdout_classes)\n",
    "    training_class_indices = [i for i in all_class_indices if i not in holdout_class_indices]\n",
    "    \n",
    "    holdout_classes = [idx_to_class[i] for i in holdout_class_indices]\n",
    "    print(f\"Holdout classes for few-shot learning: {holdout_classes}\")\n",
    "    \n",
    "    # Create datasets excluding holdout classes for main training\n",
    "    train_val_indices = [i for i, (_, label) in enumerate(train_val_dataset) if label not in holdout_class_indices]\n",
    "    test_indices = [i for i, (_, label) in enumerate(test_dataset) if label not in holdout_class_indices]\n",
    "    eval_train_indices = [i for i, (_, label) in enumerate(eval_train_dataset) if label not in holdout_class_indices]\n",
    "    \n",
    "    # For few-shot learning, include only holdout classes\n",
    "    few_shot_train_indices = [i for i, (_, label) in enumerate(train_val_dataset) if label in holdout_class_indices]\n",
    "    few_shot_test_indices = [i for i, (_, label) in enumerate(test_dataset) if label in holdout_class_indices]\n",
    "    \n",
    "    # Split train/val\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        train_val_indices, \n",
    "        test_size=val_ratio, \n",
    "        stratify=[train_val_dataset[i][1] for i in train_val_indices],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create Subset datasets\n",
    "    train_dataset = Subset(train_val_dataset, train_indices)\n",
    "    val_dataset = Subset(train_val_dataset, val_indices)\n",
    "    test_filtered_dataset = Subset(test_dataset, test_indices)\n",
    "    eval_train_dataset = Subset(eval_train_dataset, eval_train_indices)\n",
    "    \n",
    "    # Create datasets for few-shot learning\n",
    "    few_shot_train_dataset = Subset(train_val_dataset, few_shot_train_indices)\n",
    "    few_shot_test_dataset = Subset(test_dataset, few_shot_test_indices)\n",
    "    \n",
    "    # Create dictionary for class mapping\n",
    "    class_mapping = {\n",
    "        'class_to_idx': class_to_idx,\n",
    "        'idx_to_class': idx_to_class,\n",
    "        'holdout_class_indices': holdout_class_indices,\n",
    "        'training_class_indices': training_class_indices\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset,\n",
    "        'test': test_filtered_dataset,\n",
    "        'eval_train': eval_train_dataset,\n",
    "        'few_shot_train': few_shot_train_dataset,\n",
    "        'few_shot_test': few_shot_test_dataset,\n",
    "        'class_mapping': class_mapping\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataloaders(datasets_dict, batch_size=32, num_workers=2):\n",
    "    dataloaders = {}\n",
    "    \n",
    "    for key in ['train', 'val', 'test', 'eval_train', 'few_shot_train', 'few_shot_test']:\n",
    "        if key == 'train':\n",
    "            shuffle = True\n",
    "        else:\n",
    "            shuffle = False\n",
    "            \n",
    "        dataloaders[key] = DataLoader(\n",
    "            datasets_dict[key],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    return dataloaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "train_val_dataset, test_dataset, eval_train_dataset = load_oxford_pets_dataset()\n",
    "print(f\"Train+Val size: {len(train_val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")\n",
    "\n",
    "# Prepare datasets for different tasks\n",
    "datasets_dict = prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32  # Adjust based on your GPU/memory constraints\n",
    "dataloaders = create_dataloaders(datasets_dict, batch_size=batch_size)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "for key, dataloader in dataloaders.items():\n",
    "    print(f\"{key}: {len(dataloader.dataset)} samples\")\n",
    "\n",
    "class_mapping = datasets_dict['class_mapping']\n",
    "num_classes = len(class_mapping['class_to_idx'])\n",
    "print(f\"Total number of classes: {num_classes}\")\n",
    "print(f\"Number of training classes: {len(class_mapping['training_class_indices'])}\")\n",
    "print(f\"Number of few-shot classes: {len(class_mapping['holdout_class_indices'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "In this section, we'll define our metric learning model architecture using a CNN backbone and a projection head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet18', embedding_size=128, pretrained=True):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        \n",
    "        # Get backbone and its output size\n",
    "        self.backbone, backbone_output_size = self._get_backbone(backbone_name, pretrained)\n",
    "        \n",
    "        # Projection head (MLP)\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(backbone_output_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, embedding_size)\n",
    "        )\n",
    "        \n",
    "    def _get_backbone(self, backbone_name, pretrained):\n",
    "        \"\"\"\n",
    "        Create a backbone network from various architectures\n",
    "        \"\"\"\n",
    "        if backbone_name == 'resnet18':\n",
    "            backbone = models.resnet18(pretrained=pretrained)\n",
    "            output_size = 512\n",
    "        elif backbone_name == 'resnet34':\n",
    "            backbone = models.resnet34(pretrained=pretrained)\n",
    "            output_size = 512\n",
    "        elif backbone_name == 'resnet50':\n",
    "            backbone = models.resnet50(pretrained=pretrained)\n",
    "            output_size = 2048\n",
    "        elif backbone_name == 'efficientnet_b0':\n",
    "            backbone = models.efficientnet_b0(pretrained=pretrained)\n",
    "            output_size = 1280\n",
    "        elif backbone_name == 'mobilenet_v2':\n",
    "            backbone = models.mobilenet_v2(pretrained=pretrained)\n",
    "            output_size = 1280\n",
    "        elif backbone_name == 'densenet121':\n",
    "            backbone = models.densenet121(pretrained=pretrained)\n",
    "            output_size = 1024\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
    "        \n",
    "        # For ResNet models\n",
    "        if backbone_name.startswith('resnet'):\n",
    "            # Remove the classification layer\n",
    "            backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        # For EfficientNet\n",
    "        elif backbone_name.startswith('efficientnet'):\n",
    "            backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        # For MobileNet\n",
    "        elif backbone_name.startswith('mobilenet'):\n",
    "            backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        # For DenseNet\n",
    "        elif backbone_name.startswith('densenet'):\n",
    "            backbone = nn.Sequential(\n",
    "                backbone.features,\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AdaptiveAvgPool2d((1, 1))\n",
    "            )\n",
    "        \n",
    "        return backbone, output_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        embeddings = self.projection_head(features)\n",
    "        \n",
    "        # Normalize embeddings to unit length (important for cosine distance)\n",
    "        normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return normalized_embeddings\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loss Function Implementation\n",
    "\n",
    "Here we'll implement several loss functions for metric learning including Triplet Loss, Contrastive Loss, and ArcFace. We'll also implement miners for efficient training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_loss_and_miner(loss_type, margin=0.2, embedding_size=128, num_classes=32):\n",
    "    \"\"\"\n",
    "    Create loss function and miner for metric learning\n",
    "    \"\"\"\n",
    "    if loss_type == 'triplet':\n",
    "        # Triplet loss with cosine distance\n",
    "        distance = distances.CosineSimilarity()\n",
    "        reducer = reducers.ThresholdReducer(low=0)\n",
    "        loss_func = losses.TripletMarginLoss(margin=margin, distance=distance, reducer=reducer)\n",
    "        mining_func = miners.TripletMarginMiner(margin=margin, distance=distance, type_of_triplets=\"semihard\")\n",
    "        \n",
    "    elif loss_type == 'contrastive':\n",
    "        # Contrastive loss\n",
    "        distance = distances.CosineSimilarity()\n",
    "        loss_func = losses.ContrastiveLoss(pos_margin=0.8, neg_margin=0.2, distance=distance)\n",
    "        mining_func = miners.PairMarginMiner(pos_margin=0.8, neg_margin=0.2, distance=distance)\n",
    "        \n",
    "    elif loss_type == 'arcface':\n",
    "        # ArcFace loss\n",
    "        loss_func = losses.ArcFaceLoss(embedding_size, num_classes, margin=28.6, scale=64)\n",
    "        mining_func = None\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss type: {loss_type}\")\n",
    "        \n",
    "    return loss_func, mining_func\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Hard Negative Mining (Bonus Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HardNegativePairMiner(miners.BaseMiner):\n",
    "    def __init__(self, distance, neg_margin=0.2, hardest_fraction=0.5):\n",
    "        super().__init__()\n",
    "        self.distance = distance\n",
    "        self.neg_margin = neg_margin\n",
    "        self.hardest_fraction = hardest_fraction\n",
    "        \n",
    "    def mine(self, embeddings, labels, ref_emb=None, ref_labels=None):\n",
    "        ref_emb, ref_labels = embeddings, labels\n",
    "        dist_mat = self.distance(embeddings, ref_emb)\n",
    "        \n",
    "        # Get negative pairs (different classes)\n",
    "        negative_mask = labels.unsqueeze(1) != ref_labels.unsqueeze(0)\n",
    "        \n",
    "        # For each anchor, find all negative pairs\n",
    "        anchors, negatives = torch.where(negative_mask)\n",
    "        \n",
    "        if len(anchors) == 0:\n",
    "            return empty_tensor(0), empty_tensor(0), empty_tensor(0), empty_tensor(0)\n",
    "        \n",
    "        # Get distances for all negative pairs\n",
    "        distances = dist_mat[anchors, negatives]\n",
    "        \n",
    "        # Group by anchor\n",
    "        anchor_groups = defaultdict(list)\n",
    "        for i in range(len(anchors)):\n",
    "            anchor_groups[anchors[i].item()].append((negatives[i].item(), distances[i].item()))\n",
    "        \n",
    "        # For each anchor, select the hardest negatives\n",
    "        hard_a, hard_n = [], []\n",
    "        for anchor, neg_dists in anchor_groups.items():\n",
    "            # Sort negatives by distance (ascending for hardest cosine similarity)\n",
    "            neg_dists.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Select hardest fraction\n",
    "            num_to_select = max(1, int(len(neg_dists) * self.hardest_fraction))\n",
    "            selected_negs = neg_dists[:num_to_select]\n",
    "            \n",
    "            for neg, dist in selected_negs:\n",
    "                hard_a.append(anchor)\n",
    "                hard_n.append(neg)\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(hard_a, device=embeddings.device), \n",
    "            empty_tensor(0), \n",
    "            empty_tensor(0), \n",
    "            torch.tensor(hard_n, device=embeddings.device)\n",
    "        )\n",
    "        \n",
    "def empty_tensor(size):\n",
    "    return torch.tensor([], device=device, dtype=torch.long).view(size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Hyperparameter Tuning Framework\n",
    "\n",
    "This section implements a simple hyperparameter tuning framework to find the best combination of parameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add a simple hyperparameter tuning framework\n",
    "def hyperparameter_tuning(param_grid, n_trials=5, evaluation_metric='combined'):\n",
    "    \"\"\"\n",
    "    Simple hyperparameter tuning framework\n",
    "    \n",
    "    Args:\n",
    "        param_grid (dict): Dictionary where keys are parameter names and values are lists of parameter values\n",
    "        n_trials (int): Number of random combinations to try\n",
    "        evaluation_metric (str): Which metric to use for choosing the best model ('roc_auc', 'recall', 'precision', 'combined')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Best parameters and the corresponding performance\n",
    "    \"\"\"\n",
    "    print(f\"Starting hyperparameter tuning with {n_trials} trials\")\n",
    "    print(f\"Parameter grid: {param_grid}\")\n",
    "    \n",
    "    # Load data (only once)\n",
    "    train_val_dataset, test_dataset, eval_train_dataset = load_oxford_pets_dataset()\n",
    "    datasets_dict = prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset)\n",
    "    \n",
    "    # Track results\n",
    "    results = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Sample random combination of hyperparameters\n",
    "        params = {}\n",
    "        for param_name, param_values in param_grid.items():\n",
    "            params[param_name] = random.choice(param_values)\n",
    "        \n",
    "        print(f\"\\n\\nTrial {trial+1}/{n_trials}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "        \n",
    "        # Create dataloaders\n",
    "        dataloaders = create_dataloaders(\n",
    "            datasets_dict, \n",
    "            batch_size=params.get('batch_size', 32),\n",
    "            num_workers=params.get('num_workers', 2)\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        model = EmbeddingNet(\n",
    "            backbone_name=params.get('backbone_name', 'resnet18'),\n",
    "            embedding_size=params.get('embedding_size', 128),\n",
    "            pretrained=True\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Create optimizer\n",
    "        if params.get('optimizer_name', 'adam') == 'adam':\n",
    "            optimizer = optim.Adam(\n",
    "                model.parameters(), \n",
    "                lr=params.get('learning_rate', 1e-4),\n",
    "                weight_decay=params.get('weight_decay', 0)\n",
    "            )\n",
    "        elif params.get('optimizer_name', 'adam') == 'sgd':\n",
    "            optimizer = optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=params.get('learning_rate', 1e-3),\n",
    "                momentum=params.get('momentum', 0.9),\n",
    "                weight_decay=params.get('weight_decay', 0)\n",
    "            )\n",
    "        \n",
    "        # Create scheduler\n",
    "        scheduler = None\n",
    "        if params.get('use_scheduler', True):\n",
    "            if params.get('scheduler_name', 'step') == 'step':\n",
    "                scheduler = optim.lr_scheduler.StepLR(\n",
    "                    optimizer, \n",
    "                    step_size=params.get('scheduler_step_size', 5), \n",
    "                    gamma=params.get('scheduler_gamma', 0.5)\n",
    "                )\n",
    "            elif params.get('scheduler_name', 'step') == 'cosine':\n",
    "                scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                    optimizer,\n",
    "                    T_max=params.get('num_epochs', 10),\n",
    "                    eta_min=params.get('min_lr', 1e-6)\n",
    "                )\n",
    "        \n",
    "        # Train model with fewer epochs for hyperparameter tuning\n",
    "        num_epochs = params.get('num_epochs', 10)\n",
    "        loss_type = params.get('loss_type', 'triplet')\n",
    "        embedding_size = params.get('embedding_size', 128)\n",
    "        \n",
    "        try:\n",
    "            # Train with minimal output\n",
    "            model, history = train_model(\n",
    "                model=model,\n",
    "                dataloaders=dataloaders,\n",
    "                loss_type=loss_type,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                num_epochs=num_epochs,\n",
    "                embedding_size=embedding_size\n",
    "            )\n",
    "            \n",
    "            # Simple evaluation to get a performance metric\n",
    "            train_embeddings, train_labels = extract_embeddings(model, dataloaders['eval_train'])\n",
    "            test_embeddings, test_labels = extract_embeddings(model, dataloaders['test'])\n",
    "            \n",
    "            # Verification Task\n",
    "            verification_results = evaluate_verification(test_embeddings, test_labels)\n",
    "            roc_auc = verification_results['roc_auc']\n",
    "            \n",
    "            # Retrieval Task\n",
    "            retrieval_results = evaluate_retrieval(\n",
    "                query_embeddings=test_embeddings,\n",
    "                query_labels=test_labels,\n",
    "                gallery_embeddings=train_embeddings,\n",
    "                gallery_labels=train_labels,\n",
    "                k_values=[1, 5]\n",
    "            )\n",
    "            recall_at_1 = retrieval_results['recall@1']\n",
    "            precision_at_1 = retrieval_results['precision@1']\n",
    "            \n",
    "            # Calculate overall score based on the chosen evaluation metric\n",
    "            if evaluation_metric == 'roc_auc':\n",
    "                score = roc_auc\n",
    "            elif evaluation_metric == 'recall':\n",
    "                score = recall_at_1\n",
    "            elif evaluation_metric == 'precision':\n",
    "                score = precision_at_1\n",
    "            else:  # combined\n",
    "                score = 0.4 * roc_auc + 0.4 * recall_at_1 + 0.2 * precision_at_1\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'params': params,\n",
    "                'roc_auc': roc_auc,\n",
    "                'recall@1': recall_at_1,\n",
    "                'precision@1': precision_at_1,\n",
    "                'score': score,\n",
    "                'val_loss': history['val_loss'][-1]\n",
    "            })\n",
    "            \n",
    "            print(f\"Trial {trial+1} results: ROC AUC = {roc_auc:.4f}, Recall@1 = {recall_at_1:.4f}, Score = {score:.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in trial {trial+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No successful trials!\")\n",
    "        return None\n",
    "        \n",
    "    # Find best parameters\n",
    "    results.sort(key=lambda x: x['score'], reverse=True)\n",
    "    best_result = results[0]\n",
    "    \n",
    "    print(\"\\n\\n============= Hyperparameter Tuning Results ==============\")\n",
    "    print(f\"Best score: {best_result['score']:.4f}\")\n",
    "    print(f\"Best parameters: {best_result['params']}\")\n",
    "    print(f\"Performance: ROC AUC = {best_result['roc_auc']:.4f}, Recall@1 = {best_result['recall@1']:.4f}\")\n",
    "    print(\"===========================================================\")\n",
    "    \n",
    "    # Plot validation loss curves for all trials\n",
    "    if len(results) > 1:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            if 'val_loss' in result and len(result['val_loss']) > 0:\n",
    "                plt.plot(range(1, len(result['val_loss'])+1), result['val_loss'], label=f\"Trial {i+1} (score={result['score']:.4f})\")\n",
    "        \n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Validation Loss')\n",
    "        plt.title('Hyperparameter Tuning - Validation Loss Curves')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig('hyperparameter_tuning_results.png')\n",
    "        plt.show()\n",
    "    \n",
    "    return best_result\n",
    "\n",
    "# Example usage of the hyperparameter tuning framework\n",
    "def run_hyperparameter_search():\n",
    "    param_grid = {\n",
    "        'backbone_name': ['resnet18', 'resnet34', 'efficientnet_b0', 'mobilenet_v2'],\n",
    "        'embedding_size': [64, 128, 256],\n",
    "        'batch_size': [16, 32, 64],\n",
    "        'loss_type': ['triplet', 'contrastive', 'arcface'],\n",
    "        'learning_rate': [1e-4, 5e-4, 1e-3],\n",
    "        'optimizer_name': ['adam', 'sgd'],\n",
    "        'weight_decay': [0, 1e-5, 1e-4],\n",
    "        'scheduler_name': ['step', 'cosine'],\n",
    "        'num_epochs': [10]  # Keep this fixed for faster tuning\n",
    "    }\n",
    "    \n",
    "    # Run hyperparameter tuning\n",
    "    best_params = hyperparameter_tuning(param_grid, n_trials=5, evaluation_metric='combined')\n",
    "    return best_params\n",
    "\n",
    "# For a more focused search with fewer parameters\n",
    "def focused_hyperparameter_search():\n",
    "    param_grid = {\n",
    "        'backbone_name': ['resnet18'],\n",
    "        'embedding_size': [128, 256],\n",
    "        'batch_size': [32],\n",
    "        'loss_type': ['triplet', 'contrastive'],\n",
    "        'learning_rate': [1e-4, 5e-4],\n",
    "        'optimizer_name': ['adam'],\n",
    "        'scheduler_name': ['step'],\n",
    "        'num_epochs': [10]\n",
    "    }\n",
    "    \n",
    "    # Run focused hyperparameter tuning\n",
    "    best_params = hyperparameter_tuning(param_grid, n_trials=4, evaluation_metric='combined')\n",
    "    return best_params\n",
    "\n",
    "# Uncomment to run hyperparameter tuning\n",
    "# best_params = run_hyperparameter_search()\n",
    "# best_params = focused_hyperparameter_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Main Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_few_shot(support_embeddings, support_labels, query_embeddings, query_labels, n_way=5, k_shot=5, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate the model on n-way k-shot classification with confidence intervals\n",
    "    \n",
    "    Args:\n",
    "        support_embeddings: Embeddings for the support set (training examples)\n",
    "        support_labels: Labels for the support set\n",
    "        query_embeddings: Embeddings for the query set (test examples)\n",
    "        query_labels: Labels for the query set\n",
    "        n_way: Number of classes in each episode\n",
    "        k_shot: Number of examples per class in the support set\n",
    "        num_episodes: Number of episodes to run for stable results\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing accuracy statistics and confidence intervals\n",
    "    \"\"\"\n",
    "    unique_labels = torch.unique(support_labels)\n",
    "    if len(unique_labels) < n_way:\n",
    "        print(f\"Warning: Only {len(unique_labels)} classes available, but n_way={n_way}\")\n",
    "        n_way = len(unique_labels)\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    # Progress bar for episodes\n",
    "    progress_bar = tqdm(range(num_episodes), desc=f\"{n_way}-way {k_shot}-shot evaluation\")\n",
    "    \n",
    "    for episode in progress_bar:\n",
    "        # Randomly select n classes for this episode\n",
    "        selected_classes = np.random.choice(unique_labels.numpy(), n_way, replace=False)\n",
    "        \n",
    "        # Create support set (k examples per class)\n",
    "        support_set_embeddings = []\n",
    "        support_set_labels = []\n",
    "        \n",
    "        for class_idx, c in enumerate(selected_classes):\n",
    "            # Get indices of examples of class c\n",
    "            class_indices = torch.where(support_labels == c)[0]\n",
    "            \n",
    "            # Randomly select k examples\n",
    "            if len(class_indices) >= k_shot:\n",
    "                selected_indices = np.random.choice(class_indices.numpy(), k_shot, replace=False)\n",
    "            else:\n",
    "                # If not enough examples, use all and repeat some\n",
    "                selected_indices = np.random.choice(class_indices.numpy(), k_shot, replace=True)\n",
    "            \n",
    "            for idx in selected_indices:\n",
    "                support_set_embeddings.append(support_embeddings[idx])\n",
    "                support_set_labels.append(class_idx)  # Use class index as the new label\n",
    "        \n",
    "        support_set_embeddings = torch.stack(support_set_embeddings)\n",
    "        support_set_labels = torch.tensor(support_set_labels)\n",
    "        \n",
    "        # Create query set (all examples of the selected classes from the query set)\n",
    "        query_set_indices = torch.tensor([i for i, label in enumerate(query_labels) if label in selected_classes])\n",
    "        \n",
    "        if len(query_set_indices) == 0:\n",
    "            print(\"Warning: No query examples for selected classes\")\n",
    "            continue\n",
    "            \n",
    "        query_set_embeddings = query_embeddings[query_set_indices]\n",
    "        query_set_labels = query_labels[query_set_indices]\n",
    "        \n",
    "        # Map original labels to new indices (0 to n_way-1)\n",
    "        label_mapping = {selected_classes[i]: i for i in range(n_way)}\n",
    "        query_set_labels = torch.tensor([label_mapping[label.item()] for label in query_set_labels])\n",
    "        \n",
    "        # Compute prototypes (mean embedding for each class)\n",
    "        prototypes = torch.zeros(n_way, support_embeddings.size(1), device=support_embeddings.device)\n",
    "        for c in range(n_way):\n",
    "            prototypes[c] = support_set_embeddings[support_set_labels == c].mean(0)\n",
    "        \n",
    "        # Compute distances between query examples and prototypes\n",
    "        # Using cosine similarity (higher means more similar)\n",
    "        logits = torch.matmul(query_set_embeddings, prototypes.T)\n",
    "        \n",
    "        # Make predictions\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        accuracy = (predictions == query_set_labels).float().mean().item()\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        # Update progress bar with current mean accuracy\n",
    "        progress_bar.set_postfix({'mean_acc': np.mean(accuracies):.4f})\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    \n",
    "    # Calculate 95% confidence interval using bootstrap\n",
    "    bootstrap_samples = 1000\n",
    "    bootstrap_means = []\n",
    "    \n",
    "    for _ in range(bootstrap_samples):\n",
    "        # Sample with replacement from the accuracies\n",
    "        bootstrap_sample = np.random.choice(accuracies, size=len(accuracies), replace=True)\n",
    "        bootstrap_means.append(np.mean(bootstrap_sample))\n",
    "    \n",
    "    # Calculate 95% confidence interval\n",
    "    conf_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "    \n",
    "    print(f\"{n_way}-way {k_shot}-shot classification:\")\n",
    "    print(f\"  Mean accuracy: {mean_accuracy:.4f}\")\n",
    "    print(f\"  Standard deviation: {std_accuracy:.4f}\")\n",
    "    print(f\"  95% confidence interval: [{conf_interval[0]:.4f}, {conf_interval[1]:.4f}]\")\n",
    "    \n",
    "    return {\n",
    "        'mean_accuracy': mean_accuracy,\n",
    "        'std_accuracy': std_accuracy,\n",
    "        'conf_interval': conf_interval,\n",
    "        'accuracies': accuracies\n",
    "    }\n",
    "\n",
    "def evaluate_multiple_few_shot_settings(support_embeddings, support_labels, query_embeddings, query_labels):\n",
    "    \"\"\"\n",
    "    Evaluate few-shot learning across multiple n-way, k-shot configurations\n",
    "    and visualize the results with error bars\n",
    "    \"\"\"\n",
    "    # Define the settings to evaluate\n",
    "    settings = [\n",
    "        {'n_way': 5, 'k_shot': 1, 'num_episodes': 100},  # 5-way 1-shot\n",
    "        {'n_way': 5, 'k_shot': 5, 'num_episodes': 100},  # 5-way 5-shot\n",
    "        {'n_way': 10, 'k_shot': 1, 'num_episodes': 100}, # 10-way 1-shot\n",
    "        {'n_way': 10, 'k_shot': 5, 'num_episodes': 100}  # 10-way 5-shot\n",
    "    ]\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Run evaluation for each setting\n",
    "    print(\"\\n=== Few-Shot Learning Evaluation ===\\n\")\n",
    "    for setting in settings:\n",
    "        print(f\"\\nEvaluating {setting['n_way']}-way {setting['k_shot']}-shot learning...\")\n",
    "        result = evaluate_few_shot(\n",
    "            support_embeddings=support_embeddings,\n",
    "            support_labels=support_labels,\n",
    "            query_embeddings=query_embeddings,\n",
    "            query_labels=query_labels,\n",
    "            n_way=setting['n_way'],\n",
    "            k_shot=setting['k_shot'],\n",
    "            num_episodes=setting['num_episodes']\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'n_way': setting['n_way'],\n",
    "            'k_shot': setting['k_shot'],\n",
    "            'mean_accuracy': result['mean_accuracy'],\n",
    "            'std_accuracy': result['std_accuracy'],\n",
    "            'conf_interval': result['conf_interval']\n",
    "        })\n",
    "    \n",
    "    # Create a visualization of the results with error bars\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Group by n_way for bar chart\n",
    "    n_way_values = sorted(list(set([r['n_way'] for r in results])))\n",
    "    k_shot_values = sorted(list(set([r['k_shot'] for r in results])))\n",
    "    \n",
    "    # Set up bar positions\n",
    "    bar_width = 0.35\n",
    "    x = np.arange(len(n_way_values))\n",
    "    \n",
    "    # Plot bars for each k_shot value\n",
    "    for i, k in enumerate(k_shot_values):\n",
    "        means = []\n",
    "        errors_lower = []\n",
    "        errors_upper = []\n",
    "        \n",
    "        for n in n_way_values:\n",
    "            # Find result for this n_way and k_shot\n",
    "            result = next((r for r in results if r['n_way'] == n and r['k_shot'] == k), None)\n",
    "            if result:\n",
    "                means.append(result['mean_accuracy'])\n",
    "                errors_lower.append(result['mean_accuracy'] - result['conf_interval'][0])\n",
    "                errors_upper.append(result['conf_interval'][1] - result['mean_accuracy'])\n",
    "            else:\n",
    "                means.append(0)\n",
    "                errors_lower.append(0)\n",
    "                errors_upper.append(0)\n",
    "        \n",
    "        # Plot bars with error bars\n",
    "        plt.bar(\n",
    "            x + (i - 0.5*(len(k_shot_values)-1)) * bar_width, \n",
    "            means, \n",
    "            bar_width, \n",
    "            yerr=[errors_lower, errors_upper],\n",
    "            label=f'{k}-shot',\n",
    "            capsize=5\n",
    "        )\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('Number of Classes (N-way)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Few-Shot Learning Performance')\n",
    "    plt.xticks(x, [f'{n}-way' for n in n_way_values])\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, result in enumerate(results):\n",
    "        n_way_idx = n_way_values.index(result['n_way'])\n",
    "        k_shot_idx = k_shot_values.index(result['k_shot'])\n",
    "        x_pos = n_way_idx + (k_shot_idx - 0.5*(len(k_shot_values)-1)) * bar_width\n",
    "        y_pos = result['mean_accuracy'] + 0.02\n",
    "        plt.text(x_pos, y_pos, f\"{result['mean_accuracy']:.3f}\", \n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('few_shot_evaluation.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(run_comprehensive_eval=True):\n",
    "    # Uncomment to find the best parameters using hyperparameter tuning\n",
    "    # best_params = focused_hyperparameter_search()\n",
    "    # if best_params:\n",
    "    #     backbone_name = best_params['params'].get('backbone_name', 'resnet18')\n",
    "    #     embedding_size = best_params['params'].get('embedding_size', 128)\n",
    "    #     batch_size = best_params['params'].get('batch_size', 32)\n",
    "    #     loss_type = best_params['params'].get('loss_type', 'triplet')\n",
    "    #     lr = best_params['params'].get('learning_rate', 1e-4)\n",
    "    #     print(f\"Using best parameters from hyperparameter tuning: {best_params['params']}\")\n",
    "    # else:\n",
    "    #     # Use default parameters if hyperparameter tuning failed\n",
    "    #     backbone_name = 'resnet18'\n",
    "    #     embedding_size = 128\n",
    "    #     batch_size = 32\n",
    "    #     loss_type = 'triplet'\n",
    "    #     lr = 1e-4\n",
    "    \n",
    "    # Model parameters (defaults)\n",
    "    backbone_name = 'resnet18'  # Options: 'resnet18', 'resnet34', 'resnet50', 'efficientnet_b0', 'mobilenet_v2', 'densenet121'\n",
    "    embedding_size = 128\n",
    "    batch_size = 32  # Adjust based on your GPU\n",
    "    num_workers = 2\n",
    "    \n",
    "    # Training parameters\n",
    "    loss_type = 'triplet'  # Options: 'triplet', 'contrastive', 'arcface'\n",
    "    num_epochs = 20\n",
    "    lr = 1e-4\n",
    "\n",
    "    # Load data\n",
    "    train_val_dataset, test_dataset, eval_train_dataset = load_oxford_pets_dataset()\n",
    "    datasets_dict = prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset)\n",
    "    dataloaders = create_dataloaders(datasets_dict, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    # Create model\n",
    "    model = EmbeddingNet(backbone_name=backbone_name, embedding_size=embedding_size)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create optimizer and scheduler\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    # Train model\n",
    "    model, history = train_model(\n",
    "        model=model,\n",
    "        dataloaders=dataloaders,\n",
    "        loss_type=loss_type,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=num_epochs,\n",
    "        embedding_size=embedding_size\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs+1), history['train_loss'], 'b-', label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs+1), history['val_loss'], 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training and Validation Loss ({loss_type} loss)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'embedding_size': embedding_size,\n",
    "        'backbone_name': backbone_name,\n",
    "        'class_mapping': datasets_dict['class_mapping']\n",
    "    }, f'pet_metric_learning_{backbone_name}_{loss_type}.pth')\n",
    "    \n",
    "    if run_comprehensive_eval:\n",
    "        # Run comprehensive evaluation\n",
    "        print(\"\\nRunning comprehensive model evaluation...\")\n",
    "        results_dir = f\"./evaluation_results_{backbone_name}_{loss_type}\"\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        evaluation_results, eval_dir = comprehensive_evaluation(\n",
    "            model=model,\n",
    "            dataloaders=dataloaders,\n",
    "            class_mapping=datasets_dict['class_mapping'],\n",
    "            results_dir=results_dir\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nComprehensive evaluation completed. Results saved to {eval_dir}\")\n",
    "    else:\n",
    "        # Extract embeddings for standard evaluation\n",
    "        print(\"\\nExtracting embeddings for evaluation...\")\n",
    "        train_embeddings, train_labels = extract_embeddings(model, dataloaders['eval_train'])\n",
    "        test_embeddings, test_labels = extract_embeddings(model, dataloaders['test'])\n",
    "        few_shot_train_embeddings, few_shot_train_labels = extract_embeddings(model, dataloaders['few_shot_train'])\n",
    "        few_shot_test_embeddings, few_shot_test_labels = extract_embeddings(model, dataloaders['few_shot_test'])\n",
    "        \n",
    "        # Evaluation tasks\n",
    "        print(\"\\n1. Verification Task:\")\n",
    "        verification_results = evaluate_verification(test_embeddings, test_labels)\n",
    "        \n",
    "        print(f\"\\nVerification Results:\")\n",
    "        print(f\"ROC AUC: {verification_results['roc_auc']:.4f}\")\n",
    "        print(f\"Equal Error Rate (EER): {verification_results['eer']:.4f}\")\n",
    "        \n",
    "        print(\"\\n2. Retrieval Task:\")\n",
    "        retrieval_results = evaluate_retrieval(\n",
    "            query_embeddings=test_embeddings,\n",
    "            query_labels=test_labels,\n",
    "            gallery_embeddings=train_embeddings,\n",
    "            gallery_labels=train_labels,\n",
    "            k_values=[1, 5, 10]\n",
    "        )\n",
    "        \n",
    "        print(\"\\n3. Few-shot Classification:\")\n",
    "        # Run comprehensive few-shot evaluation across multiple settings\n",
    "        few_shot_results = evaluate_multiple_few_shot_settings(\n",
    "            support_embeddings=few_shot_train_embeddings,\n",
    "            support_labels=few_shot_train_labels,\n",
    "            query_embeddings=few_shot_test_embeddings,\n",
    "            query_labels=few_shot_test_labels\n",
    "        )\n",
    "        \n",
    "        # Embedding visualization\n",
    "        print(\"\\n4. Embedding Visualization:\")\n",
    "        test_projection = visualize_embeddings(\n",
    "            embeddings=test_embeddings,\n",
    "            labels=test_labels,\n",
    "            class_mapping=datasets_dict['class_mapping'],\n",
    "            method='tsne',\n",
    "            title='t-SNE Visualization of Test Embeddings'\n",
    "        )\n",
    "        \n",
    "        # Visualize few-shot embeddings\n",
    "        print(\"\\nVisualizing few-shot embeddings:\")\n",
    "        # Combine few-shot train and test embeddings for visualization\n",
    "        all_few_shot_embeddings = torch.cat([few_shot_train_embeddings, few_shot_test_embeddings], dim=0)\n",
    "        all_few_shot_labels = torch.cat([few_shot_train_labels, few_shot_test_labels], dim=0)\n",
    "        \n",
    "        few_shot_projection = visualize_embeddings(\n",
    "            embeddings=all_few_shot_embeddings,\n",
    "            labels=all_few_shot_labels,\n",
    "            class_mapping=datasets_dict['class_mapping'],\n",
    "            method='tsne',\n",
    "            title='t-SNE Visualization of Few-Shot Embeddings'\n",
    "        )\n",
    "        \n",
    "        # Enhanced Grad-CAM Visualization\n",
    "        print(\"\\n5. Enhanced Grad-CAM Visualization:\")\n",
    "        visualize_grad_cam_comparisons(model, dataloaders['test'], datasets_dict['class_mapping'], num_comparisons=2)\n",
    "        \n",
    "        # Original Grad-CAM Visualization\n",
    "        print(\"\\n6. Individual Grad-CAM Visualization:\")\n",
    "        visualize_grad_cam(model, dataloaders['test'], datasets_dict['class_mapping'], num_images=3)\n",
    "    \n",
    "    print(\"\\nEvaluation completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(run_comprehensive_eval=True)  # Set to False for standard evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Bonus: Multiple Loss Function Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_loss_functions():\n",
    "    \"\"\"\n",
    "    Compare different loss functions for metric learning\n",
    "    \"\"\"\n",
    "    # Model parameters\n",
    "    backbone_name = 'resnet18'\n",
    "    embedding_size = 128\n",
    "    batch_size = 32\n",
    "    num_workers = 2\n",
    "    num_epochs = 15\n",
    "    \n",
    "    # Loss functions to compare\n",
    "    loss_types = ['triplet', 'contrastive', 'arcface']\n",
    "    \n",
    "    # Load data (only once)\n",
    "    train_val_dataset, test_dataset, eval_train_dataset = load_oxford_pets_dataset()\n",
    "    datasets_dict = prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset)\n",
    "    dataloaders = create_dataloaders(datasets_dict, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for loss_type in loss_types:\n",
    "        print(f\"\\n{'=' * 40}\")\n",
    "        print(f\"Training with {loss_type} loss\")\n",
    "        print(f\"{'=' * 40}\")\n",
    "        \n",
    "        # Create model\n",
    "        model = EmbeddingNet(backbone_name=backbone_name, embedding_size=embedding_size)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Create optimizer and scheduler\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "        \n",
    "        # Train model\n",
    "        model, history = train_model(\n",
    "            model=model,\n",
    "            dataloaders=dataloaders,\n",
    "            loss_type=loss_type,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            num_epochs=num_epochs,\n",
    "            embedding_size=embedding_size\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings for evaluation\n",
    "        print(\"\\nExtracting embeddings for evaluation...\")\n",
    "        train_embeddings, train_labels = extract_embeddings(model, dataloaders['eval_train'])\n",
    "        test_embeddings, test_labels = extract_embeddings(model, dataloaders['test'])\n",
    "        \n",
    "        # Evaluation\n",
    "        verification_results = evaluate_verification(test_embeddings, test_labels)\n",
    "        \n",
    "        retrieval_results = evaluate_retrieval(\n",
    "            query_embeddings=test_embeddings,\n",
    "            query_labels=test_labels,\n",
    "            gallery_embeddings=train_embeddings,\n",
    "            gallery_labels=train_labels,\n",
    "            k_values=[1, 5]\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[loss_type] = {\n",
    "            'verification': {\n",
    "                'roc_auc': verification_results['roc_auc'],\n",
    "                'eer': verification_results['eer']\n",
    "            },\n",
    "            'retrieval': {\n",
    "                'recall@1': retrieval_results['recall@1'],\n",
    "                'recall@5': retrieval_results['recall@5'],\n",
    "                'precision@1': retrieval_results['precision@1'],\n",
    "                'precision@5': retrieval_results['precision@5']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save model\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'embedding_size': embedding_size,\n",
    "            'backbone_name': backbone_name,\n",
    "            'class_mapping': datasets_dict['class_mapping']\n",
    "        }, f'pet_metric_learning_{backbone_name}_{loss_type}_comparison.pth')\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\n{'='*50}\")\n",
    "    print(\"Comparison of Loss Functions\")\n",
    "    print({'='*50})\n",
    "    \n",
    "    # Create a comparison table\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Loss Function': [],\n",
    "        'ROC AUC': [],\n",
    "        'EER': [],\n",
    "        'Recall@1': [],\n",
    "        'Recall@5': [],\n",
    "        'Precision@1': [],\n",
    "        'Precision@5': []\n",
    "    })\n",
    "    \n",
    "    for loss_type, metrics in results.items():\n",
    "        comparison_df = comparison_df.append({\n",
    "            'Loss Function': loss_type,\n",
    "            'ROC AUC': metrics['verification']['roc_auc'],\n",
    "            'EER': metrics['verification']['eer'],\n",
    "            'Recall@1': metrics['retrieval']['recall@1'],\n",
    "            'Recall@5': metrics['retrieval']['recall@5'],\n",
    "            'Precision@1': metrics['retrieval']['precision@1'],\n",
    "            'Precision@5': metrics['retrieval']['precision@5']\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    metrics = ['ROC AUC', 'Recall@1', 'Recall@5', 'Precision@1', 'Precision@5']\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, loss_type in enumerate(loss_types):\n",
    "        values = [\n",
    "            results[loss_type]['verification']['roc_auc'],\n",
    "            results[loss_type]['retrieval']['recall@1'],\n",
    "            results[loss_type]['retrieval']['recall@5'],\n",
    "            results[loss_type]['retrieval']['precision@1'],\n",
    "            results[loss_type]['retrieval']['precision@5']\n",
    "        ]\n",
    "        plt.bar(x + i*width, values, width, label=loss_type)\n",
    "    \n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Comparison of Loss Functions')\n",
    "    plt.xticks(x + width, metrics)\n",
    "    plt.legend()\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.savefig('loss_function_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return results, comparison_df\n",
    "\n",
    "# Uncomment to run the comparison\n",
    "# loss_comparison_results, loss_comparison_df = compare_loss_functions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Bonus: Streamlit Demo\n",
    "\n",
    "We've created a user-friendly Streamlit application that demonstrates the practical use of our trained metric learning model for pet similarity search. The application allows users to upload a pet image and find similar-looking pets based on the embeddings learned by our model.\n",
    "\n",
    "### Running the Streamlit App\n",
    "\n",
    "The complete Streamlit application code is available in the separate notebook file: `Charles_Watson_streamlit_pet_similarity_app.ipynb`.\n",
    "\n",
    "To run the demo:\n",
    "\n",
    "1. Ensure you have the necessary Python packages installed:\n",
    "   ```bash\n",
    "   pip install streamlit torch torchvision Pillow numpy tqdm matplotlib\n",
    "   ```\n",
    "\n",
    "2. Convert the Streamlit notebook to a Python script:\n",
    "   ```bash\n",
    "   jupyter nbconvert --to python Charles_Watson_streamlit_pet_similarity_app.ipynb\n",
    "   ```\n",
    "\n",
    "3. Make sure a trained model file (e.g., `pet_metric_learning_resnet18_triplet.pth`) exists in the working directory.\n",
    "\n",
    "4. Run the Streamlit application:\n",
    "   ```bash\n",
    "   streamlit run Charles_Watson_streamlit_pet_similarity_app.py\n",
    "   ```\n",
    "\n",
    "### Features of the Streamlit App\n",
    "\n",
    "- **Upload Interface**: Users can upload any pet image for similarity search\n",
    "- **Configuration Options**: Customize model path, database location, and number of results\n",
    "- **Interactive Results**: View similar pets with their breed labels and similarity scores\n",
    "- **Database Management**: Load and reload the pet image database as needed\n",
    "\n",
    "The app uses the same embedding model architecture (`EmbeddingNet`) as developed in this notebook (`Charles_Watson_Ndethi_Kibaki-Code.ipynb`), ensuring consistency between training and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we have implemented a comprehensive metric learning pipeline for pet breed classification using the Oxford-IIIT Pet Dataset. We have:\n",
    "\n",
    "1. Built a custom embedding model with a CNN backbone and projection head\n",
    "2. Implemented various loss functions for metric learning (Triplet, Contrastive, ArcFace)\n",
    "3. Developed evaluation methods for verification, retrieval, and few-shot classification\n",
    "4. Created visualization tools for embedding spaces and feature importance (Grad-CAM)\n",
    "5. Included bonus implementations for hard negative mining and loss function comparison\n",
    "6. Moved the Streamlit demo to a separate `pet_similarity_app.py` file.\n",
    "\n",
    "The code is modular and can be easily adapted for different settings and experimentation. To run the complete training and evaluation pipeline, simply execute the `main()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model Evaluation Framework\n",
    "\n",
    "This section implements a comprehensive evaluation framework that provides deeper insights into model performance across multiple dimensions: verification, retrieval, few-shot learning, and embedding quality. The framework generates detailed metrics and visualizations to understand strengths and weaknesses of the trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def comprehensive_evaluation(model, dataloaders, class_mapping, results_dir='./evaluation_results'):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the model on all metrics\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        dataloaders: Dictionary of dataloaders ('train', 'val', 'test', etc.)\n",
    "        class_mapping: Dictionary with class mapping information\n",
    "        results_dir: Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of all metrics\n",
    "    \"\"\"\n",
    "    # Create results directory if it doesn't exist\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp for uniqueness\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    eval_id = f\"evaluation_{timestamp}\"\n",
    "    eval_dir = os.path.join(results_dir, eval_id)\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "    \n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    # Extract embeddings for all sets\n",
    "    print(\"Extracting embeddings for evaluation...\")\n",
    "    all_embeddings = {}\n",
    "    all_labels = {}\n",
    "    \n",
    "    for split_name in ['eval_train', 'test', 'few_shot_train', 'few_shot_test']:\n",
    "        if split_name in dataloaders:\n",
    "            print(f\"  Processing {split_name} set...\")\n",
    "            embeddings, labels = extract_embeddings(model, dataloaders[split_name])\n",
    "            all_embeddings[split_name] = embeddings\n",
    "            all_labels[split_name] = labels\n",
    "    \n",
    "    # 1. Verification metrics\n",
    "    print(\"\\n1. Computing verification metrics...\")\n",
    "    verification_results = {}\n",
    "    \n",
    "    # Generate pairs for verification with more comprehensive settings\n",
    "    test_pairs, test_pair_labels = generate_verification_pairs(\n",
    "        all_embeddings['test'], all_labels['test'], num_pos_pairs=2000, num_neg_pairs=2000)\n",
    "    \n",
    "    # Compute ROC and other metrics\n",
    "    fpr, tpr, thresholds = roc_curve(test_pair_labels, test_pairs)\n",
    "    verification_results['roc_auc'] = auc(fpr, tpr)\n",
    "    \n",
    "    # Equal Error Rate (EER)\n",
    "    fnr = 1 - tpr\n",
    "    eer_idx = np.nanargmin(np.absolute(fnr - fpr))\n",
    "    eer_threshold = thresholds[eer_idx]\n",
    "    eer = fpr[eer_idx]\n",
    "    verification_results['eer'] = eer\n",
    "    verification_results['eer_threshold'] = eer_threshold.item() if isinstance(eer_threshold, np.ndarray) else eer_threshold\n",
    "    \n",
    "    # F1 Score at different thresholds\n",
    "    precision_values, recall_values, pr_thresholds = precision_recall_curve(test_pair_labels, test_pairs)\n",
    "    f1_scores = 2 * precision_values * recall_values / (precision_values + recall_values + 1e-10)\n",
    "    best_f1_idx = np.argmax(f1_scores)\n",
    "    verification_results['best_f1_score'] = f1_scores[best_f1_idx]\n",
    "    verification_results['best_f1_threshold'] = pr_thresholds[best_f1_idx] if best_f1_idx < len(pr_thresholds) else pr_thresholds[-1]\n",
    "    \n",
    "    # Plot ROC curve with more information\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC curve (AUC = {verification_results[\"roc_auc\"]:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random')\n",
    "    plt.plot([0, 1], [1, 0], 'g-.', linewidth=1.5, alpha=0.7, label='Perfect')\n",
    "    plt.plot([eer, eer], [0, 1-eer], 'r:', linewidth=1.5)\n",
    "    plt.plot([0, eer], [1-eer, 1-eer], 'r:', linewidth=1.5, label=f'EER = {eer:.4f}')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(eval_dir, 'roc_curve.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Retrieval metrics with more detail\n",
    "    print(\"\\n2. Computing retrieval metrics...\")\n",
    "    retrieval_results = {}\n",
    "    \n",
    "    # More comprehensive retrieval evaluation\n",
    "    query_embeddings = all_embeddings['test']\n",
    "    query_labels = all_labels['test']\n",
    "    gallery_embeddings = all_embeddings['eval_train']\n",
    "    gallery_labels = all_labels['eval_train']\n",
    "    \n",
    "    # Compute pair-wise similarity matrix\n",
    "    similarity_matrix = torch.matmul(query_embeddings, gallery_embeddings.T)\n",
    "    \n",
    "    # Evaluate at different k values\n",
    "    k_values = [1, 5, 10, 20]\n",
    "    for k in k_values:\n",
    "        # Get top-k indices for each query\n",
    "        _, indices = torch.topk(similarity_matrix, k=min(k, gallery_embeddings.size(0)), dim=1)\n",
    "        \n",
    "        # Compute metrics\n",
    "        recall_at_k = 0\n",
    "        precision_at_k = 0\n",
    "        map_at_k = 0  # Mean Average Precision@K\n",
    "        ndcg_at_k = 0  # Normalized Discounted Cumulative Gain@K\n",
    "        \n",
    "        for i, query_label in enumerate(query_labels):\n",
    "            retrieved_labels = gallery_labels[indices[i]]\n",
    "            relevant = (retrieved_labels == query_label).float()\n",
    "            \n",
    "            # Recall@K: How many of the relevant items are retrieved\n",
    "            # (at least one relevant item in top-k)\n",
    "            recall_at_k += (relevant.sum() > 0).float().item()\n",
    "            \n",
    "            # Precision@K: How many of the retrieved items are relevant\n",
    "            precision_at_k += (relevant.sum() / k).item()\n",
    "            \n",
    "            # Average Precision calculation\n",
    "            if relevant.sum() > 0:\n",
    "                precision_at_ranks = torch.cumsum(relevant, dim=0) / torch.arange(1, k+1).float().to(relevant.device)\n",
    "                ap = (precision_at_ranks * relevant).sum() / max(relevant.sum().item(), 1)\n",
    "                map_at_k += ap.item()\n",
    "            \n",
    "            # NDCG calculation\n",
    "            dcg = torch.sum(relevant / torch.log2(torch.arange(2, k+2).float().to(relevant.device)))\n",
    "            # Ideal DCG would have all relevant items at the top\n",
    "            ideal_relevant = torch.zeros_like(relevant)\n",
    "            ideal_relevant[:min(int(relevant.sum().item()), k)] = 1\n",
    "            idcg = torch.sum(ideal_relevant / torch.log2(torch.arange(2, k+2).float().to(relevant.device)))\n",
    "            ndcg = dcg / max(idcg.item(), 1e-10)\n",
    "            ndcg_at_k += ndcg.item()\n",
    "        \n",
    "        n_queries = len(query_labels)\n",
    "        retrieval_results[f'recall@{k}'] = recall_at_k / n_queries\n",
    "        retrieval_results[f'precision@{k}'] = precision_at_k / n_queries\n",
    "        retrieval_results[f'map@{k}'] = map_at_k / n_queries\n",
    "        retrieval_results[f'ndcg@{k}'] = ndcg_at_k / n_queries\n",
    "        \n",
    "        print(f\"  Recall@{k}: {retrieval_results[f'recall@{k}']:.4f}\")\n",
    "        print(f\"  Precision@{k}: {retrieval_results[f'precision@{k}']:.4f}\")\n",
    "        print(f\"  MAP@{k}: {retrieval_results[f'map@{k}']:.4f}\")\n",
    "        print(f\"  NDCG@{k}: {retrieval_results[f'ndcg@{k}']:.4f}\")\n",
    "    \n",
    "    # Plot retrieval metrics across k values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metrics = ['recall', 'precision', 'map', 'ndcg']\n",
    "    metric_colors = {'recall': 'blue', 'precision': 'red', 'map': 'green', 'ndcg': 'purple'}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        values = [retrieval_results[f'{metric}@{k}'] for k in k_values]\n",
    "        plt.plot(k_values, values, 'o-', label=f'{metric.upper()}@K', color=metric_colors[metric])\n",
    "    \n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Retrieval Metrics vs. K')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(eval_dir, 'retrieval_metrics.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Few-shot learning with confidence intervals\n",
    "    print(\"\\n3. Computing few-shot learning metrics...\")\n",
    "    few_shot_results = {}\n",
    "    \n",
    "    if 'few_shot_train' in all_embeddings and 'few_shot_test' in all_embeddings:\n",
    "        # Test different N-way K-shot configurations\n",
    "        few_shot_configs = [\n",
    "            {'n_way': 5, 'k_shot': 1, 'num_episodes': 100},\n",
    "            {'n_way': 5, 'k_shot': 5, 'num_episodes': 100},\n",
    "            {'n_way': 10, 'k_shot': 1, 'num_episodes': 100},\n",
    "            {'n_way': 10, 'k_shot': 5, 'num_episodes': 100}\n",
    "        ]\n",
    "        \n",
    "        for config in few_shot_configs:\n",
    "            n_way = config['n_way']\n",
    "            k_shot = config['k_shot']\n",
    "            num_episodes = config['num_episodes']\n",
    "            \n",
    "            print(f\"  Evaluating {n_way}-way {k_shot}-shot learning...\")\n",
    "            fs_result = evaluate_few_shot(\n",
    "                all_embeddings['few_shot_train'],\n",
    "                all_labels['few_shot_train'],\n",
    "                all_embeddings['few_shot_test'],\n",
    "                all_labels['few_shot_test'],\n",
    "                n_way=n_way,\n",
    "                k_shot=k_shot,\n",
    "                num_episodes=num_episodes\n",
    "            )\n",
    "            few_shot_results[f'{n_way}way_{k_shot}shot'] = {\n",
    "                'mean_accuracy': fs_result['mean_accuracy'],\n",
    "                'std_accuracy': fs_result['std_accuracy'],\n",
    "                'conf_interval': [float(x) for x in fs_result['conf_interval']]\n",
    "            }\n",
    "    else:\n",
    "        print(\"  Few-shot datasets not available. Skipping few-shot evaluation.\")\n",
    "    \n",
    "    # 4. Embedding quality analysis\n",
    "    print(\"\\n4. Analyzing embedding quality...\")\n",
    "    embedding_quality = {}\n",
    "    \n",
    "    # Intra-class and inter-class distances\n",
    "    test_embeddings = all_embeddings['test']\n",
    "    test_labels = all_labels['test']\n",
    "    intra_class_dists = []\n",
    "    inter_class_dists = []\n",
    "    \n",
    "    # Calculate pairwise distances\n",
    "    for i in range(len(test_embeddings)):\n",
    "        for j in range(i+1, len(test_embeddings)):\n",
    "            # Cosine distance (1 - cosine similarity)\n",
    "            similarity = torch.dot(test_embeddings[i], test_embeddings[j])\n",
    "            distance = 1.0 - similarity.item()\n",
    "            \n",
    "            if test_labels[i] == test_labels[j]:\n",
    "                intra_class_dists.append(distance)\n",
    "            else:\n",
    "                inter_class_dists.append(distance)\n",
    "    \n",
    "    intra_class_dists = np.array(intra_class_dists)\n",
    "    inter_class_dists = np.array(inter_class_dists)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    embedding_quality['intra_class_mean'] = float(np.mean(intra_class_dists))\n",
    "    embedding_quality['intra_class_std'] = float(np.std(intra_class_dists))\n",
    "    embedding_quality['inter_class_mean'] = float(np.mean(inter_class_dists))\n",
    "    embedding_quality['inter_class_std'] = float(np.std(inter_class_dists))\n",
    "    \n",
    "    # Fisher criterion (larger is better separation)\n",
    "    intra_var = np.var(intra_class_dists)\n",
    "    inter_var = np.var(inter_class_dists)\n",
    "    intra_mean = np.mean(intra_class_dists)\n",
    "    inter_mean = np.mean(inter_class_dists)\n",
    "    fisher_score = ((inter_mean - intra_mean) ** 2) / (intra_var + inter_var)\n",
    "    embedding_quality['fisher_score'] = float(fisher_score)\n",
    "    \n",
    "    print(f\"  Intra-class distance: {embedding_quality['intra_class_mean']:.4f} ± {embedding_quality['intra_class_std']:.4f}\")\n",
    "    print(f\"  Inter-class distance: {embedding_quality['inter_class_mean']:.4f} ± {embedding_quality['inter_class_std']:.4f}\")\n",
    "    print(f\"  Fisher score: {embedding_quality['fisher_score']:.4f}\")\n",
    "    \n",
    "    # Plot distance distributions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(intra_class_dists, bins=50, alpha=0.6, label='Intra-class', density=True)\n",
    "    plt.hist(inter_class_dists, bins=50, alpha=0.6, label='Inter-class', density=True)\n",
    "    plt.xlabel('Cosine Distance')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Distribution of Intra-class vs Inter-class Distances')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(eval_dir, 'distance_distributions.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Confusion matrix of most confused classes\n",
    "    print(\"\\n5. Identifying commonly confused classes...\")\n",
    "    # Create a confusion dictionary between classes\n",
    "    confusion_dict = {i: {} for i in range(len(np.unique(test_labels)))}\n",
    "    \n",
    "    # For each query, see what other classes it's closest to\n",
    "    class_counts = np.bincount(test_labels.numpy())\n",
    "    unique_classes = np.unique(test_labels.numpy())\n",
    "    \n",
    "    # Create a matrix to track confusion between classes\n",
    "    confusion_matrix = torch.zeros(len(unique_classes), len(unique_classes))\n",
    "    \n",
    "    # Calculate confusion based on nearest neighbors\n",
    "    for i, query_emb in enumerate(test_embeddings):\n",
    "        query_label = test_labels[i].item()\n",
    "        \n",
    "        # Calculate similarities to all other embeddings\n",
    "        similarities = torch.matmul(query_emb.unsqueeze(0), test_embeddings.T).squeeze()\n",
    "        \n",
    "        # Remove self-similarity\n",
    "        similarities[i] = -1.0\n",
    "        \n",
    "        # Get top 5 most similar\n",
    "        topk_values, topk_indices = torch.topk(similarities, k=min(5, len(similarities)-1))\n",
    "        \n",
    "        # Update confusion matrix\n",
    "        for idx in topk_indices:\n",
    "            other_label = test_labels[idx].item()\n",
    "            if query_label != other_label:\n",
    "                confusion_matrix[query_label, other_label] += 1\n",
    "    \n",
    "    # Normalize confusion matrix by class frequency\n",
    "    for i in range(confusion_matrix.size(0)):\n",
    "        if class_counts[i] > 0:\n",
    "            confusion_matrix[i] /= class_counts[i]\n",
    "    \n",
    "    # Convert class indices to class names\n",
    "    idx_to_class = class_mapping['idx_to_class']\n",
    "    class_names = [idx_to_class[i] for i in unique_classes]\n",
    "    \n",
    "    # Get most confused pairs\n",
    "    confused_pairs = []\n",
    "    n_classes = confusion_matrix.size(0)\n",
    "    flattened = confusion_matrix.view(-1)\n",
    "    topk_values, topk_indices = torch.topk(flattened, k=min(10, n_classes*n_classes))\n",
    "    \n",
    "    for idx in topk_indices:\n",
    "        if topk_values[idx] > 0:\n",
    "            i = idx.item() // n_classes\n",
    "            j = idx.item() % n_classes\n",
    "            class1 = idx_to_class[i]\n",
    "            class2 = idx_to_class[j]\n",
    "            confused_pairs.append((class1, class2, topk_values[idx].item()))\n",
    "    \n",
    "    # Display most confused class pairs\n",
    "    print(\"  Most confused class pairs:\")\n",
    "    for class1, class2, score in confused_pairs[:5]:\n",
    "        print(f\"    {class1} confused with {class2}: {score:.4f}\")\n",
    "    \n",
    "    # Combine all results\n",
    "    all_results = {\n",
    "        'verification': verification_results,\n",
    "        'retrieval': retrieval_results,\n",
    "        'few_shot': few_shot_results,\n",
    "        'embedding_quality': embedding_quality\n",
    "    }\n",
    "    \n",
    "    # Save results as JSON\n",
    "    with open(os.path.join(eval_dir, 'evaluation_results.json'), 'w') as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "    \n",
    "    print(f\"\\nEvaluation complete. Results saved to {eval_dir}\")\n",
    "    \n",
    "    return all_results, eval_dir\n",
    "\n",
    "# Helper function: Generate verification pairs with better sampling\n",
    "def generate_verification_pairs(embeddings, labels, num_pos_pairs=1000, num_neg_pairs=1000):\n",
    "    \"\"\"\n",
    "    Generate balanced positive and negative pairs for verification task, with better sampling\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Embedding tensor\n",
    "        labels: Labels tensor\n",
    "        num_pos_pairs: Number of positive pairs to generate\n",
    "        num_neg_pairs: Number of negative pairs to generate\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (similarity scores, pair labels)\n",
    "    \"\"\"\n",
    "    # Convert to numpy for easier indexing if they're not already\n",
    "    if isinstance(embeddings, torch.Tensor):\n",
    "        embeddings_np = embeddings.cpu().numpy()\n",
    "    else:\n",
    "        embeddings_np = embeddings\n",
    "        \n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels_np = labels.cpu().numpy()\n",
    "    else:\n",
    "        labels_np = labels\n",
    "    \n",
    "    unique_labels = np.unique(labels_np)\n",
    "    pairs = []\n",
    "    pair_labels = []\n",
    "    \n",
    "    # Generate positive pairs (same class) with stratified sampling\n",
    "    pos_pair_count = 0\n",
    "    pairs_per_class = max(1, num_pos_pairs // len(unique_labels))\n",
    "    \n",
    "    # First try to sample evenly from each class\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels_np == label)[0]\n",
    "        if len(indices) < 2:\n",
    "            continue\n",
    "            \n",
    "        # Calculate how many pairs we can generate from this class\n",
    "        max_pairs = min(pairs_per_class, len(indices) * (len(indices) - 1) // 2)\n",
    "        \n",
    "        # Sample unique pairs\n",
    "        pair_count = 0\n",
    "        max_attempts = max_pairs * 3  # Allow some failed attempts\n",
    "        attempts = 0\n",
    "        \n",
    "        while pair_count < max_pairs and attempts < max_attempts:\n",
    "            idx1, idx2 = np.random.choice(indices, 2, replace=False)\n",
    "            \n",
    "            # Skip if pair already exists\n",
    "            if any((p[0] == idx1 and p[1] == idx2) or (p[0] == idx2 and p[1] == idx1) for p in pairs):\n",
    "                attempts += 1\n",
    "                continue\n",
    "                \n",
    "            # Use cosine similarity since our embeddings are normalized\n",
    "            similarity = np.dot(embeddings_np[idx1], embeddings_np[idx2])\n",
    "            \n",
    "            pairs.append((idx1, idx2))\n",
    "            pair_labels.append(1)  # 1 for same class\n",
    "            pairs.append(similarity)\n",
    "            pos_pair_count += 1\n",
    "            pair_count += 1\n",
    "        \n",
    "    # If we need more positive pairs, sample randomly\n",
    "    if pos_pair_count < num_pos_pairs:\n",
    "        attempts = 0\n",
    "        max_attempts = (num_pos_pairs - pos_pair_count) * 5\n",
    "        \n",
    "        while pos_pair_count < num_pos_pairs and attempts < max_attempts:\n",
    "            label = np.random.choice(unique_labels)\n",
    "            indices = np.where(labels_np == label)[0]\n",
    "            if len(indices) < 2:\n",
    "                attempts += 1\n",
    "                continue\n",
    "                \n",
    "            idx1, idx2 = np.random.choice(indices, 2, replace=False)\n",
    "            \n",
    "            # Skip if pair already exists\n",
    "            if any((p[0] == idx1 and p[1] == idx2) or (p[0] == idx2 and p[1] == idx1) for p in pairs):\n",
    "                attempts += 1\n",
    "                continue\n",
    "                \n",
    "            similarity = np.dot(embeddings_np[idx1], embeddings_np[idx2])\n",
    "            \n",
    "            pairs.append((idx1, idx2))\n",
    "            pair_labels.append(1)\n",
    "            pairs.append(similarity)\n",
    "            pos_pair_count += 1\n",
    "    \n",
    "    # Generate negative pairs (different classes)\n",
    "    # Use a mix of random and hard negative mining\n",
    "    neg_pair_count = 0\n",
    "    \n",
    "    # First, compute all-to-all similarities for more efficient hard negative mining\n",
    "    sim_matrix = np.zeros((len(embeddings_np), len(embeddings_np)))\n",
    "    for i in range(len(embeddings_np)):\n",
    "        sim_matrix[i] = np.dot(embeddings_np, embeddings_np[i])\n",
    "    \n",
    "    # Generate some truly random negatives\n",
    "    random_neg_pairs = num_neg_pairs // 2\n",
    "    attempts = 0\n",
    "    max_attempts = random_neg_pairs * 5\n",
    "    \n",
    "    while neg_pair_count < random_neg_pairs and attempts < max_attempts:\n",
    "        idx1 = np.random.randint(0, len(labels_np))\n",
    "        idx2 = np.random.randint(0, len(labels_np))\n",
    "        \n",
    "        # Ensure different classes\n",
    "        if labels_np[idx1] == labels_np[idx2]:\n",
    "            attempts += 1\n",
    "            continue\n",
    "            \n",
    "        # Skip if pair already exists\n",
    "        if any((p[0] == idx1 and p[1] == idx2) or (p[0] == idx2 and p[1] == idx1) for p in pairs):\n",
    "            attempts += 1\n",
    "            continue\n",
    "            \n",
    "        similarity = sim_matrix[idx1, idx2]\n",
    "        \n",
    "        pairs.append((idx1, idx2))\n",
    "        pair_labels.append(0)  # 0 for different class\n",
    "        pairs.append(similarity)\n",
    "        neg_pair_count += 1\n",
    "    \n",
    "    # Now generate hard negatives (different classes but high similarity)\n",
    "    hard_neg_pairs = num_neg_pairs - neg_pair_count\n",
    "    \n",
    "    # For each class, find most similar examples from other classes\n",
    "    for label in unique_labels:\n",
    "        if neg_pair_count >= num_neg_pairs:\n",
    "            break\n",
    "            \n",
    "        # Get examples of this class\n",
    "        class_indices = np.where(labels_np == label)[0]\n",
    "        if len(class_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        # For each example, find most similar from other classes\n",
    "        for idx1 in class_indices:\n",
    "            if neg_pair_count >= num_neg_pairs:\n",
    "                break\n",
    "                \n",
    "            # Get similarities to all other examples\n",
    "            similarities = sim_matrix[idx1]\n",
    "            \n",
    "            # Create a mask for examples from different classes\n",
    "            diff_class_mask = (labels_np != label)\n",
    "            \n",
    "            # Get most similar example from a different class\n",
    "            diff_class_sims = similarities * diff_class_mask\n",
    "            if np.max(diff_class_sims) == 0:  # No valid candidates\n",
    "                continue\n",
    "                \n",
    "            idx2 = np.argmax(diff_class_sims)\n",
    "            similarity = similarities[idx2]\n",
    "            \n",
    "            # Skip if pair already exists\n",
    "            if any((p[0] == idx1 and p[1] == idx2) or (p[0] == idx2 and p[1] == idx1) for p in pairs):\n",
    "                continue\n",
    "                \n",
    "            pairs.append((idx1, idx2))\n",
    "            pair_labels.append(0)  # 0 for different class\n",
    "            pairs.append(similarity)\n",
    "            neg_pair_count += 1\n",
    "    \n",
    "    similarities = [pairs[i*2+1] for i in range(len(pair_labels))]\n",
    "    return np.array(similarities), np.array(pair_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Main Execution\n",
    "\n",
    "The updated main function includes a call to our comprehensive evaluation framework to generate detailed metrics."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
