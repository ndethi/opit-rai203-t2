{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment 2: Metric Learning with Oxford-IIIT Pet Dataset\n",
    "\n",
    "## Introduction and Setup\n",
    "\n",
    "This notebook implements a deep metric learning approach for the Oxford-IIIT Pet Dataset, focusing on learning an embedding space where similar pet breeds are close together and dissimilar ones are far apart. We'll explore different loss functions, evaluate the model on verification, retrieval, and few-shot classification tasks, and visualize the embedding space.\n",
    "\n",
    "### Environment Setup and Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if running in Colab (to install dependencies and set up environment)\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Install required packages\n",
    "if IN_COLAB:\n",
    "    !pip install pytorch-metric-learning\n",
    "    !pip install faiss-gpu\n",
    "    !pip install umap-learn\n",
    "    !pip install matplotlib seaborn scikit-learn tqdm\n",
    "    !pip install gradio\n",
    "    !pip install grad-cam\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import pytorch_metric_learning\n",
    "from pytorch_metric_learning import losses, miners, distances, reducers, testers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import umap\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Data Loading and Preprocessing\n",
    "\n",
    "In this section, we'll load the Oxford-IIIT Pet Dataset, perform necessary preprocessing, and create appropriate data loaders for our metric learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to load the dataset\n",
    "def load_oxford_pets_dataset(root=\"./data\", download=True):\n",
    "    train_val_dataset = datasets.OxfordIIITPet(\n",
    "        root=root, \n",
    "        split=\"trainval\", \n",
    "        transform=train_transform, \n",
    "        download=download\n",
    "    )\n",
    "    \n",
    "    test_dataset = datasets.OxfordIIITPet(\n",
    "        root=root, \n",
    "        split=\"test\", \n",
    "        transform=eval_transform, \n",
    "        download=download\n",
    "    )\n",
    "    \n",
    "    # For evaluation, create a version of the training set with eval transforms\n",
    "    eval_train_dataset = datasets.OxfordIIITPet(\n",
    "        root=root, \n",
    "        split=\"trainval\", \n",
    "        transform=eval_transform, \n",
    "        download=False\n",
    "    )\n",
    "    \n",
    "    return train_val_dataset, test_dataset, eval_train_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset Preparation for Different Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data for training, validation and few-shot evaluation\n",
    "def prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset, num_holdout_classes=5, val_ratio=0.2):\n",
    "    # Get the class names\n",
    "    class_to_idx = train_val_dataset.class_to_idx\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    num_classes = len(class_to_idx)\n",
    "    \n",
    "    # Split classes for few-shot learning (hold out some classes for testing)\n",
    "    all_class_indices = list(range(num_classes))\n",
    "    holdout_class_indices = random.sample(all_class_indices, num_holdout_classes)\n",
    "    training_class_indices = [i for i in all_class_indices if i not in holdout_class_indices]\n",
    "    \n",
    "    holdout_classes = [idx_to_class[i] for i in holdout_class_indices]\n",
    "    print(f\"Holdout classes for few-shot learning: {holdout_classes}\")\n",
    "    \n",
    "    # Create datasets excluding holdout classes for main training\n",
    "    train_val_indices = [i for i, (_, label) in enumerate(train_val_dataset) if label not in holdout_class_indices]\n",
    "    test_indices = [i for i, (_, label) in enumerate(test_dataset) if label not in holdout_class_indices]\n",
    "    eval_train_indices = [i for i, (_, label) in enumerate(eval_train_dataset) if label not in holdout_class_indices]\n",
    "    \n",
    "    # For few-shot learning, include only holdout classes\n",
    "    few_shot_train_indices = [i for i, (_, label) in enumerate(train_val_dataset) if label in holdout_class_indices]\n",
    "    few_shot_test_indices = [i for i, (_, label) in enumerate(test_dataset) if label in holdout_class_indices]\n",
    "    \n",
    "    # Split train/val\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        train_val_indices, \n",
    "        test_size=val_ratio, \n",
    "        stratify=[train_val_dataset[i][1] for i in train_val_indices],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create Subset datasets\n",
    "    train_dataset = Subset(train_val_dataset, train_indices)\n",
    "    val_dataset = Subset(train_val_dataset, val_indices)\n",
    "    test_filtered_dataset = Subset(test_dataset, test_indices)\n",
    "    eval_train_dataset = Subset(eval_train_dataset, eval_train_indices)\n",
    "    \n",
    "    # Create datasets for few-shot learning\n",
    "    few_shot_train_dataset = Subset(train_val_dataset, few_shot_train_indices)\n",
    "    few_shot_test_dataset = Subset(test_dataset, few_shot_test_indices)\n",
    "    \n",
    "    # Create dictionary for class mapping\n",
    "    class_mapping = {\n",
    "        'class_to_idx': class_to_idx,\n",
    "        'idx_to_class': idx_to_class,\n",
    "        'holdout_class_indices': holdout_class_indices,\n",
    "        'training_class_indices': training_class_indices\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset,\n",
    "        'test': test_filtered_dataset,\n",
    "        'eval_train': eval_train_dataset,\n",
    "        'few_shot_train': few_shot_train_dataset,\n",
    "        'few_shot_test': few_shot_test_dataset,\n",
    "        'class_mapping': class_mapping\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataloaders(datasets_dict, batch_size=32, num_workers=2):\n",
    "    dataloaders = {}\n",
    "    \n",
    "    for key in ['train', 'val', 'test', 'eval_train', 'few_shot_train', 'few_shot_test']:\n",
    "        if key == 'train':\n",
    "            shuffle = True\n",
    "        else:\n",
    "            shuffle = False\n",
    "            \n",
    "        dataloaders[key] = DataLoader(\n",
    "            datasets_dict[key],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    return dataloaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "train_val_dataset, test_dataset, eval_train_dataset = load_oxford_pets_dataset()\n",
    "print(f\"Train+Val size: {len(train_val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")\n",
    "\n",
    "# Prepare datasets for different tasks\n",
    "datasets_dict = prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32  # Adjust based on your GPU/memory constraints\n",
    "dataloaders = create_dataloaders(datasets_dict, batch_size=batch_size)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "for key, dataloader in dataloaders.items():\n",
    "    print(f\"{key}: {len(dataloader.dataset)} samples\")\n",
    "\n",
    "class_mapping = datasets_dict['class_mapping']\n",
    "num_classes = len(class_mapping['class_to_idx'])\n",
    "print(f\"Total number of classes: {num_classes}\")\n",
    "print(f\"Number of training classes: {len(class_mapping['training_class_indices'])}\")\n",
    "print(f\"Number of few-shot classes: {len(class_mapping['holdout_class_indices'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "In this section, we'll define our metric learning model architecture using a CNN backbone and a projection head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet18', embedding_size=128, pretrained=True):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        \n",
    "        # Get backbone and its output size\n",
    "        self.backbone, backbone_output_size = self._get_backbone(backbone_name, pretrained)\n",
    "        \n",
    "        # Projection head (MLP)\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(backbone_output_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, embedding_size)\n",
    "        )\n",
    "        \n",
    "    def _get_backbone(self, backbone_name, pretrained):\n",
    "        \"\"\"\n",
    "        Create a backbone network from various architectures\n",
    "        \"\"\"\n",
    "        if backbone_name == 'resnet18':\n",
    "            backbone = models.resnet18(pretrained=pretrained)\n",
    "            output_size = 512\n",
    "        elif backbone_name == 'resnet34':\n",
    "            backbone = models.resnet34(pretrained=pretrained)\n",
    "            output_size = 512\n",
    "        elif backbone_name == 'resnet50':\n",
    "            backbone = models.resnet50(pretrained=pretrained)\n",
    "            output_size = 2048\n",
    "        elif backbone_name == 'efficientnet_b0':\n",
    "            backbone = models.efficientnet_b0(pretrained=pretrained)\n",
    "            output_size = 1280\n",
    "        elif backbone_name == 'mobilenet_v2':\n",
    "            backbone = models.mobilenet_v2(pretrained=pretrained)\n",
    "            output_size = 1280\n",
    "        elif backbone_name == 'densenet121':\n",
    "            backbone = models.densenet121(pretrained=pretrained)\n",
    "            output_size = 1024\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
    "        \n",
    "        # For ResNet models\n",
    "        if backbone_name.startswith('resnet'):\n",
    "            # Remove the classification layer\n",
    "            backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        # For EfficientNet\n",
    "        elif backbone_name.startswith('efficientnet'):\n",
    "            backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        # For MobileNet\n",
    "        elif backbone_name.startswith('mobilenet'):\n",
    "            backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        # For DenseNet\n",
    "        elif backbone_name.startswith('densenet'):\n",
    "            backbone = nn.Sequential(\n",
    "                backbone.features,\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AdaptiveAvgPool2d((1, 1))\n",
    "            )\n",
    "        \n",
    "        return backbone, output_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        embeddings = self.projection_head(features)\n",
    "        \n",
    "        # Normalize embeddings to unit length (important for cosine distance)\n",
    "        normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return normalized_embeddings\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loss Function Implementation\n",
    "\n",
    "Here we'll implement several loss functions for metric learning including Triplet Loss, Contrastive Loss, and ArcFace. We'll also implement miners for efficient training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_loss_and_miner(loss_type, margin=0.2, embedding_size=128, num_classes=32):\n",
    "    \"\"\"\n",
    "    Create loss function and miner for metric learning\n",
    "    \"\"\"\n",
    "    if loss_type == 'triplet':\n",
    "        # Triplet loss with cosine distance\n",
    "        distance = distances.CosineSimilarity()\n",
    "        reducer = reducers.ThresholdReducer(low=0)\n",
    "        loss_func = losses.TripletMarginLoss(margin=margin, distance=distance, reducer=reducer)\n",
    "        mining_func = miners.TripletMarginMiner(margin=margin, distance=distance, type_of_triplets=\"semihard\")\n",
    "        \n",
    "    elif loss_type == 'contrastive':\n",
    "        # Contrastive loss\n",
    "        distance = distances.CosineSimilarity()\n",
    "        loss_func = losses.ContrastiveLoss(pos_margin=0.8, neg_margin=0.2, distance=distance)\n",
    "        mining_func = miners.PairMarginMiner(pos_margin=0.8, neg_margin=0.2, distance=distance)\n",
    "        \n",
    "    elif loss_type == 'arcface':\n",
    "        # ArcFace loss\n",
    "        loss_func = losses.ArcFaceLoss(embedding_size, num_classes, margin=28.6, scale=64)\n",
    "        mining_func = None\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss type: {loss_type}\")\n",
    "        \n",
    "    return loss_func, mining_func\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Hard Negative Mining (Bonus Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HardNegativePairMiner(miners.BaseMiner):\n",
    "    def __init__(self, distance, neg_margin=0.2, hardest_fraction=0.5):\n",
    "        super().__init__()\n",
    "        self.distance = distance\n",
    "        self.neg_margin = neg_margin\n",
    "        self.hardest_fraction = hardest_fraction\n",
    "        \n",
    "    def mine(self, embeddings, labels, ref_emb=None, ref_labels=None):\n",
    "        ref_emb, ref_labels = embeddings, labels\n",
    "        dist_mat = self.distance(embeddings, ref_emb)\n",
    "        \n",
    "        # Get negative pairs (different classes)\n",
    "        negative_mask = labels.unsqueeze(1) != ref_labels.unsqueeze(0)\n",
    "        \n",
    "        # For each anchor, find all negative pairs\n",
    "        anchors, negatives = torch.where(negative_mask)\n",
    "        \n",
    "        if len(anchors) == 0:\n",
    "            return empty_tensor(0), empty_tensor(0), empty_tensor(0), empty_tensor(0)\n",
    "        \n",
    "        # Get distances for all negative pairs\n",
    "        distances = dist_mat[anchors, negatives]\n",
    "        \n",
    "        # Group by anchor\n",
    "        anchor_groups = defaultdict(list)\n",
    "        for i in range(len(anchors)):\n",
    "            anchor_groups[anchors[i].item()].append((negatives[i].item(), distances[i].item()))\n",
    "        \n",
    "        # For each anchor, select the hardest negatives\n",
    "        hard_a, hard_n = [], []\n",
    "        for anchor, neg_dists in anchor_groups.items():\n",
    "            # Sort negatives by distance (ascending for hardest cosine similarity)\n",
    "            neg_dists.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Select hardest fraction\n",
    "            num_to_select = max(1, int(len(neg_dists) * self.hardest_fraction))\n",
    "            selected_negs = neg_dists[:num_to_select]\n",
    "            \n",
    "            for neg, dist in selected_negs:\n",
    "                hard_a.append(anchor)\n",
    "                hard_n.append(neg)\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(hard_a, device=embeddings.device), \n",
    "            empty_tensor(0), \n",
    "            empty_tensor(0), \n",
    "            torch.tensor(hard_n, device=embeddings.device)\n",
    "        )\n",
    "        \n",
    "def empty_tensor(size):\n",
    "    return torch.tensor([], device=device, dtype=torch.long).view(size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Hyperparameter Tuning Framework\n",
    "\n",
    "This section implements a simple hyperparameter tuning framework to find the best combination of parameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add a simple hyperparameter tuning framework\n",
    "def hyperparameter_tuning(param_grid, n_trials=5, evaluation_metric='combined'):\n",
    "    \"\"\"\n",
    "    Simple hyperparameter tuning framework\n",
    "    \n",
    "    Args:\n",
    "        param_grid (dict): Dictionary where keys are parameter names and values are lists of parameter values\n",
    "        n_trials (int): Number of random combinations to try\n",
    "        evaluation_metric (str): Which metric to use for choosing the best model ('roc_auc', 'recall', 'precision', 'combined')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Best parameters and the corresponding performance\n",
    "    \"\"\"\n",
    "    print(f\"Starting hyperparameter tuning with {n_trials} trials\")\n",
    "    print(f\"Parameter grid: {param_grid}\")\n",
    "    \n",
    "    # Load data (only once)\n",
    "    train_val_dataset, test_dataset, eval_train_dataset = load_oxford_pets_dataset()\n",
    "    datasets_dict = prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset)\n",
    "    \n",
    "    # Track results\n",
    "    results = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Sample random combination of hyperparameters\n",
    "        params = {}\n",
    "        for param_name, param_values in param_grid.items():\n",
    "            params[param_name] = random.choice(param_values)\n",
    "        \n",
    "        print(f\"\\n\\nTrial {trial+1}/{n_trials}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "        \n",
    "        # Create dataloaders\n",
    "        dataloaders = create_dataloaders(\n",
    "            datasets_dict, \n",
    "            batch_size=params.get('batch_size', 32),\n",
    "            num_workers=params.get('num_workers', 2)\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        model = EmbeddingNet(\n",
    "            backbone_name=params.get('backbone_name', 'resnet18'),\n",
    "            embedding_size=params.get('embedding_size', 128),\n",
    "            pretrained=True\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Create optimizer\n",
    "        if params.get('optimizer_name', 'adam') == 'adam':\n",
    "            optimizer = optim.Adam(\n",
    "                model.parameters(), \n",
    "                lr=params.get('learning_rate', 1e-4),\n",
    "                weight_decay=params.get('weight_decay', 0)\n",
    "            )\n",
    "        elif params.get('optimizer_name', 'adam') == 'sgd':\n",
    "            optimizer = optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=params.get('learning_rate', 1e-3),\n",
    "                momentum=params.get('momentum', 0.9),\n",
    "                weight_decay=params.get('weight_decay', 0)\n",
    "            )\n",
    "        \n",
    "        # Create scheduler\n",
    "        scheduler = None\n",
    "        if params.get('use_scheduler', True):\n",
    "            if params.get('scheduler_name', 'step') == 'step':\n",
    "                scheduler = optim.lr_scheduler.StepLR(\n",
    "                    optimizer, \n",
    "                    step_size=params.get('scheduler_step_size', 5), \n",
    "                    gamma=params.get('scheduler_gamma', 0.5)\n",
    "                )\n",
    "            elif params.get('scheduler_name', 'step') == 'cosine':\n",
    "                scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                    optimizer,\n",
    "                    T_max=params.get('num_epochs', 10),\n",
    "                    eta_min=params.get('min_lr', 1e-6)\n",
    "                )\n",
    "        \n",
    "        # Train model with fewer epochs for hyperparameter tuning\n",
    "        num_epochs = params.get('num_epochs', 10)\n",
    "        loss_type = params.get('loss_type', 'triplet')\n",
    "        embedding_size = params.get('embedding_size', 128)\n",
    "        \n",
    "        try:\n",
    "            # Train with minimal output\n",
    "            model, history = train_model(\n",
    "                model=model,\n",
    "                dataloaders=dataloaders,\n",
    "                loss_type=loss_type,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                num_epochs=num_epochs,\n",
    "                embedding_size=embedding_size\n",
    "            )\n",
    "            \n",
    "            # Simple evaluation to get a performance metric\n",
    "            train_embeddings, train_labels = extract_embeddings(model, dataloaders['eval_train'])\n",
    "            test_embeddings, test_labels = extract_embeddings(model, dataloaders['test'])\n",
    "            \n",
    "            # Verification Task\n",
    "            verification_results = evaluate_verification(test_embeddings, test_labels)\n",
    "            roc_auc = verification_results['roc_auc']\n",
    "            \n",
    "            # Retrieval Task\n",
    "            retrieval_results = evaluate_retrieval(\n",
    "                query_embeddings=test_embeddings,\n",
    "                query_labels=test_labels,\n",
    "                gallery_embeddings=train_embeddings,\n",
    "                gallery_labels=train_labels,\n",
    "                k_values=[1, 5]\n",
    "            )\n",
    "            recall_at_1 = retrieval_results['recall@1']\n",
    "            precision_at_1 = retrieval_results['precision@1']\n",
    "            \n",
    "            # Calculate overall score based on the chosen evaluation metric\n",
    "            if evaluation_metric == 'roc_auc':\n",
    "                score = roc_auc\n",
    "            elif evaluation_metric == 'recall':\n",
    "                score = recall_at_1\n",
    "            elif evaluation_metric == 'precision':\n",
    "                score = precision_at_1\n",
    "            else:  # combined\n",
    "                score = 0.4 * roc_auc + 0.4 * recall_at_1 + 0.2 * precision_at_1\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'params': params,\n",
    "                'roc_auc': roc_auc,\n",
    "                'recall@1': recall_at_1,\n",
    "                'precision@1': precision_at_1,\n",
    "                'score': score,\n",
    "                'val_loss': history['val_loss'][-1]\n",
    "            })\n",
    "            \n",
    "            print(f\"Trial {trial+1} results: ROC AUC = {roc_auc:.4f}, Recall@1 = {recall_at_1:.4f}, Score = {score:.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in trial {trial+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No successful trials!\")\n",
    "        return None\n",
    "        \n",
    "    # Find best parameters\n",
    "    results.sort(key=lambda x: x['score'], reverse=True)\n",
    "    best_result = results[0]\n",
    "    \n",
    "    print(\"\\n\\n============= Hyperparameter Tuning Results ==============\")\n",
    "    print(f\"Best score: {best_result['score']:.4f}\")\n",
    "    print(f\"Best parameters: {best_result['params']}\")\n",
    "    print(f\"Performance: ROC AUC = {best_result['roc_auc']:.4f}, Recall@1 = {best_result['recall@1']:.4f}\")\n",
    "    print(\"===========================================================\")\n",
    "    \n",
    "    # Plot validation loss curves for all trials\n",
    "    if len(results) > 1:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            if 'val_loss' in result and len(result['val_loss']) > 0:\n",
    "                plt.plot(range(1, len(result['val_loss'])+1), result['val_loss'], label=f\"Trial {i+1} (score={result['score']:.4f})\")\n",
    "        \n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Validation Loss')\n",
    "        plt.title('Hyperparameter Tuning - Validation Loss Curves')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig('hyperparameter_tuning_results.png')\n",
    "        plt.show()\n",
    "    \n",
    "    return best_result\n",
    "\n",
    "# Example usage of the hyperparameter tuning framework\n",
    "def run_hyperparameter_search():\n",
    "    param_grid = {\n",
    "        'backbone_name': ['resnet18', 'resnet34', 'efficientnet_b0', 'mobilenet_v2'],\n",
    "        'embedding_size': [64, 128, 256],\n",
    "        'batch_size': [16, 32, 64],\n",
    "        'loss_type': ['triplet', 'contrastive', 'arcface'],\n",
    "        'learning_rate': [1e-4, 5e-4, 1e-3],\n",
    "        'optimizer_name': ['adam', 'sgd'],\n",
    "        'weight_decay': [0, 1e-5, 1e-4],\n",
    "        'scheduler_name': ['step', 'cosine'],\n",
    "        'num_epochs': [10]  # Keep this fixed for faster tuning\n",
    "    }\n",
    "    \n",
    "    # Run hyperparameter tuning\n",
    "    best_params = hyperparameter_tuning(param_grid, n_trials=5, evaluation_metric='combined')\n",
    "    return best_params\n",
    "\n",
    "# For a more focused search with fewer parameters\n",
    "def focused_hyperparameter_search():\n",
    "    param_grid = {\n",
    "        'backbone_name': ['resnet18'],\n",
    "        'embedding_size': [128, 256],\n",
    "        'batch_size': [32],\n",
    "        'loss_type': ['triplet', 'contrastive'],\n",
    "        'learning_rate': [1e-4, 5e-4],\n",
    "        'optimizer_name': ['adam'],\n",
    "        'scheduler_name': ['step'],\n",
    "        'num_epochs': [10]\n",
    "    }\n",
    "    \n",
    "    # Run focused hyperparameter tuning\n",
    "    best_params = hyperparameter_tuning(param_grid, n_trials=4, evaluation_metric='combined')\n",
    "    return best_params\n",
    "\n",
    "# Uncomment to run hyperparameter tuning\n",
    "# best_params = run_hyperparameter_search()\n",
    "# best_params = focused_hyperparameter_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Main Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_few_shot(support_embeddings, support_labels, query_embeddings, query_labels, n_way=5, k_shot=5, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate the model on n-way k-shot classification with confidence intervals\n",
    "    \n",
    "    Args:\n",
    "        support_embeddings: Embeddings for the support set (training examples)\n",
    "        support_labels: Labels for the support set\n",
    "        query_embeddings: Embeddings for the query set (test examples)\n",
    "        query_labels: Labels for the query set\n",
    "        n_way: Number of classes in each episode\n",
    "        k_shot: Number of examples per class in the support set\n",
    "        num_episodes: Number of episodes to run for stable results\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing accuracy statistics and confidence intervals\n",
    "    \"\"\"\n",
    "    unique_labels = torch.unique(support_labels)\n",
    "    if len(unique_labels) < n_way:\n",
    "        print(f\"Warning: Only {len(unique_labels)} classes available, but n_way={n_way}\")\n",
    "        n_way = len(unique_labels)\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    # Progress bar for episodes\n",
    "    progress_bar = tqdm(range(num_episodes), desc=f\"{n_way}-way {k_shot}-shot evaluation\")\n",
    "    \n",
    "    for episode in progress_bar:\n",
    "        # Randomly select n classes for this episode\n",
    "        selected_classes = np.random.choice(unique_labels.numpy(), n_way, replace=False)\n",
    "        \n",
    "        # Create support set (k examples per class)\n",
    "        support_set_embeddings = []\n",
    "        support_set_labels = []\n",
    "        \n",
    "        for class_idx, c in enumerate(selected_classes):\n",
    "            # Get indices of examples of class c\n",
    "            class_indices = torch.where(support_labels == c)[0]\n",
    "            \n",
    "            # Randomly select k examples\n",
    "            if len(class_indices) >= k_shot:\n",
    "                selected_indices = np.random.choice(class_indices.numpy(), k_shot, replace=False)\n",
    "            else:\n",
    "                # If not enough examples, use all and repeat some\n",
    "                selected_indices = np.random.choice(class_indices.numpy(), k_shot, replace=True)\n",
    "            \n",
    "            for idx in selected_indices:\n",
    "                support_set_embeddings.append(support_embeddings[idx])\n",
    "                support_set_labels.append(class_idx)  # Use class index as the new label\n",
    "        \n",
    "        support_set_embeddings = torch.stack(support_set_embeddings)\n",
    "        support_set_labels = torch.tensor(support_set_labels)\n",
    "        \n",
    "        # Create query set (all examples of the selected classes from the query set)\n",
    "        query_set_indices = torch.tensor([i for i, label in enumerate(query_labels) if label in selected_classes])\n",
    "        \n",
    "        if len(query_set_indices) == 0:\n",
    "            print(\"Warning: No query examples for selected classes\")\n",
    "            continue\n",
    "            \n",
    "        query_set_embeddings = query_embeddings[query_set_indices]\n",
    "        query_set_labels = query_labels[query_set_indices]\n",
    "        \n",
    "        # Map original labels to new indices (0 to n_way-1)\n",
    "        label_mapping = {selected_classes[i]: i for i in range(n_way)}\n",
    "        query_set_labels = torch.tensor([label_mapping[label.item()] for label in query_set_labels])\n",
    "        \n",
    "        # Compute prototypes (mean embedding for each class)\n",
    "        prototypes = torch.zeros(n_way, support_embeddings.size(1), device=support_embeddings.device)\n",
    "        for c in range(n_way):\n",
    "            prototypes[c] = support_set_embeddings[support_set_labels == c].mean(0)\n",
    "        \n",
    "        # Compute distances between query examples and prototypes\n",
    "        # Using cosine similarity (higher means more similar)\n",
    "        logits = torch.matmul(query_set_embeddings, prototypes.T)\n",
    "        \n",
    "        # Make predictions\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        accuracy = (predictions == query_set_labels).float().mean().item()\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        # Update progress bar with current mean accuracy\n",
    "        progress_bar.set_postfix({'mean_acc': np.mean(accuracies):.4f})\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    \n",
    "    # Calculate 95% confidence interval using bootstrap\n",
    "    bootstrap_samples = 1000\n",
    "    bootstrap_means = []\n",
    "    \n",
    "    for _ in range(bootstrap_samples):\n",
    "        # Sample with replacement from the accuracies\n",
    "        bootstrap_sample = np.random.choice(accuracies, size=len(accuracies), replace=True)\n",
    "        bootstrap_means.append(np.mean(bootstrap_sample))\n",
    "    \n",
    "    # Calculate 95% confidence interval\n",
    "    conf_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "    \n",
    "    print(f\"{n_way}-way {k_shot}-shot classification:\")\n",
    "    print(f\"  Mean accuracy: {mean_accuracy:.4f}\")\n",
    "    print(f\"  Standard deviation: {std_accuracy:.4f}\")\n",
    "    print(f\"  95% confidence interval: [{conf_interval[0]:.4f}, {conf_interval[1]:.4f}]\")\n",
    "    \n",
    "    return {\n",
    "        'mean_accuracy': mean_accuracy,\n",
    "        'std_accuracy': std_accuracy,\n",
    "        'conf_interval': conf_interval,\n",
    "        'accuracies': accuracies\n",
    "    }\n",
    "\n",
    "def evaluate_multiple_few_shot_settings(support_embeddings, support_labels, query_embeddings, query_labels):\n",
    "    \"\"\"\n",
    "    Evaluate few-shot learning across multiple n-way, k-shot configurations\n",
    "    and visualize the results with error bars\n",
    "    \"\"\"\n",
    "    # Define the settings to evaluate\n",
    "    settings = [\n",
    "        {'n_way': 5, 'k_shot': 1, 'num_episodes': 100},  # 5-way 1-shot\n",
    "        {'n_way': 5, 'k_shot': 5, 'num_episodes': 100},  # 5-way 5-shot\n",
    "        {'n_way': 10, 'k_shot': 1, 'num_episodes': 100}, # 10-way 1-shot\n",
    "        {'n_way': 10, 'k_shot': 5, 'num_episodes': 100}  # 10-way 5-shot\n",
    "    ]\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Run evaluation for each setting\n",
    "    print(\"\\n=== Few-Shot Learning Evaluation ===\\n\")\n",
    "    for setting in settings:\n",
    "        print(f\"\\nEvaluating {setting['n_way']}-way {setting['k_shot']}-shot learning...\")\n",
    "        result = evaluate_few_shot(\n",
    "            support_embeddings=support_embeddings,\n",
    "            support_labels=support_labels,\n",
    "            query_embeddings=query_embeddings,\n",
    "            query_labels=query_labels,\n",
    "            n_way=setting['n_way'],\n",
    "            k_shot=setting['k_shot'],\n",
    "            num_episodes=setting['num_episodes']\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'n_way': setting['n_way'],\n",
    "            'k_shot': setting['k_shot'],\n",
    "            'mean_accuracy': result['mean_accuracy'],\n",
    "            'std_accuracy': result['std_accuracy'],\n",
    "            'conf_interval': result['conf_interval']\n",
    "        })\n",
    "    \n",
    "    # Create a visualization of the results with error bars\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Group by n_way for bar chart\n",
    "    n_way_values = sorted(list(set([r['n_way'] for r in results])))\n",
    "    k_shot_values = sorted(list(set([r['k_shot'] for r in results])))\n",
    "    \n",
    "    # Set up bar positions\n",
    "    bar_width = 0.35\n",
    "    x = np.arange(len(n_way_values))\n",
    "    \n",
    "    # Plot bars for each k_shot value\n",
    "    for i, k in enumerate(k_shot_values):\n",
    "        means = []\n",
    "        errors_lower = []\n",
    "        errors_upper = []\n",
    "        \n",
    "        for n in n_way_values:\n",
    "            # Find result for this n_way and k_shot\n",
    "            result = next((r for r in results if r['n_way'] == n and r['k_shot'] == k), None)\n",
    "            if result:\n",
    "                means.append(result['mean_accuracy'])\n",
    "                errors_lower.append(result['mean_accuracy'] - result['conf_interval'][0])\n",
    "                errors_upper.append(result['conf_interval'][1] - result['mean_accuracy'])\n",
    "            else:\n",
    "                means.append(0)\n",
    "                errors_lower.append(0)\n",
    "                errors_upper.append(0)\n",
    "        \n",
    "        # Plot bars with error bars\n",
    "        plt.bar(\n",
    "            x + (i - 0.5*(len(k_shot_values)-1)) * bar_width, \n",
    "            means, \n",
    "            bar_width, \n",
    "            yerr=[errors_lower, errors_upper],\n",
    "            label=f'{k}-shot',\n",
    "            capsize=5\n",
    "        )\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('Number of Classes (N-way)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Few-Shot Learning Performance')\n",
    "    plt.xticks(x, [f'{n}-way' for n in n_way_values])\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, result in enumerate(results):\n",
    "        n_way_idx = n_way_values.index(result['n_way'])\n",
    "        k_shot_idx = k_shot_values.index(result['k_shot'])\n",
    "        x_pos = n_way_idx + (k_shot_idx - 0.5*(len(k_shot_values)-1)) * bar_width\n",
    "        y_pos = result['mean_accuracy'] + 0.02\n",
    "        plt.text(x_pos, y_pos, f\"{result['mean_accuracy']:.3f}\", \n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('few_shot_evaluation.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Uncomment to find the best parameters using hyperparameter tuning\n",
    "    # best_params = focused_hyperparameter_search()\n",
    "    # if best_params:\n",
    "    #     backbone_name = best_params['params'].get('backbone_name', 'resnet18')\n",
    "    #     embedding_size = best_params['params'].get('embedding_size', 128)\n",
    "    #     batch_size = best_params['params'].get('batch_size', 32)\n",
    "    #     loss_type = best_params['params'].get('loss_type', 'triplet')\n",
    "    #     lr = best_params['params'].get('learning_rate', 1e-4)\n",
    "    #     print(f\"Using best parameters from hyperparameter tuning: {best_params['params']}\")\n",
    "    # else:\n",
    "    #     # Use default parameters if hyperparameter tuning failed\n",
    "    #     backbone_name = 'resnet18'\n",
    "    #     embedding_size = 128\n",
    "    #     batch_size = 32\n",
    "    #     loss_type = 'triplet'\n",
    "    #     lr = 1e-4\n",
    "    \n",
    "    # Model parameters (defaults)\n",
    "    backbone_name = 'resnet18'  # Options: 'resnet18', 'resnet34', 'resnet50', 'efficientnet_b0', 'mobilenet_v2', 'densenet121'\n",
    "    embedding_size = 128\n",
    "    batch_size = 32  # Adjust based on your GPU\n",
    "    num_workers = 2\n",
    "    \n",
    "    # Training parameters\n",
    "    loss_type = 'triplet'  # Options: 'triplet', 'contrastive', 'arcface'\n",
    "    num_epochs = 20\n",
    "    lr = 1e-4\n",
    "\n",
    "    # Load data\n",
    "    train_val_dataset, test_dataset, eval_train_dataset = load_oxford_pets_dataset()\n",
    "    datasets_dict = prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset)\n",
    "    dataloaders = create_dataloaders(datasets_dict, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    # Create model\n",
    "    model = EmbeddingNet(backbone_name=backbone_name, embedding_size=embedding_size)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create optimizer and scheduler\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    # Train model\n",
    "    model, history = train_model(\n",
    "        model=model,\n",
    "        dataloaders=dataloaders,\n",
    "        loss_type=loss_type,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=num_epochs,\n",
    "        embedding_size=embedding_size\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs+1), history['train_loss'], 'b-', label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs+1), history['val_loss'], 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training and Validation Loss ({loss_type} loss)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Extract embeddings for evaluation\n",
    "    print(\"\\nExtracting embeddings for evaluation...\")\n",
    "    train_embeddings, train_labels = extract_embeddings(model, dataloaders['eval_train'])\n",
    "    test_embeddings, test_labels = extract_embeddings(model, dataloaders['test'])\n",
    "    few_shot_train_embeddings, few_shot_train_labels = extract_embeddings(model, dataloaders['few_shot_train'])\n",
    "    few_shot_test_embeddings, few_shot_test_labels = extract_embeddings(model, dataloaders['few_shot_test'])\n",
    "    \n",
    "    # Evaluation tasks\n",
    "    print(\"\\n1. Verification Task:\")\n",
    "    verification_results = evaluate_verification(test_embeddings, test_labels)\n",
    "    \n",
    "    print(f\"\\nVerification Results:\")\n",
    "    print(f\"ROC AUC: {verification_results['roc_auc']:.4f}\")\n",
    "    print(f\"Equal Error Rate (EER): {verification_results['eer']:.4f}\")\n",
    "    \n",
    "    print(\"\\n2. Retrieval Task:\")\n",
    "    retrieval_results = evaluate_retrieval(\n",
    "        query_embeddings=test_embeddings,\n",
    "        query_labels=test_labels,\n",
    "        gallery_embeddings=train_embeddings,\n",
    "        gallery_labels=train_labels,\n",
    "        k_values=[1, 5, 10]\n",
    "    )\n",
    "    \n",
    "    print(\"\\n3. Few-shot Classification:\")\n",
    "    # Run comprehensive few-shot evaluation across multiple settings\n",
    "    few_shot_results = evaluate_multiple_few_shot_settings(\n",
    "        support_embeddings=few_shot_train_embeddings,\n",
    "        support_labels=few_shot_train_labels,\n",
    "        query_embeddings=few_shot_test_embeddings,\n",
    "        query_labels=few_shot_test_labels\n",
    "    )\n",
    "    \n",
    "    # Embedding visualization\n",
    "    print(\"\\n4. Embedding Visualization:\")\n",
    "    test_projection = visualize_embeddings(\n",
    "        embeddings=test_embeddings,\n",
    "        labels=test_labels,\n",
    "        class_mapping=datasets_dict['class_mapping'],\n",
    "        method='tsne',\n",
    "        title='t-SNE Visualization of Test Embeddings'\n",
    "    )\n",
    "    \n",
    "    # Visualize few-shot embeddings\n",
    "    print(\"\\nVisualizing few-shot embeddings:\")\n",
    "    # Combine few-shot train and test embeddings for visualization\n",
    "    all_few_shot_embeddings = torch.cat([few_shot_train_embeddings, few_shot_test_embeddings], dim=0)\n",
    "    all_few_shot_labels = torch.cat([few_shot_train_labels, few_shot_test_labels], dim=0)\n",
    "    \n",
    "    few_shot_projection = visualize_embeddings(\n",
    "        embeddings=all_few_shot_embeddings,\n",
    "        labels=all_few_shot_labels,\n",
    "        class_mapping=datasets_dict['class_mapping'],\n",
    "        method='tsne',\n",
    "        title='t-SNE Visualization of Few-Shot Embeddings'\n",
    "    )\n",
    "    \n",
    "    # Bonus: Grad-CAM Visualization\n",
    "    print(\"\\n5. Grad-CAM Visualization:\")\n",
    "    visualize_grad_cam(model, dataloaders['test'], datasets_dict['class_mapping'], num_images=3)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'embedding_size': embedding_size,\n",
    "        'backbone_name': backbone_name,\n",
    "        'class_mapping': datasets_dict['class_mapping']\n",
    "    }, f'pet_metric_learning_{backbone_name}_{loss_type}.pth')\n",
    "    \n",
    "    print(\"\\nEvaluation completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Bonus: Multiple Loss Function Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_loss_functions():\n",
    "    \"\"\"\n",
    "    Compare different loss functions for metric learning\n",
    "    \"\"\"\n",
    "    # Model parameters\n",
    "    backbone_name = 'resnet18'\n",
    "    embedding_size = 128\n",
    "    batch_size = 32\n",
    "    num_workers = 2\n",
    "    num_epochs = 15\n",
    "    \n",
    "    # Loss functions to compare\n",
    "    loss_types = ['triplet', 'contrastive', 'arcface']\n",
    "    \n",
    "    # Load data (only once)\n",
    "    train_val_dataset, test_dataset, eval_train_dataset = load_oxford_pets_dataset()\n",
    "    datasets_dict = prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset)\n",
    "    dataloaders = create_dataloaders(datasets_dict, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for loss_type in loss_types:\n",
    "        print(f\"\\n{'=' * 40}\")\n",
    "        print(f\"Training with {loss_type} loss\")\n",
    "        print(f\"{'=' * 40}\")\n",
    "        \n",
    "        # Create model\n",
    "        model = EmbeddingNet(backbone_name=backbone_name, embedding_size=embedding_size)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Create optimizer and scheduler\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "        \n",
    "        # Train model\n",
    "        model, history = train_model(\n",
    "            model=model,\n",
    "            dataloaders=dataloaders,\n",
    "            loss_type=loss_type,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            num_epochs=num_epochs,\n",
    "            embedding_size=embedding_size\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings for evaluation\n",
    "        print(\"\\nExtracting embeddings for evaluation...\")\n",
    "        train_embeddings, train_labels = extract_embeddings(model, dataloaders['eval_train'])\n",
    "        test_embeddings, test_labels = extract_embeddings(model, dataloaders['test'])\n",
    "        \n",
    "        # Evaluation\n",
    "        verification_results = evaluate_verification(test_embeddings, test_labels)\n",
    "        \n",
    "        retrieval_results = evaluate_retrieval(\n",
    "            query_embeddings=test_embeddings,\n",
    "            query_labels=test_labels,\n",
    "            gallery_embeddings=train_embeddings,\n",
    "            gallery_labels=train_labels,\n",
    "            k_values=[1, 5]\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[loss_type] = {\n",
    "            'verification': {\n",
    "                'roc_auc': verification_results['roc_auc'],\n",
    "                'eer': verification_results['eer']\n",
    "            },\n",
    "            'retrieval': {\n",
    "                'recall@1': retrieval_results['recall@1'],\n",
    "                'recall@5': retrieval_results['recall@5'],\n",
    "                'precision@1': retrieval_results['precision@1'],\n",
    "                'precision@5': retrieval_results['precision@5']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save model\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'embedding_size': embedding_size,\n",
    "            'backbone_name': backbone_name,\n",
    "            'class_mapping': datasets_dict['class_mapping']\n",
    "        }, f'pet_metric_learning_{backbone_name}_{loss_type}_comparison.pth')\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\n{'='*50}\")\n",
    "    print(\"Comparison of Loss Functions\")\n",
    "    print({'='*50})\n",
    "    \n",
    "    # Create a comparison table\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Loss Function': [],\n",
    "        'ROC AUC': [],\n",
    "        'EER': [],\n",
    "        'Recall@1': [],\n",
    "        'Recall@5': [],\n",
    "        'Precision@1': [],\n",
    "        'Precision@5': []\n",
    "    })\n",
    "    \n",
    "    for loss_type, metrics in results.items():\n",
    "        comparison_df = comparison_df.append({\n",
    "            'Loss Function': loss_type,\n",
    "            'ROC AUC': metrics['verification']['roc_auc'],\n",
    "            'EER': metrics['verification']['eer'],\n",
    "            'Recall@1': metrics['retrieval']['recall@1'],\n",
    "            'Recall@5': metrics['retrieval']['recall@5'],\n",
    "            'Precision@1': metrics['retrieval']['precision@1'],\n",
    "            'Precision@5': metrics['retrieval']['precision@5']\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    metrics = ['ROC AUC', 'Recall@1', 'Recall@5', 'Precision@1', 'Precision@5']\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, loss_type in enumerate(loss_types):\n",
    "        values = [\n",
    "            results[loss_type]['verification']['roc_auc'],\n",
    "            results[loss_type]['retrieval']['recall@1'],\n",
    "            results[loss_type]['retrieval']['recall@5'],\n",
    "            results[loss_type]['retrieval']['precision@1'],\n",
    "            results[loss_type]['retrieval']['precision@5']\n",
    "        ]\n",
    "        plt.bar(x + i*width, values, width, label=loss_type)\n",
    "    \n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Comparison of Loss Functions')\n",
    "    plt.xticks(x + width, metrics)\n",
    "    plt.legend()\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.savefig('loss_function_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return results, comparison_df\n",
    "\n",
    "# Uncomment to run the comparison\n",
    "# loss_comparison_results, loss_comparison_df = compare_loss_functions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Bonus: Streamlit Demo\n",
    "\n",
    "We've created a user-friendly Streamlit application that demonstrates the practical use of our trained metric learning model for pet similarity search. The application allows users to upload a pet image and find similar-looking pets based on the embeddings learned by our model.\n",
    "\n",
    "### Running the Streamlit App\n",
    "\n",
    "The complete Streamlit application code is available in the separate notebook file: `Charles_Watson_streamlit_pet_similarity_app.ipynb`.\n",
    "\n",
    "To run the demo:\n",
    "\n",
    "1. Ensure you have the necessary Python packages installed:\n",
    "   ```bash\n",
    "   pip install streamlit torch torchvision Pillow numpy tqdm matplotlib\n",
    "   ```\n",
    "\n",
    "2. Convert the Streamlit notebook to a Python script:\n",
    "   ```bash\n",
    "   jupyter nbconvert --to python Charles_Watson_streamlit_pet_similarity_app.ipynb\n",
    "   ```\n",
    "\n",
    "3. Make sure a trained model file (e.g., `pet_metric_learning_resnet18_triplet.pth`) exists in the working directory.\n",
    "\n",
    "4. Run the Streamlit application:\n",
    "   ```bash\n",
    "   streamlit run Charles_Watson_streamlit_pet_similarity_app.py\n",
    "   ```\n",
    "\n",
    "### Features of the Streamlit App\n",
    "\n",
    "- **Upload Interface**: Users can upload any pet image for similarity search\n",
    "- **Configuration Options**: Customize model path, database location, and number of results\n",
    "- **Interactive Results**: View similar pets with their breed labels and similarity scores\n",
    "- **Database Management**: Load and reload the pet image database as needed\n",
    "\n",
    "The app uses the same embedding model architecture (`EmbeddingNet`) as developed in this notebook (`Charles_Watson_Ndethi_Kibaki-Code.ipynb`), ensuring consistency between training and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we have implemented a comprehensive metric learning pipeline for pet breed classification using the Oxford-IIIT Pet Dataset. We have:\n",
    "\n",
    "1. Built a custom embedding model with a CNN backbone and projection head\n",
    "2. Implemented various loss functions for metric learning (Triplet, Contrastive, ArcFace)\n",
    "3. Developed evaluation methods for verification, retrieval, and few-shot classification\n",
    "4. Created visualization tools for embedding spaces and feature importance (Grad-CAM)\n",
    "5. Included bonus implementations for hard negative mining and loss function comparison\n",
    "6. Moved the Streamlit demo to a separate `pet_similarity_app.py` file.\n",
    "\n",
    "The code is modular and can be easily adapted for different settings and experimentation. To run the complete training and evaluation pipeline, simply execute the `main()` function."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
