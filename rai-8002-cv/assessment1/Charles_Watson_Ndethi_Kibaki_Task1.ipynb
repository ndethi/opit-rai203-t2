{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Building a CNN from Scratch for Pet Classification\n",
    "\n",
    "# 1. Introduction and Setup\n",
    "\n",
    "In this notebook, we'll build a CNN from scratch for the Oxford-IIIT Pet Dataset classification challenge. We'll start with binary classification (Dog vs. Cat) and then extend to fine-grained breed classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Set Up Environment and Seed for Reproducibility\n",
    "\n",
    "Setting a random seed ensures that our results are reproducible across different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Set deterministic backend\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed()\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Preprocessing\n",
    "\n",
    "Let's load and explore the Oxford-IIIT Pet Dataset, which contains images of 37 pet breeds (25 dog breeds and 12 cat breeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://thor.robots.ox.ac.uk/pets/images.tar.gz to data/oxford-iiit-pet/images.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 791918971/791918971 [04:38<00:00, 2841509.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/oxford-iiit-pet/images.tar.gz to data/oxford-iiit-pet\n",
      "Downloading https://thor.robots.ox.ac.uk/pets/annotations.tar.gz to data/oxford-iiit-pet/annotations.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19173078/19173078 [00:16<00:00, 1196913.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/oxford-iiit-pet/annotations.tar.gz to data/oxford-iiit-pet\n",
      "Total Train+Validation size: 3680\n",
      "Total Test size: 3669\n",
      "Number of classes: 37\n",
      "Dog classes (29): ['American Bulldog', 'American Pit Bull Terrier', 'Basset Hound', 'Beagle', 'Boxer', 'British Shorthair', 'Chihuahua', 'Egyptian Mau', 'English Cocker Spaniel', 'English Setter', 'German Shorthaired', 'Great Pyrenees', 'Havanese', 'Japanese Chin', 'Keeshond', 'Leonberger', 'Maine Coon', 'Miniature Pinscher', 'Newfoundland', 'Pomeranian', 'Pug', 'Russian Blue', 'Saint Bernard', 'Samoyed', 'Scottish Terrier', 'Shiba Inu', 'Staffordshire Bull Terrier', 'Wheaten Terrier', 'Yorkshire Terrier']\n",
      "Cat classes (8): ['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'Persian', 'Ragdoll', 'Siamese', 'Sphynx']\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load train+validation and test sets\n",
    "trainval_dataset = datasets.OxfordIIITPet(root=\"./data\", split=\"trainval\", transform=transform, download=True)\n",
    "test_dataset = datasets.OxfordIIITPet(root=\"./data\", split=\"test\", transform=transform, download=True)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Total Train+Validation size: {len(trainval_dataset)}\")  # Should be 3,680\n",
    "print(f\"Total Test size: {len(test_dataset)}\")  # Should be 3,669\n",
    "\n",
    "# Explore dataset classes\n",
    "class_to_idx = trainval_dataset.class_to_idx\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "print(f\"Number of classes: {len(class_to_idx)}\")\n",
    "\n",
    "# Check which classes are dogs vs cats\n",
    "dog_classes = [name for name in class_to_idx.keys() if name not in [\"Abyssinian\", \"Bengal\", \"Birman\", \"Bombay\", \"British_Shorthair\", \n",
    "                                                                  \"Egyptian_Mau\", \"Maine_Coon\", \"Persian\", \"Ragdoll\", \n",
    "                                                                  \"Russian_Blue\", \"Siamese\", \"Sphynx\"]]\n",
    "cat_classes = [name for name in class_to_idx.keys() if name not in dog_classes]\n",
    "\n",
    "print(f\"Dog classes ({len(dog_classes)}): {dog_classes}\")\n",
    "print(f\"Cat classes ({len(cat_classes)}): {cat_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create Binary Classification Dataset (Dogs vs. Cats)\n",
    "\n",
    "For the first task, we need to convert the 37 breed labels into binary labels (Dog vs. Cat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary classification dataset (Dog vs. Cat)\n",
    "class BinaryPetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        # Oxford dataset has first 25 classes as dogs (0-24) and last 12 as cats (25-36)\n",
    "        self.dog_indices = list(range(25))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        # Convert multi-class label to binary: Dogs (1) vs. Cats (0)\n",
    "        binary_label = 1 if label in self.dog_indices else 0\n",
    "        return img, binary_label\n",
    "\n",
    "# Create binary versions of datasets\n",
    "binary_trainval_dataset = BinaryPetDataset(trainval_dataset)\n",
    "binary_test_dataset = BinaryPetDataset(test_dataset)\n",
    "\n",
    "# Split trainval into train and validation (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(binary_trainval_dataset))\n",
    "val_size = len(binary_trainval_dataset) - train_size\n",
    "binary_train_dataset, binary_val_dataset = random_split(binary_trainval_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "binary_train_loader = DataLoader(binary_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "binary_val_loader = DataLoader(binary_val_dataset, batch_size=batch_size)\n",
    "binary_test_loader = DataLoader(binary_test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Binary train set size: {len(binary_train_dataset)}\")\n",
    "print(f\"Binary validation set size: {len(binary_val_dataset)}\")\n",
    "print(f\"Binary test set size: {len(binary_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Implement Data Augmentation\n",
    "\n",
    "Data augmentation helps improve model generalization by artificially expanding the training set through transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations with augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define transformations for validation/testing (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Reload datasets with augmentation\n",
    "trainval_dataset_aug = datasets.OxfordIIITPet(root=\"./data\", split=\"trainval\", transform=train_transform, download=False)\n",
    "test_dataset_aug = datasets.OxfordIIITPet(root=\"./data\", split=\"test\", transform=val_transform, download=False)\n",
    "\n",
    "# Create augmented binary datasets\n",
    "binary_trainval_dataset_aug = BinaryPetDataset(trainval_dataset_aug)\n",
    "binary_test_dataset_aug = BinaryPetDataset(test_dataset_aug)\n",
    "\n",
    "# Split trainval into train and validation\n",
    "binary_train_dataset_aug, binary_val_dataset_aug = random_split(binary_trainval_dataset_aug, [train_size, val_size])\n",
    "\n",
    "# Create augmented data loaders\n",
    "binary_train_loader_aug = DataLoader(binary_train_dataset_aug, batch_size=batch_size, shuffle=True)\n",
    "binary_val_loader_aug = DataLoader(binary_val_dataset_aug, batch_size=batch_size)\n",
    "binary_test_loader_aug = DataLoader(binary_test_dataset_aug, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Augmented Samples\n",
    "\n",
    "Let's visualize some augmented samples to see the effect of our data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some augmented samples\n",
    "def show_augmented_samples(dataloader, num_samples=5):\n",
    "    # Get a batch of images and labels\n",
    "    images, labels = next(iter(dataloader))\n",
    "    \n",
    "    # Display images\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
    "    for i in range(num_samples):\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        # Denormalize for visualization\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Label: {'Dog' if labels[i] == 1 else 'Cat'}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_augmented_samples(binary_train_loader_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Task 1 - Step 1: Binary Classification (Dog vs. Cat)\n",
    "\n",
    "First, we'll build a CNN model for binary classification between dogs and cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define CNN Model Architecture\n",
    "\n",
    "We'll design a custom CNN architecture with convolutional layers, batch normalization, pooling, and dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryPetCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryPetCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv block 1\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        # Conv block 2\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        # Conv block 3\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        # Conv block 4\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 256 * 14 * 14)\n",
    "        \n",
    "        # Fully connected with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Return sigmoid for binary classification\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Initialize the model and move to device\n",
    "binary_model = BinaryPetCNN().to(device)\n",
    "print(binary_model)\n",
    "\n",
    "# Calculate number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of trainable parameters: {count_parameters(binary_model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define Loss Function, Optimizer, and Training Loop\n",
    "\n",
    "We'll use Binary Cross Entropy Loss for binary classification, Adam optimizer, and implement a training loop with learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross Entropy Loss for binary classification\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Adam optimizer with learning rate and weight decay (L2 regularization)\n",
    "optimizer = optim.Adam(binary_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler to reduce LR when validation loss plateaus\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# Training function\n",
    "def train_binary_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20):\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        # Iterate over data\n",
    "        for inputs, labels in binary_train_loader_aug:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().to(device).view(-1, 1)  # Shape for BCE loss\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            running_corrects += torch.sum(preds == labels)\n",
    "        \n",
    "        epoch_loss = running_loss / len(binary_train_loader_aug.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(binary_train_loader_aug.dataset)\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        \n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        \n",
    "        # No gradient computation for validation\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in binary_val_loader_aug:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.float().to(device).view(-1, 1)\n",
    "                \n",
    "                # Forward\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Statistics\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                preds = (outputs > 0.5).float()\n",
    "                val_running_corrects += torch.sum(preds == labels)\n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(binary_val_loader_aug.dataset)\n",
    "        val_epoch_acc = val_running_corrects.double() / len(binary_val_loader_aug.dataset)\n",
    "        \n",
    "        history['val_loss'].append(val_epoch_loss)\n",
    "        history['val_acc'].append(val_epoch_acc.item())\n",
    "        \n",
    "        print(f'Val Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}')\n",
    "        \n",
    "        # Update learning rate based on validation loss\n",
    "        scheduler.step(val_epoch_loss)\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            torch.save(model.state_dict(), 'best_binary_model.pth')\n",
    "    \n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_val_acc:.4f}')\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load('best_binary_model.pth'))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Train the Binary Classification Model\n",
    "\n",
    "Now let's train our binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trained_binary_model, binary_history = train_binary_model(\n",
    "    binary_model,\n",
    "    binary_train_loader_aug, \n",
    "    binary_val_loader_aug, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    num_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Visualize Training Results\n",
    "\n",
    "Let's visualize the training and validation loss/accuracy curves to analyze our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss/accuracy\n",
    "def plot_training_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['train_loss'], label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['train_acc'], label='Train Accuracy')\n",
    "    ax2.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(binary_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Evaluate Binary Classification Model on Test Set\n",
    "\n",
    "Now that we've trained our model, let's evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary_model(model, test_loader):\n",
    "    model.eval()\n",
    "    running_corrects = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).float().view(-1)\n",
    "            \n",
    "            # Collect predictions and labels for confusion matrix\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Count correct predictions\n",
    "            running_corrects += torch.sum(preds == labels)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = running_corrects.double() / len(test_loader.dataset)\n",
    "    print(f'Test Accuracy: {acc:.4f}')\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 scores\n",
    "    report = classification_report(all_labels, all_preds, target_names=['Cat', 'Dog'])\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    classes = ['Cat', 'Dog']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations to confusion matrix\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return acc, cm, report\n",
    "\n",
    "# Evaluate the binary model on test set\n",
    "binary_test_acc, binary_cm, binary_report = evaluate_binary_model(trained_binary_model, binary_test_loader_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Task 1 - Step 2: Fine-Grained Breed Classification\n",
    "\n",
    "Now, we'll extend our model to classify the 37 specific pet breeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Create Multi-class Dataset Loaders\n",
    "\n",
    "We'll create data loaders for the multi-class classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-class data loaders (using augmentation)\n",
    "multi_train_dataset = datasets.OxfordIIITPet(root=\"./data\", split=\"trainval\", transform=train_transform, download=False)\n",
    "multi_test_dataset = datasets.OxfordIIITPet(root=\"./data\", split=\"test\", transform=val_transform, download=False)\n",
    "\n",
    "# Split trainval into train and validation\n",
    "train_size = int(0.8 * len(multi_train_dataset))\n",
    "val_size = len(multi_train_dataset) - train_size\n",
    "multi_train_dataset, multi_val_dataset = random_split(multi_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "multi_train_loader = DataLoader(multi_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "multi_val_loader = DataLoader(multi_val_dataset, batch_size=batch_size)\n",
    "multi_test_loader = DataLoader(multi_test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Multi-class train set size: {len(multi_train_dataset)}\")\n",
    "print(f\"Multi-class validation set size: {len(multi_val_dataset)}\")\n",
    "print(f\"Multi-class test set size: {len(multi_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Define Multi-class CNN Architecture\n",
    "\n",
    "We'll modify our CNN architecture to handle 37 classes instead of binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassPetCNN(nn.Module):\n",
    "    def __init__(self, num_classes=37):\n",
    "        super(MultiClassPetCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(512 * 7 * 7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv block 1\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        # Conv block 2\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        # Conv block 3\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        # Conv block 4\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        # Conv block 5\n",
    "        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 512 * 7 * 7)\n",
    "        \n",
    "        # Fully connected with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the model and move to device\n",
    "multi_model = MultiClassPetCNN().to(device)\n",
    "print(multi_model)\n",
    "\n",
    "# Calculate number of parameters\n",
    "print(f\"Number of trainable parameters: {count_parameters(multi_model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Define Loss Function, Optimizer, and Training Loop for Multi-class Model\n",
    "\n",
    "For multi-class classification, we'll use Cross Entropy Loss instead of BCE Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss for multi-class classification\n",
    "multi_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer with learning rate and weight decay\n",
    "multi_optimizer = optim.Adam(multi_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "multi_scheduler = optim.lr_scheduler.ReduceLROnPlateau(multi_optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# Training function for multi-class model\n",
    "def train_multi_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        # Iterate over data\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        \n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        \n",
    "        # No gradient computation for validation\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Statistics\n",
    "                val_running_loss += loss.item() * inputs.size(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rai7004",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
