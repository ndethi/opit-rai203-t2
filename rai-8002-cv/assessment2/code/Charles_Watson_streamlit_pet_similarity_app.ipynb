{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit Pet Breed Similarity Search App\n",
    "\n",
    "This Streamlit application allows users to upload a pet image and find similar-looking pets from a pre-loaded database using a trained deep metric learning model.\n",
    "\n",
    "## Running the Streamlit App\n",
    "\n",
    "To run this Streamlit app:\n",
    "1. Ensure you have the necessary Python packages installed (`streamlit`, `torch`, `torchvision`, `Pillow`, `numpy`, `tqdm`, `faiss-cpu` or `faiss-gpu`, `scikit-learn`, `matplotlib`, `umap-learn`).\n",
    "2. Make sure you have the trained model file (e.g., `pet_metric_learning_resnet18_triplet.pth`) and the pet image database (e.g., `./data/oxford-iiit-pet/images`) available in the correct paths relative to where you run the command. You can configure these paths in the application's sidebar.\n",
    "3. Open your terminal in the directory containing this notebook.\n",
    "4. Convert this notebook to a Python script: `jupyter nbconvert --to python streamlit_pet_similarity_app.ipynb`\n",
    "5. Run the command: `streamlit run streamlit_pet_similarity_app.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pet Similarity Search - Streamlit Application\n",
    "\n",
    "import streamlit as st\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights, ResNet50_Weights\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.manifold import TSNE\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the EmbeddingNet class here (copied from the notebook)\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet18', embedding_size=128, pretrained=True):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        if backbone_name == 'resnet18':\n",
    "            weights = ResNet18_Weights.DEFAULT if pretrained else None\n",
    "            self.backbone = models.resnet18(weights=weights)\n",
    "            backbone_output_size = 512\n",
    "        elif backbone_name == 'resnet50':\n",
    "            weights = ResNet50_Weights.DEFAULT if pretrained else None\n",
    "            self.backbone = models.resnet50(weights=weights)\n",
    "            backbone_output_size = 2048\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
    "\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(backbone_output_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        embeddings = self.projection_head(features)\n",
    "        normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return normalized_embeddings\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "# Load the trained model\n",
    "@st.cache_resource\n",
    "def load_model(model_path='pet_metric_learning_resnet18_triplet.pth'):\n",
    "    if not os.path.exists(model_path):\n",
    "        st.error(f\"Model file not found at {model_path}. Please ensure the trained model is in the correct location.\")\n",
    "        return None, None, None\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "        model = EmbeddingNet(\n",
    "            backbone_name=checkpoint.get('backbone_name', 'resnet18'),\n",
    "            embedding_size=checkpoint.get('embedding_size', 128),\n",
    "            pretrained=False\n",
    "        )\n",
    "\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        class_mapping = checkpoint.get('class_mapping', {'idx_to_class': {}})\n",
    "        embedding_size = checkpoint.get('embedding_size', 128)\n",
    "\n",
    "        return model, class_mapping, embedding_size\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading model: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Enhanced preprocessing with data augmentation options\n",
    "def get_transform(augment=False):\n",
    "    if not augment:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# Function to compute embedding for an image\n",
    "def get_embedding(model, image, augment=False):\n",
    "    transform = get_transform(augment=augment)\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image_tensor)\n",
    "\n",
    "    return embedding.cpu()\n",
    "\n",
    "# Enhanced function to process images with multiple augmentations and aggregate\n",
    "def get_robust_embedding(model, image, num_augmentations=5):\n",
    "    embeddings = []\n",
    "    embeddings.append(get_embedding(model, image, augment=False))\n",
    "    for _ in range(num_augmentations):\n",
    "        embeddings.append(get_embedding(model, image, augment=True))\n",
    "    avg_embedding = torch.mean(torch.cat(embeddings, dim=0), dim=0, keepdim=True)\n",
    "    return F.normalize(avg_embedding, p=2, dim=1)\n",
    "\n",
    "# Function to load a single image and process it\n",
    "def load_and_process_image(args):\n",
    "    img_path, transform, model = args\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "        label = os.path.basename(img_path).split('_')[0]\n",
    "        return img, img_path, label, img_tensor\n",
    "    except Exception as e:\n",
    "        return None, img_path, None, None\n",
    "\n",
    "# Function to compute embeddings in batches\n",
    "def compute_embeddings_batch(model, image_tensors):\n",
    "    with torch.no_grad():\n",
    "        return model(image_tensors)\n",
    "\n",
    "# Function to find similar pets using FAISS\n",
    "def find_similar_pets_faiss(query_embedding, faiss_index, database_images, database_labels, top_k=5):\n",
    "    if faiss_index is None or len(database_images) == 0:\n",
    "        return [], [], []\n",
    "    query_np = query_embedding.numpy().astype('float32')\n",
    "    if len(query_np.shape) == 2 and query_np.shape[0] == 1:\n",
    "        query_np = query_np.reshape(-1)\n",
    "    query_np = query_np.reshape(1, -1)\n",
    "    similarities, indices = faiss_index.search(query_np, k=min(top_k, len(database_labels)))\n",
    "    indices = indices[0].tolist()\n",
    "    similarities = similarities[0].tolist()\n",
    "    similar_images = [database_images[i] for i in indices]\n",
    "    similar_labels = [database_labels[i] for i in indices]\n",
    "    return similar_images, similar_labels, similarities\n",
    "\n",
    "# Load database images and compute embeddings - optimized version\n",
    "@st.cache_data\n",
    "def load_database(directory, _model, embedding_size, batch_size=32, max_db_size=500, use_parallel=True):\n",
    "    start_time = time.time()\n",
    "    transform = get_transform()\n",
    "    image_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_files.append(os.path.join(root, file))\n",
    "    random.shuffle(image_files)\n",
    "    image_files = image_files[:max_db_size]\n",
    "    images = []\n",
    "    labels = []\n",
    "    file_paths = []\n",
    "    batched_tensors = []\n",
    "    st.write(f\"Found {len(image_files)} images. Processing up to {max_db_size}...\")\n",
    "    progress_bar = st.progress(0)\n",
    "    if use_parallel:\n",
    "        args_list = [(img_path, transform, _model) for img_path in image_files]\n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=min(os.cpu_count(), 8)) as executor:\n",
    "            futures = {executor.submit(load_and_process_image, args): i for i, args in enumerate(args_list)}\n",
    "            for i, future in enumerate(as_completed(futures)):\n",
    "                result = future.result()\n",
    "                if result[0] is not None:\n",
    "                    img, path, label, tensor = result\n",
    "                    images.append(img)\n",
    "                    file_paths.append(path)\n",
    "                    labels.append(label)\n",
    "                    batched_tensors.append(tensor)\n",
    "                progress_bar.progress((i + 1) / len(image_files))\n",
    "    else:\n",
    "        for i, img_path in enumerate(tqdm(image_files, desc=\"Loading Database\")):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_tensor = transform(img).unsqueeze(0)\n",
    "                label = os.path.basename(img_path).split('_')[0]\n",
    "                images.append(img)\n",
    "                file_paths.append(img_path)\n",
    "                labels.append(label)\n",
    "                batched_tensors.append(img_tensor)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            progress_bar.progress((i + 1) / len(image_files))\n",
    "    all_embeddings = []\n",
    "    _model = _model.to(device)\n",
    "    batch_idx = 0\n",
    "    while batch_idx < len(batched_tensors):\n",
    "        end_idx = min(batch_idx + batch_size, len(batched_tensors))\n",
    "        batch = torch.cat(batched_tensors[batch_idx:end_idx], dim=0).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = _model(batch).cpu()\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "        batch_idx = end_idx\n",
    "    if all_embeddings:\n",
    "        embeddings_tensor = torch.cat(all_embeddings, dim=0)\n",
    "    else:\n",
    "        embeddings_tensor = torch.tensor([])\n",
    "    if len(embeddings_tensor) > 0:\n",
    "        embeddings_np = embeddings_tensor.numpy().astype('float32')\n",
    "        faiss_index = faiss.IndexFlatIP(embedding_size)\n",
    "        faiss_index.add(embeddings_np)\n",
    "    else:\n",
    "        faiss_index = None\n",
    "    progress_bar.empty()\n",
    "    load_time = time.time() - start_time\n",
    "    breed_distribution = dict(Counter(labels))\n",
    "    stats = {\n",
    "        'num_images': len(images),\n",
    "        'num_breeds': len(breed_distribution),\n",
    "        'breed_distribution': breed_distribution,\n",
    "        'load_time': load_time\n",
    "    }\n",
    "    return images, embeddings_tensor, labels, file_paths, faiss_index, stats\n",
    "\n",
    "# Function to visualize embeddings with t-SNE\n",
    "def plot_tsne(embeddings, labels, query_embedding=None, n_components=2):\n",
    "    if len(embeddings) == 0:\n",
    "        return None\n",
    "    if query_embedding is not None:\n",
    "        query_embedding_np = query_embedding.cpu().numpy()\n",
    "        all_embeddings = np.vstack([query_embedding_np, embeddings.cpu().numpy()])\n",
    "        is_query = np.zeros(len(all_embeddings), dtype=bool)\n",
    "        is_query[0] = True\n",
    "    else:\n",
    "        all_embeddings = embeddings.cpu().numpy()\n",
    "        is_query = None\n",
    "    tsne = TSNE(n_components=n_components, random_state=42, perplexity=min(30, max(5, len(all_embeddings)//10)))\n",
    "    embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    if is_query is not None:\n",
    "        scatter = ax.scatter(\n",
    "            embeddings_2d[~is_query, 0],\n",
    "            embeddings_2d[~is_query, 1],\n",
    "            c=[hash(label) % 100 for label in labels],\n",
    "            cmap='viridis',\n",
    "            alpha=0.6,\n",
    "            s=50\n",
    "        )\n",
    "        ax.scatter(\n",
    "            embeddings_2d[is_query, 0],\n",
    "            embeddings_2d[is_query, 1],\n",
    "            color='red',\n",
    "            s=200,\n",
    "            marker='*',\n",
    "            edgecolors='black',\n",
    "            label='Query'\n",
    "        )\n",
    "    else:\n",
    "        scatter = ax.scatter(\n",
    "            embeddings_2d[:, 0],\n",
    "            embeddings_2d[:, 1],\n",
    "            c=[hash(label) % 100 for label in labels],\n",
    "            cmap='viridis',\n",
    "            alpha=0.6,\n",
    "            s=50\n",
    "        )\n",
    "    ax.set_title('t-SNE Visualization of Pet Embeddings')\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    unique_labels = set(labels)\n",
    "    for label in unique_labels:\n",
    "        indices = [i for i, l in enumerate(labels) if l == label]\n",
    "        if indices:\n",
    "            idx = indices[0]\n",
    "            if is_query is not None:\n",
    "                idx_2d = idx + 1\n",
    "            else:\n",
    "                idx_2d = idx\n",
    "            ax.annotate(\n",
    "                label,\n",
    "                (embeddings_2d[idx_2d, 0], embeddings_2d[idx_2d, 1]),\n",
    "                textcoords='offset points',\n",
    "                xytext=(5, 5),\n",
    "                ha='center',\n",
    "                fontsize=8,\n",
    "                bbox=dict(boxstyle='round,pad=0.3', fc='yellow', alpha=0.3)\n",
    "            )\n",
    "    return fig\n",
    "\n",
    "# Function to save results for later analysis\n",
    "def save_results(query_image, similar_images, similar_labels, similarities, filename='similarity_results.json'):\n",
    "    results_dir = Path('results')\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    query_path = results_dir / 'query_image.jpg'\n",
    "    query_image.save(query_path)\n",
    "    similar_paths = []\n",
    "    for i, img in enumerate(similar_images):\n",
    "        img_path = results_dir / f'similar_{i}.jpg'\n",
    "        img.save(img_path)\n",
    "        similar_paths.append(str(img_path))\n",
    "    results = {\n",
    "        'query_image': str(query_path),\n",
    "        'similar_images': similar_paths,\n",
    "        'similar_labels': similar_labels,\n",
    "        'similarities': similarities,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    with open(results_dir / filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    return str(results_dir / filename)\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(query_label, similar_labels, top_k=5):\n",
    "    similar_labels = similar_labels[:top_k]\n",
    "    matches = [1 if label == query_label else 0 for label in similar_labels]\n",
    "    precision_at_k = sum(matches) / len(similar_labels) if similar_labels else 0\n",
    "    try:\n",
    "        first_match = matches.index(1) + 1\n",
    "    except ValueError:\n",
    "        first_match = \"Not found\"\n",
    "    if 1 in matches:\n",
    "        mrr = 1.0 / (matches.index(1) + 1)\n",
    "    else:\n",
    "        mrr = 0.0\n",
    "    return {\n",
    "        'precision@k': precision_at_k,\n",
    "        'first_match': first_match,\n",
    "        'mrr': mrr,\n",
    "        'matches': matches\n",
    "    }\n",
    "\n",
    "# Main Streamlit app\n",
    "def run_app():\n",
    "    st.set_page_config(layout=\"wide\")\n",
    "    st.title(\"🐾 Pet Breed Similarity Search 🐾\")\n",
    "    st.write(\"Upload a pet image to find similar-looking pets in our database!\")\n",
    "    st.sidebar.header(\"System Info\")\n",
    "    st.sidebar.write(f\"Using device: {device}\")\n",
    "    st.sidebar.header(\"Configuration\")\n",
    "    model_file = st.sidebar.text_input(\"Model File Path\", \"pet_metric_learning_resnet18_triplet.pth\")\n",
    "    database_dir = st.sidebar.text_input(\"Database Directory Path\", \"./data/oxford-iiit-pet/images\")\n",
    "    top_k_similar = st.sidebar.slider(\"Number of Similar Pets to Show\", 1, 20, 5)\n",
    "    use_parallel = st.sidebar.checkbox(\"Use Parallel Processing\", value=True)\n",
    "    max_db_size = st.sidebar.number_input(\"Max Database Size\", min_value=10, max_value=5000, value=500, step=50)\n",
    "    batch_size = st.sidebar.selectbox(\"Batch Size\", options=[8, 16, 32, 64], index=1)\n",
    "    st.sidebar.header(\"Robustness Options\")\n",
    "    use_augmentation = st.sidebar.checkbox(\"Use Data Augmentation\", value=False)\n",
    "    num_augmentations = st.sidebar.slider(\"Number of Augmentations\", 1, 10, 5, disabled=not use_augmentation)\n",
    "    model, class_mapping, embedding_size = load_model(model_file)\n",
    "    if model is None:\n",
    "        st.stop()\n",
    "    idx_to_class = class_mapping.get('idx_to_class', {})\n",
    "    st.sidebar.header(\"Database\")\n",
    "    if st.sidebar.button(\"Load/Reload Database\"):\n",
    "        st.cache_data.clear()\n",
    "        st.experimental_rerun()\n",
    "    if not os.path.isdir(database_dir):\n",
    "        st.warning(f\"Database directory not found: '{database_dir}'. Please provide a valid path in the sidebar.\")\n",
    "        database_images, database_embeddings, database_labels, file_paths, faiss_index, stats = [], torch.tensor([]), [], [], None, {\n",
    "            'num_images': 0,\n",
    "            'num_breeds': 0,\n",
    "            'breed_distribution': {},\n",
    "            'load_time': 0\n",
    "        }\n",
    "    else:\n",
    "        with st.spinner(f\"Loading database from '{database_dir}'... This might take a while.\"):\n",
    "            database_images, database_embeddings, database_labels, file_paths, faiss_index, stats = load_database(\n",
    "                database_dir, model, embedding_size, batch_size=batch_size, max_db_size=max_db_size, use_parallel=use_parallel\n",
    "            )\n",
    "        if not database_images:\n",
    "            st.error(\"No images loaded from the database directory. Check the path and image files.\")\n",
    "        else:\n",
    "            st.sidebar.success(f\"Loaded {stats['num_images']} images from {stats['num_breeds']} breeds in {stats['load_time']:.2f}s.\")\n",
    "    tab1, tab2, tab3 = st.tabs([\"Similarity Search\", \"Database Stats\", \"Embedding Visualization\"])\n",
    "    with tab1:\n",
    "        col1, col2 = st.columns([1, 2])\n",
    "        with col1:\n",
    "            st.header(\"Query Image\")\n",
    "            uploaded_file = st.file_uploader(\"Choose a pet image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "            if uploaded_file is not None:\n",
    "                query_image = Image.open(uploaded_file).convert('RGB')\n",
    "                st.image(query_image, caption=\"Your Query Image\", use_column_width=True)\n",
    "                query_label = st.text_input(\"Optional: Enter the breed of this pet for evaluation metrics\", \"\")\n",
    "        with col2:\n",
    "            st.header(\"Similar Pets Found\")\n",
    "            if uploaded_file is not None and len(database_images) > 0:\n",
    "                if st.button(\"Find Similar Pets\"):\n",
    "                    with st.spinner(\"Comparing your pet...\"):\n",
    "                        start_time = time.time()\n",
    "                        if use_augmentation:\n",
    "                            query_embedding = get_robust_embedding(model, query_image, num_augmentations)\n",
    "                        else:\n",
    "                            query_embedding = get_embedding(model, query_image)\n",
    "                        similar_images, similar_labels, similarities = find_similar_pets_faiss(\n",
    "                            query_embedding, faiss_index, database_images, database_labels, top_k=top_k_similar\n",
    "                        )\n",
    "                        search_time = time.time() - start_time\n",
    "                        if not similar_images:\n",
    "                            st.warning(\"Could not find any similar pets.\")\n",
    "                        else:\n",
    "                            st.success(f\"Found {len(similar_images)} similar pets in {search_time:.3f} seconds.\")\n",
    "                            num_results = len(similar_images)\n",
    "                            num_cols = min(5, num_results)\n",
    "                            rows = (num_results + num_cols - 1) // num_cols\n",
    "                            for row in range(rows):\n",
    "                                cols = st.columns(num_cols)\n",
    "                                for col_idx in range(num_cols):\n",
    "                                    idx = row * num_cols + col_idx\n",
    "                                    if idx < num_results:\n",
    "                                        with cols[col_idx]:\n",
    "                                            st.image(similar_images[idx], caption=f\"{similar_labels[idx].replace('_', ' ').title()}\", use_column_width=True)\n",
    "                                            st.write(f\"Similarity: {similarities[idx]:.3f}\")\n",
    "                            if query_label and query_label.strip():\n",
    "                                metrics = compute_metrics(query_label.strip().lower(), [label.lower() for label in similar_labels], top_k=len(similar_labels))\n",
    "                                st.subheader(\"Evaluation Metrics\")\n",
    "                                metrics_cols = st.columns(3)\n",
    "                                metrics_cols[0].metric(\"Precision@k\", f\"{metrics['precision@k']:.2f}\")\n",
    "                                metrics_cols[1].metric(\"First Match Position\", metrics['first_match'])\n",
    "                                metrics_cols[2].metric(\"Mean Reciprocal Rank\", f\"{metrics['mrr']:.2f}\")\n",
    "                                match_html = \"<div style='display:flex;'>\"\n",
    "                                for i, match in enumerate(metrics['matches']):\n",
    "                                    color = \"green\" if match else \"red\"\n",
    "                                    match_html += f\"<div style='margin-right:5px;width:20px;height:20px;background:{color};'></div>\"\n",
    "                                match_html += \"</div>\"\n",
    "                                st.write(\"Match Pattern:\")\n",
    "                                st.markdown(match_html, unsafe_allow_html=True)\n",
    "                            if st.button(\"Save Results\"):\n",
    "                                result_path = save_results(query_image, similar_images, similar_labels, similarities)\n",
    "                                st.success(f\"Results saved to {result_path}\")\n",
    "                            st.subheader(\"Embedding Visualization\")\n",
    "                            with st.spinner(\"Generating t-SNE visualization...\"):\n",
    "                                tsne_fig = plot_tsne(database_embeddings, database_labels, query_embedding)\n",
    "                                if tsne_fig:\n",
    "                                    st.pyplot(tsne_fig)\n",
    "                                else:\n",
    "                                    st.warning(\"Could not generate embedding visualization.\")\n",
    "            elif uploaded_file is None:\n",
    "                st.info(\"Upload an image to start the search.\")\n",
    "            elif len(database_images) == 0:\n",
    "                st.warning(\"Database is empty or not loaded. Please check the database path and click 'Load/Reload Database' in the sidebar.\")\n",
    "    with tab2:\n",
    "        if len(database_images) > 0:\n",
    "            st.header(\"Database Statistics\")\n",
    "            st.write(f\"Total images: {stats['num_images']}\")\n",
    "            st.write(f\"Number of breeds: {stats['num_breeds']}\")\n",
    "            st.write(f\"Load time: {stats['load_time']:.2f} seconds\")\n",
    "            st.subheader(\"Breed Distribution\")\n",
    "            breeds = list(stats['breed_distribution'].keys())\n",
    "            counts = list(stats['breed_distribution'].values())\n",
    "            breed_fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            y_pos = np.arange(len(breeds))\n",
    "            ax.barh(y_pos, counts)\n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(breeds)\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_xlabel('Count')\n",
    "            ax.set_title('Number of Images per Breed')\n",
    "            st.pyplot(breed_fig)\n",
    "        else:\n",
    "            st.info(\"Load a database to see statistics.\")\n",
    "    with tab3:\n",
    "        if len(database_images) > 0:\n",
    "            st.header(\"Embedding Space Visualization\")\n",
    "            st.write(\"This visualization shows the distribution of pet breed embeddings in a 2D space using t-SNE.\")\n",
    "            with st.spinner(\"Generating embedding visualization...\"):\n",
    "                vis_fig = plot_tsne(database_embeddings, database_labels)\n",
    "                if vis_fig:\n",
    "                    st.pyplot(vis_fig)\n",
    "                else:\n",
    "                    st.warning(\"Could not generate visualization.\")\n",
    "        else:\n",
    "            st.info(\"Load a database to visualize embeddings.\")\n",
    "\n",
    "# The following check allows the script to be run directly when converted to a Python file\n",
    "if __name__ == \"__main__\":\n",
    "    run_app()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
