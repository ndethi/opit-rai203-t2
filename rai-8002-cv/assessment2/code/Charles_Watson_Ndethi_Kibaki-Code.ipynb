{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndethi/opit-rai203-t2/blob/main/rai-8002-cv/assessment2/code/Charles_Watson_Ndethi_Kibaki-Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_dDT8m6f7Lm"
      },
      "source": [
        "# Assessment 2: Metric Learning with Oxford-IIIT Pet Dataset\n",
        "\n",
        "## Introduction and Setup\n",
        "\n",
        "This notebook implements a deep metric learning approach for the Oxford-IIIT Pet Dataset, focusing on learning an embedding space where similar pet breeds are close together and dissimilar ones are far apart. We'll explore different loss functions, evaluate the model on verification, retrieval, and few-shot classification tasks, and visualize the embedding space.\n",
        "\n",
        "### Environment Setup and Package Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfUxyuWmf7Lo",
        "outputId": "79eb374c-5333-4690-e3bd-563aefb66061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-metric-learning\n",
            "  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (1.6.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (3.0.2)\n",
            "Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-metric-learning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-metric-learning-2.8.1\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->umap-learn) (3.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Check if running in Colab (to install dependencies and set up environment)\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "# Install required packages\n",
        "if IN_COLAB:\n",
        "    !pip install pytorch-metric-learning\n",
        "    !pip install faiss-gpu\n",
        "    !pip install umap-learn\n",
        "    !pip install matplotlib seaborn scikit-learn tqdm\n",
        "    !pip install gradio\n",
        "    !pip install grad-cam\n",
        "\n",
        "### Import Libraries\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
        "from torchvision import datasets, models, transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import pytorch_metric_learning\n",
        "from pytorch_metric_learning import losses, miners, distances, reducers, testers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import umap\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# Update imports to include model weights\n",
        "from torchvision.models import (ResNet18_Weights, ResNet34_Weights, ResNet50_Weights,\n",
        "                               EfficientNet_B0_Weights, MobileNet_V2_Weights, DenseNet121_Weights)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Optimize CUDA operations\n",
        "if torch.cuda.is_available():\n",
        "    # Enable cuDNN benchmarking for performance optimization with fixed-size inputs\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    # Deterministic algorithms are slower but ensure reproducible results\n",
        "    # torch.backends.cudnn.deterministic = True  # Uncomment for strict reproducibility\n",
        "\n",
        "    # Check if AMP is available (Mixed precision training)\n",
        "    amp_available = True\n",
        "    print(f\"CUDA enabled with device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Using mixed precision training: {amp_available}\")\n",
        "else:\n",
        "    amp_available = False\n",
        "    print(\"CUDA not available. Using CPU only.\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1qdsTPyf7Lp"
      },
      "source": [
        "### Mount Google Drive (if in Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcTDNm5Df7Lq"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Define base directory in Google Drive\n",
        "    drive_base_dir = '/content/drive/MyDrive/ColabNotebooks/PetMetricLearning'\n",
        "    os.makedirs(drive_base_dir, exist_ok=True)\n",
        "    print(f\"Google Drive mounted. Base directory: {drive_base_dir}\")\n",
        "else:\n",
        "    # Define a local base directory if not in Colab\n",
        "    drive_base_dir = './pet_metric_learning_results'\n",
        "    os.makedirs(drive_base_dir, exist_ok=True)\n",
        "    print(f\"Not in Colab. Using local directory: {drive_base_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJl-HNF0f7Lq"
      },
      "source": [
        "\n",
        "\n",
        "## Data Loading and Preprocessing\n",
        "\n",
        "In this section, we'll load the Oxford-IIIT Pet Dataset, perform necessary preprocessing, and create appropriate data loaders for our metric learning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83BgRfEdf7Lq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Function to load the dataset\n",
        "def load_oxford_pets_dataset(root=\"./data\", download=True):\n",
        "    train_val_dataset = datasets.OxfordIIITPet(\n",
        "        root=root,\n",
        "        split=\"trainval\",\n",
        "        transform=train_transform,\n",
        "        download=download\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.OxfordIIITPet(\n",
        "        root=root,\n",
        "        split=\"test\",\n",
        "        transform=eval_transform,\n",
        "        download=download\n",
        "    )\n",
        "\n",
        "    # For evaluation, create a version of the training set with eval transforms\n",
        "    eval_train_dataset = datasets.OxfordIIITPet(\n",
        "        root=root,\n",
        "        split=\"trainval\",\n",
        "        transform=eval_transform,\n",
        "        download=False\n",
        "    )\n",
        "\n",
        "    return train_val_dataset, test_dataset, eval_train_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ick3jeUEf7Lq"
      },
      "source": [
        "\n",
        "### Dataset Preparation for Different Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V__e7AbRf7Lq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Split data for training, validation and few-shot evaluation\n",
        "def prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset, num_holdout_classes=5, val_ratio=0.2):\n",
        "    # Get the class names\n",
        "    class_to_idx = train_val_dataset.class_to_idx\n",
        "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
        "    num_classes = len(class_to_idx)\n",
        "\n",
        "    # Split classes for few-shot learning (hold out some classes for testing)\n",
        "    all_class_indices = list(range(num_classes))\n",
        "    holdout_class_indices = random.sample(all_class_indices, num_holdout_classes)\n",
        "    training_class_indices = [i for i in all_class_indices if i not in holdout_class_indices]\n",
        "\n",
        "    holdout_classes = [idx_to_class[i] for i in holdout_class_indices]\n",
        "    print(f\"Holdout classes for few-shot learning: {holdout_classes}\")\n",
        "\n",
        "    # Create datasets excluding holdout classes for main training\n",
        "    train_val_indices = [i for i, (_, label) in enumerate(train_val_dataset) if label not in holdout_class_indices]\n",
        "    test_indices = [i for i, (_, label) in enumerate(test_dataset) if label not in holdout_class_indices]\n",
        "    eval_train_indices = [i for i, (_, label) in enumerate(eval_train_dataset) if label not in holdout_class_indices]\n",
        "\n",
        "    # For few-shot learning, include only holdout classes\n",
        "    few_shot_train_indices = [i for i, (_, label) in enumerate(train_val_dataset) if label in holdout_class_indices]\n",
        "    few_shot_test_indices = [i for i, (_, label) in enumerate(test_dataset) if label in holdout_class_indices]\n",
        "\n",
        "    # Split train/val\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        train_val_indices,\n",
        "        test_size=val_ratio,\n",
        "        stratify=[train_val_dataset[i][1] for i in train_val_indices],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Create Subset datasets\n",
        "    train_dataset = Subset(train_val_dataset, train_indices)\n",
        "    val_dataset = Subset(train_val_dataset, val_indices)\n",
        "    test_filtered_dataset = Subset(test_dataset, test_indices)\n",
        "    eval_train_dataset = Subset(eval_train_dataset, eval_train_indices)\n",
        "\n",
        "    # Create datasets for few-shot learning\n",
        "    few_shot_train_dataset = Subset(train_val_dataset, few_shot_train_indices)\n",
        "    few_shot_test_dataset = Subset(test_dataset, few_shot_test_indices)\n",
        "\n",
        "    # Create dictionary for class mapping\n",
        "    class_mapping = {\n",
        "        'class_to_idx': class_to_idx,\n",
        "        'idx_to_class': idx_to_class,\n",
        "        'holdout_class_indices': holdout_class_indices,\n",
        "        'training_class_indices': training_class_indices\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'train': train_dataset,\n",
        "        'val': val_dataset,\n",
        "        'test': test_filtered_dataset,\n",
        "        'eval_train': eval_train_dataset,\n",
        "        'few_shot_train': few_shot_train_dataset,\n",
        "        'few_shot_test': few_shot_test_dataset,\n",
        "        'class_mapping': class_mapping\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3FTMi8mf7Lr"
      },
      "source": [
        "\n",
        "### Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJLRJ7dAf7Lr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_dataloaders(datasets_dict, batch_size=32, num_workers=4):\n",
        "    dataloaders = {}\n",
        "\n",
        "    for key in ['train', 'val', 'test', 'eval_train', 'few_shot_train', 'few_shot_test']:\n",
        "        if key == 'train':\n",
        "            shuffle = True\n",
        "        else:\n",
        "            shuffle = False\n",
        "\n",
        "        dataloaders[key] = DataLoader(\n",
        "            datasets_dict[key],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    return dataloaders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOjwgxUHf7Lr"
      },
      "source": [
        "\n",
        "\n",
        "### Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYuwpJzJf7Lr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the dataset\n",
        "train_val_dataset, test_dataset, eval_train_dataset = load_oxford_pets_dataset()\n",
        "print(f\"Train+Val size: {len(train_val_dataset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")\n",
        "\n",
        "# Prepare datasets for different tasks\n",
        "datasets_dict = prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 32  # Adjust based on your GPU/memory constraints\n",
        "dataloaders = create_dataloaders(datasets_dict, batch_size=batch_size)\n",
        "\n",
        "# Print dataset statistics\n",
        "print(\"\\nDataset Statistics:\")\n",
        "for key, dataloader in dataloaders.items():\n",
        "    print(f\"{key}: {len(dataloader.dataset)} samples\")\n",
        "\n",
        "class_mapping = datasets_dict['class_mapping']\n",
        "num_classes = len(class_mapping['class_to_idx'])\n",
        "print(f\"Total number of classes: {num_classes}\")\n",
        "print(f\"Number of training classes: {len(class_mapping['training_class_indices'])}\")\n",
        "print(f\"Number of few-shot classes: {len(class_mapping['holdout_class_indices'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj8saioCf7Ls"
      },
      "source": [
        "\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "In this section, we'll define our metric learning model architecture using a CNN backbone and a projection head.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUy8575Bf7Ls"
      },
      "outputs": [],
      "source": [
        "\n",
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self, backbone_name='resnet18', embedding_size=128, pretrained=True):\n",
        "        super(EmbeddingNet, self).__init__()\n",
        "\n",
        "        # Get backbone and its output size\n",
        "        self.backbone, backbone_output_size = self._get_backbone(backbone_name, pretrained)\n",
        "\n",
        "        # Projection head (MLP)\n",
        "        self.projection_head = nn.Sequential(\n",
        "            nn.Linear(backbone_output_size, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, embedding_size)\n",
        "        )\n",
        "\n",
        "    def _get_backbone(self, backbone_name, pretrained):\n",
        "        \"\"\"\n",
        "        Create a backbone network from various architectures\n",
        "        \"\"\"\n",
        "        if backbone_name == 'resnet18':\n",
        "            weights = ResNet18_Weights.DEFAULT if pretrained else None\n",
        "            backbone = models.resnet18(weights=weights)\n",
        "            output_size = 512\n",
        "        elif backbone_name == 'resnet34':\n",
        "            weights = ResNet34_Weights.DEFAULT if pretrained else None\n",
        "            backbone = models.resnet34(weights=weights)\n",
        "            output_size = 512\n",
        "        elif backbone_name == 'resnet50':\n",
        "            weights = ResNet50_Weights.DEFAULT if pretrained else None\n",
        "            backbone = models.resnet50(weights=weights)\n",
        "            output_size = 2048\n",
        "        elif backbone_name == 'efficientnet_b0':\n",
        "            weights = EfficientNet_B0_Weights.DEFAULT if pretrained else None\n",
        "            backbone = models.efficientnet_b0(weights=weights)\n",
        "            output_size = 1280\n",
        "        elif backbone_name == 'mobilenet_v2':\n",
        "            weights = MobileNet_V2_Weights.DEFAULT if pretrained else None\n",
        "            backbone = models.mobilenet_v2(weights=weights)\n",
        "            output_size = 1280\n",
        "        elif backbone_name == 'densenet121':\n",
        "            weights = DenseNet121_Weights.DEFAULT if pretrained else None\n",
        "            backbone = models.densenet121(weights=weights)\n",
        "            output_size = 1024\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
        "\n",
        "        # For ResNet models\n",
        "        if backbone_name.startswith('resnet'):\n",
        "            # Remove the classification layer\n",
        "            backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
        "        # For EfficientNet\n",
        "        elif backbone_name.startswith('efficientnet'):\n",
        "            backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
        "        # For MobileNet\n",
        "        elif backbone_name.startswith('mobilenet'):\n",
        "            backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
        "        # For DenseNet\n",
        "        elif backbone_name.startswith('densenet'):\n",
        "            backbone = nn.Sequential(\n",
        "                backbone.features,\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.AdaptiveAvgPool2d((1, 1))\n",
        "            )\n",
        "\n",
        "        return backbone, output_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        embeddings = self.projection_head(features)\n",
        "\n",
        "        # Normalize embeddings to unit length (important for cosine distance)\n",
        "        normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "        return normalized_embeddings\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Sb43T9f7Ls"
      },
      "source": [
        "\n",
        "## Loss Function Implementation\n",
        "\n",
        "Here we'll implement several loss functions for metric learning including Triplet Loss, Contrastive Loss, and ArcFace. We'll also implement miners for efficient training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPiIG6Rvf7Ls"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_loss_and_miner(loss_type, margin=0.2, embedding_size=128, num_classes=32):\n",
        "    \"\"\"\n",
        "    Create loss function and miner for metric learning\n",
        "    \"\"\"\n",
        "    if loss_type == 'triplet':\n",
        "        # Triplet loss with cosine distance\n",
        "        distance = distances.CosineSimilarity()\n",
        "        reducer = reducers.ThresholdReducer(low=0)\n",
        "        loss_func = losses.TripletMarginLoss(margin=margin, distance=distance, reducer=reducer)\n",
        "        mining_func = miners.TripletMarginMiner(margin=margin, distance=distance, type_of_triplets=\"semihard\")\n",
        "\n",
        "    elif loss_type == 'contrastive':\n",
        "        # Contrastive loss\n",
        "        distance = distances.CosineSimilarity()\n",
        "        loss_func = losses.ContrastiveLoss(pos_margin=0.8, neg_margin=0.2, distance=distance)\n",
        "        mining_func = miners.PairMarginMiner(pos_margin=0.8, neg_margin=0.2, distance=distance)\n",
        "\n",
        "    elif loss_type == 'arcface':\n",
        "        # ArcFace loss\n",
        "        loss_func = losses.ArcFaceLoss(embedding_size, num_classes, margin=28.6, scale=64)\n",
        "        mining_func = None\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported loss type: {loss_type}\")\n",
        "\n",
        "    return loss_func, mining_func\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGXJrIyJf7Ls"
      },
      "source": [
        "\n",
        "\n",
        "### Hard Negative Mining (Bonus Implementation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqUKMBTZf7Ls"
      },
      "outputs": [],
      "source": [
        "\n",
        "class HardNegativePairMiner(miners.BaseMiner):\n",
        "    def __init__(self, distance, neg_margin=0.2, hardest_fraction=0.5):\n",
        "        super().__init__()\n",
        "        self.distance = distance\n",
        "        self.neg_margin = neg_margin\n",
        "        self.hardest_fraction = hardest_fraction\n",
        "\n",
        "    def mine(self, embeddings, labels, ref_emb=None, ref_labels=None):\n",
        "        ref_emb, ref_labels = embeddings, labels\n",
        "        dist_mat = self.distance(embeddings, ref_emb)\n",
        "\n",
        "        # Get negative pairs (different classes)\n",
        "        negative_mask = labels.unsqueeze(1) != ref_labels.unsqueeze(0)\n",
        "\n",
        "        # For each anchor, find all negative pairs\n",
        "        anchors, negatives = torch.where(negative_mask)\n",
        "\n",
        "        if len(anchors) == 0:\n",
        "            return empty_tensor(0), empty_tensor(0), empty_tensor(0), empty_tensor(0)\n",
        "\n",
        "        # Get distances for all negative pairs\n",
        "        distances = dist_mat[anchors, negatives]\n",
        "\n",
        "        # Group by anchor\n",
        "        anchor_groups = defaultdict(list)\n",
        "        for i in range(len(anchors)):\n",
        "            anchor_groups[anchors[i].item()].append((negatives[i].item(), distances[i].item()))\n",
        "\n",
        "        # For each anchor, select the hardest negatives\n",
        "        hard_a, hard_n = [], []\n",
        "        for anchor, neg_dists in anchor_groups.items():\n",
        "            # Sort negatives by distance (ascending for hardest cosine similarity)\n",
        "            neg_dists.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Select hardest fraction\n",
        "            num_to_select = max(1, int(len(neg_dists) * self.hardest_fraction))\n",
        "            selected_negs = neg_dists[:num_to_select]\n",
        "\n",
        "            for neg, dist in selected_negs:\n",
        "                hard_a.append(anchor)\n",
        "                hard_n.append(neg)\n",
        "\n",
        "        return (\n",
        "            torch.tensor(hard_a, device=embeddings.device),\n",
        "            empty_tensor(0),\n",
        "            empty_tensor(0),\n",
        "            torch.tensor(hard_n, device=embeddings.device)\n",
        "        )\n",
        "\n",
        "def empty_tensor(size):\n",
        "    return torch.tensor([], device=device, dtype=torch.long).view(size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkxBgYGhf7Lt"
      },
      "source": [
        "\n",
        "\n",
        "## Training Pipeline\n",
        "\n",
        "Next, we'll implement the training pipeline for our metric learning model, including early stopping and checkpointing to Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZKnmRXhf7Lt"
      },
      "outputs": [],
      "source": [
        "# Early Stopping Class\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='best_model.pth', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                           Default: 0\n",
        "            path (str): Path for the best model checkpoint to be saved to.\n",
        "                        Default: 'best_model.pth'\n",
        "            trace_func (function): trace print function.\n",
        "                                   Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.inf  # Use np.inf instead of np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving best model to {self.path} ...')\n",
        "        # Ensure the directory exists\n",
        "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xsy2Rubtf7Lt"
      },
      "outputs": [],
      "source": [
        "\n",
        "import glob # Import glob for finding checkpoint files\n",
        "\n",
        "def train_model(model, dataloaders, loss_type, optimizer=None, scheduler=None, num_epochs=15, embedding_size=128, checkpoint_dir='checkpoints'):\n",
        "    \"\"\"\n",
        "    Train the metric learning model with enhanced monitoring, optimizations, and checkpointing.\n",
        "    \"\"\"\n",
        "    # Ensure checkpoint directory exists\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
        "\n",
        "    # Setup for mixed precision training\n",
        "    scaler = torch.amp.GradScaler('cuda', enabled=amp_available) if torch.cuda.is_available() else None\n",
        "\n",
        "    # Get the number of training classes (excluding holdout classes)\n",
        "    num_training_classes = len(datasets_dict['class_mapping']['training_class_indices'])\n",
        "\n",
        "    # Create loss function and miner\n",
        "    loss_func, mining_func = create_loss_and_miner(\n",
        "        loss_type=loss_type,\n",
        "        embedding_size=embedding_size,\n",
        "        num_classes=num_training_classes\n",
        "    )\n",
        "\n",
        "    # If using ArcFace, we need to create a class map for the training dataset\n",
        "    if loss_type == 'arcface':\n",
        "        # Map original class indices to consecutive integers for ArcFace\n",
        "        class_map = {original: i for i, original in enumerate(datasets_dict['class_mapping']['training_class_indices'])}\n",
        "\n",
        "    # Create optimizer and scheduler if not provided\n",
        "    if optimizer is None:\n",
        "        # Define default optimizer and scheduler creation logic if needed\n",
        "        # For now, assume they are passed or handle error\n",
        "        raise ValueError(\"Optimizer must be provided for checkpoint loading.\")\n",
        "        # optimizer, auto_scheduler = get_optimizer_and_scheduler(model) # Example\n",
        "        # scheduler = auto_scheduler if scheduler is None else scheduler # Example\n",
        "\n",
        "    # Initialize training history and starting epoch\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'lr': [],\n",
        "        'batch_losses': [],\n",
        "        'gradient_norms': [],\n",
        "        'epoch_times': []\n",
        "    }\n",
        "    start_epoch = 0\n",
        "\n",
        "    # --- Checkpoint Loading ---\n",
        "    latest_checkpoint_path = None\n",
        "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pth'))\n",
        "    if checkpoint_files:\n",
        "        # Find the checkpoint with the highest epoch number\n",
        "        latest_epoch = -1\n",
        "        for f in checkpoint_files:\n",
        "            try:\n",
        "                epoch_num = int(os.path.basename(f).split('_')[-1].split('.')[0])\n",
        "                if epoch_num > latest_epoch:\n",
        "                    latest_epoch = epoch_num\n",
        "                    latest_checkpoint_path = f\n",
        "            except ValueError:\n",
        "                continue # Ignore files that don't match the pattern\n",
        "\n",
        "    if latest_checkpoint_path:\n",
        "        print(f\"Resuming training from checkpoint: {latest_checkpoint_path}\")\n",
        "        checkpoint = torch.load(latest_checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict'] is not None:\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        if 'history' in checkpoint:\n",
        "             history = checkpoint['history'] # Load history if saved\n",
        "        # Ensure loaded states are on the correct device\n",
        "        for state in optimizer.state.values():\n",
        "            for k, v in state.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    state[k] = v.to(device)\n",
        "        print(f\"Resumed from epoch {start_epoch}\")\n",
        "    else:\n",
        "        print(\"No checkpoint found, starting training from scratch.\")\n",
        "    # --- End Checkpoint Loading ---\n",
        "\n",
        "    # Initialize early stopping\n",
        "    early_stopping = EarlyStopping(patience=5, verbose=True, path=best_model_path)\n",
        "    # Load previous best score if resuming\n",
        "    if start_epoch > 0 and history['val_loss']:\n",
        "        early_stopping.val_loss_min = min(history['val_loss'])\n",
        "        early_stopping.best_score = -early_stopping.val_loss_min\n",
        "        print(f\"Loaded previous best validation loss: {early_stopping.val_loss_min:.6f}\")\n",
        "\n",
        "    # Training loop\n",
        "    # history dictionary is already initialized or loaded\n",
        "\n",
        "    # Create experiment folder for saving results (if needed, separate from checkpoints)\n",
        "    # backbone_name = model.backbone.__class__.__name__ # Get backbone name dynamically if possible\n",
        "    # experiment_id = f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{loss_type}_{backbone_name}\"\n",
        "    # experiment_dir = os.path.join(\"experiments\", experiment_id)\n",
        "    # os.makedirs(experiment_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        epoch_train_losses = [] # Track batch losses for this epoch's history\n",
        "        epoch_grad_norms = []   # Track grad norms for this epoch's history\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            # batch_losses = [] # Moved epoch_train_losses outside phase loop\n",
        "            # grad_norms = []   # Moved epoch_grad_norms outside phase loop\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in tqdm(dataloaders[phase], desc=phase):\n",
        "                inputs = inputs.to(device, non_blocking=True)  # non_blocking for asynchronous transfer\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "                # Map labels for ArcFace if needed\n",
        "                if loss_type == 'arcface':\n",
        "                    # Filter out samples from holdout classes\n",
        "                    valid_idx = torch.tensor([i for i, l in enumerate(labels) if l.item() in class_map], device=device)\n",
        "                    if len(valid_idx) == 0:\n",
        "                        continue\n",
        "\n",
        "                    inputs = inputs[valid_idx]\n",
        "                    arcface_labels = torch.tensor([class_map[l.item()] for l in labels[valid_idx]], device=device)\n",
        "                    labels = arcface_labels\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad(set_to_none=True)  # set_to_none=True is more memory efficient\n",
        "\n",
        "                # Forward pass\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Use autocast for both train and val if scaler is enabled\n",
        "                    with torch.amp.autocast(device_type=device.type, enabled=(scaler is not None)):\n",
        "                        embeddings = model(inputs)\n",
        "\n",
        "                        # Get indices for mining if using a mining function\n",
        "                        if mining_func is not None:\n",
        "                            hard_pairs = mining_func(embeddings, labels)\n",
        "                            loss = loss_func(embeddings, labels, hard_pairs)\n",
        "                        else:\n",
        "                            loss = loss_func(embeddings, labels)\n",
        "\n",
        "                    # Backward pass + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        if scaler is not None:\n",
        "                            # Backward pass with gradient scaling\n",
        "                            scaler.scale(loss).backward()\n",
        "\n",
        "                            # Compute gradient norm after unscaling\n",
        "                            scaler.unscale_(optimizer)\n",
        "                            total_norm = clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                            epoch_grad_norms.append(total_norm.item())\n",
        "\n",
        "                            # Optimizer step with scaler\n",
        "                            scaler.step(optimizer)\n",
        "                            scaler.update()\n",
        "                        else:\n",
        "                            # Regular backward pass without AMP\n",
        "                            loss.backward()\n",
        "                            total_norm = clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                            epoch_grad_norms.append(total_norm.item())\n",
        "                            optimizer.step()\n",
        "\n",
        "                # Record batch loss\n",
        "                batch_loss = loss.item()\n",
        "                if phase == 'train':\n",
        "                    epoch_train_losses.append(batch_loss)\n",
        "                running_loss += batch_loss * inputs.size(0)\n",
        "\n",
        "            # Calculate epoch loss\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "\n",
        "            if phase == 'train':\n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                history['lr'].append(current_lr)\n",
        "                history['train_loss'].append(epoch_loss)\n",
        "                history['batch_losses'].extend(epoch_train_losses) # Append all batch losses for the epoch\n",
        "                if epoch_grad_norms:\n",
        "                    history['gradient_norms'].extend(epoch_grad_norms) # Append all grad norms for the epoch\n",
        "                    print(f'{phase} Loss: {epoch_loss:.4f}, LR: {current_lr:.6f}, Grad Norm: {np.mean(epoch_grad_norms):.4f}')\n",
        "                else:\n",
        "                    print(f'{phase} Loss: {epoch_loss:.4f}, LR: {current_lr:.6f}')\n",
        "            else:\n",
        "                history['val_loss'].append(epoch_loss)\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "                # Update learning rate scheduler based on validation loss\n",
        "                if scheduler is not None and isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                    scheduler.step(epoch_loss)\n",
        "\n",
        "                # Check early stopping\n",
        "                early_stopping(epoch_loss, model)\n",
        "                if early_stopping.early_stop:\n",
        "                    print(\"Early stopping triggered\")\n",
        "                    break # Break inner loop (phases)\n",
        "\n",
        "        # Update step-based schedulers at the end of each epoch\n",
        "        if scheduler is not None and not isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "            scheduler.step()\n",
        "\n",
        "        # Record epoch time\n",
        "        epoch_end = time.time()\n",
        "        epoch_time = epoch_end - epoch_start\n",
        "        history['epoch_times'].append(epoch_time)\n",
        "        print(f\"Epoch completed in {epoch_time:.2f}s\")\n",
        "\n",
        "        # --- Save Checkpoint After Each Epoch ---\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
        "        save_dict = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'embedding_size': embedding_size,\n",
        "            # 'backbone_name': backbone_name, # Can be derived from model or config\n",
        "            'class_mapping': datasets_dict['class_mapping'],\n",
        "            'history': history # Save history in checkpoint\n",
        "        }\n",
        "        if scheduler:\n",
        "             save_dict['scheduler_state_dict'] = scheduler.state_dict()\n",
        "        torch.save(save_dict, checkpoint_path)\n",
        "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "        # --- End Save Checkpoint ---\n",
        "\n",
        "        # Check if early stopping was triggered in the inner loop\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Training stopped early due to no improvement in validation loss\")\n",
        "            break # Break outer loop (epochs)\n",
        "\n",
        "        # Optional: Remove older checkpoints to save space\n",
        "        # Keep maybe last 5 checkpoints + best model\n",
        "        # ... (implementation omitted for brevity)\n",
        "\n",
        "    # Load the best model weights saved by EarlyStopping\n",
        "    if os.path.exists(best_model_path):\n",
        "        print(f\"Loading best model from {best_model_path}\")\n",
        "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    else:\n",
        "        print(\"Warning: Best model file not found. Using the model state from the last epoch.\")\n",
        "\n",
        "    # Save final training history (optional, as it's saved in checkpoints)\n",
        "    history_path = os.path.join(checkpoint_dir, 'training_history_final.json')\n",
        "    with open(history_path, 'w') as f:\n",
        "        # Convert any non-serializable items in history\n",
        "        serializable_history = {\n",
        "            k: v if isinstance(v, list) and all(isinstance(x, (int, float)) for x in v) else str(v)\n",
        "            for k, v in history.items()\n",
        "        }\n",
        "        json.dump(serializable_history, f, indent=2)\n",
        "        print(f\"Final training history saved to {history_path}\")\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0XqIAVWf7Lu"
      },
      "source": [
        "\n",
        "\n",
        "### Embedding Extraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCfLsUiAf7Lu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_embeddings(model, dataloader):\n",
        "    \"\"\"\n",
        "    Extract embeddings for a dataset\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, batch_labels in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
        "            inputs = inputs.to(device)\n",
        "            batch_embeddings = model(inputs)\n",
        "            embeddings.append(batch_embeddings.cpu())\n",
        "            labels.append(batch_labels)\n",
        "\n",
        "    embeddings = torch.cat(embeddings, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "\n",
        "    return embeddings, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gfOM1Gkf7Lu"
      },
      "source": [
        "\n",
        "\n",
        "## Evaluation Functions\n",
        "\n",
        "### 1. Verification Task\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn6IFtWmf7Lu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_verification_pairs(embeddings, labels, num_pos_pairs=1000, num_neg_pairs=1000):\n",
        "    \"\"\"\n",
        "    Create positive and negative pairs for verification task\n",
        "    \"\"\"\n",
        "    unique_labels = torch.unique(labels)\n",
        "    pairs = []\n",
        "    pair_labels = []\n",
        "\n",
        "    # Generate positive pairs (same class)\n",
        "    pos_pair_count = 0\n",
        "    for label in unique_labels:\n",
        "        indices = torch.where(labels == label)[0]\n",
        "        if len(indices) >= 2:\n",
        "            for i in range(min(num_pos_pairs // len(unique_labels) + 1, len(indices) // 2)):\n",
        "                idx1, idx2 = np.random.choice(indices, 2, replace=False)\n",
        "                pairs.append((idx1.item(), idx2.item()))\n",
        "                pair_labels.append(1)  # 1 for same class\n",
        "                pos_pair_count += 1\n",
        "                if pos_pair_count >= num_pos_pairs:\n",
        "                    break\n",
        "        if pos_pair_count >= num_pos_pairs:\n",
        "            break\n",
        "\n",
        "    # Generate negative pairs (different classes)\n",
        "    neg_pair_count = 0\n",
        "    while neg_pair_count < num_neg_pairs:\n",
        "        label1, label2 = np.random.choice(unique_labels, 2, replace=False)\n",
        "        indices1 = torch.where(labels == label1)[0]\n",
        "        indices2 = torch.where(labels == label2)[0]\n",
        "\n",
        "        if len(indices1) > 0 and len(indices2) > 0:\n",
        "            idx1 = np.random.choice(indices1)\n",
        "            idx2 = np.random.choice(indices2)\n",
        "            pairs.append((idx1.item(), idx2.item()))\n",
        "            pair_labels.append(0)  # 0 for different class\n",
        "            neg_pair_count += 1\n",
        "\n",
        "    return np.array(pairs), np.array(pair_labels)\n",
        "\n",
        "def evaluate_verification(embeddings, labels):\n",
        "    \"\"\"\n",
        "    Evaluate the model on verification task (same/different class)\n",
        "    \"\"\"\n",
        "    pairs, pair_labels = create_verification_pairs(embeddings, labels)\n",
        "\n",
        "    # Compute distances between pairs\n",
        "    distances = []\n",
        "    for idx1, idx2 in pairs:\n",
        "        # Using cosine similarity (-1 to 1) where higher value means more similar\n",
        "        distance = F.cosine_similarity(\n",
        "            embeddings[idx1].unsqueeze(0),\n",
        "            embeddings[idx2].unsqueeze(0)\n",
        "        ).item()\n",
        "        distances.append(distance)\n",
        "\n",
        "    distances = np.array(distances)\n",
        "\n",
        "    # Compute ROC curve and AUC\n",
        "    # Note: For cosine similarity, higher means more similar, so we need to negate it for ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(pair_labels, distances)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Compute Equal Error Rate (EER)\n",
        "    fnr = 1 - tpr\n",
        "    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
        "    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC)')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig('verification_roc_curve.png')\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'roc_auc': roc_auc,\n",
        "        'eer': eer,\n",
        "        'eer_threshold': eer_threshold,\n",
        "        'pairs': pairs,\n",
        "        'pair_labels': pair_labels,\n",
        "        'distances': distances\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1EtB8BVf7Lu"
      },
      "source": [
        "\n",
        "\n",
        "### 2. Retrieval Task\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOEe4TTcf7Lu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_retrieval(query_embeddings, query_labels, gallery_embeddings, gallery_labels, k_values=[1, 5, 10]):\n",
        "    \"\"\"\n",
        "    Evaluate the model on retrieval task\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for k in k_values:\n",
        "        # Compute similarity matrix\n",
        "        similarity_matrix = torch.matmul(query_embeddings, gallery_embeddings.T)\n",
        "\n",
        "        # Get top-k indices for each query\n",
        "        _, indices = torch.topk(similarity_matrix, k=k, dim=1)\n",
        "\n",
        "        # Compute Recall@K and Precision@K\n",
        "        recall_k = 0\n",
        "        precision_k = 0\n",
        "\n",
        "        for i, query_label in enumerate(query_labels):\n",
        "            retrieved_labels = gallery_labels[indices[i]]\n",
        "            relevant = (retrieved_labels == query_label).float()\n",
        "\n",
        "            # Recall@K: How many of the relevant items are retrieved\n",
        "            recall_k += (relevant.sum() > 0).float().item()\n",
        "\n",
        "            # Precision@K: How many of the retrieved items are relevant\n",
        "            precision_k += (relevant.sum() / k).item()\n",
        "\n",
        "        recall_k /= len(query_labels)\n",
        "        precision_k /= len(query_labels)\n",
        "\n",
        "        results[f'recall@{k}'] = recall_k\n",
        "        results[f'precision@{k}'] = precision_k\n",
        "\n",
        "        print(f\"Recall@{k}: {recall_k:.4f}\")\n",
        "        print(f\"Precision@{k}: {precision_k:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC4cffoyf7Lv"
      },
      "source": [
        "\n",
        "### 3. Few-shot Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN3Pvswif7Lv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_few_shot(support_embeddings, support_labels, query_embeddings, query_labels, n_way=5, k_shot=5):\n",
        "    \"\"\"\n",
        "    Evaluate the model on n-way k-shot classification\n",
        "    \"\"\"\n",
        "    unique_labels = torch.unique(support_labels)\n",
        "    if len(unique_labels) < n_way:\n",
        "        print(f\"Warning: Only {len(unique_labels)} classes available, but n_way={n_way}\")\n",
        "        n_way = len(unique_labels)\n",
        "\n",
        "    # Randomly select n classes\n",
        "    selected_classes = np.random.choice(unique_labels.numpy(), n_way, replace=False)\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    # Run multiple episodes for stable results\n",
        "    num_episodes = 50\n",
        "    for episode in range(num_episodes):\n",
        "        # Create support set (k examples per class)\n",
        "        support_set_embeddings = []\n",
        "        support_set_labels = []\n",
        "\n",
        "        for class_idx, c in enumerate(selected_classes):\n",
        "            # Get indices of examples of class c\n",
        "            class_indices = torch.where(support_labels == c)[0]\n",
        "\n",
        "            # Randomly select k examples\n",
        "            if len(class_indices) >= k_shot:\n",
        "                selected_indices = np.random.choice(class_indices.numpy(), k_shot, replace=False)\n",
        "            else:\n",
        "                # If not enough examples, use all and repeat some\n",
        "                selected_indices = np.random.choice(class_indices.numpy(), k_shot, replace=True)\n",
        "\n",
        "            for idx in selected_indices:\n",
        "                support_set_embeddings.append(support_embeddings[idx])\n",
        "                support_set_labels.append(class_idx)  # Use class index as the new label\n",
        "\n",
        "        support_set_embeddings = torch.stack(support_set_embeddings)\n",
        "        support_set_labels = torch.tensor(support_set_labels)\n",
        "\n",
        "        # Create query set (all examples of the selected classes from the query set)\n",
        "        query_set_indices = torch.tensor([i for i, label in enumerate(query_labels) if label in selected_classes])\n",
        "\n",
        "        if len(query_set_indices) == 0:\n",
        "            print(\"Warning: No query examples for selected classes\")\n",
        "            continue\n",
        "\n",
        "        query_set_embeddings = query_embeddings[query_set_indices]\n",
        "        query_set_labels = query_labels[query_set_indices]\n",
        "\n",
        "        # Map original labels to new indices (0 to n_way-1)\n",
        "        label_mapping = {selected_classes[i]: i for i in range(n_way)}\n",
        "        query_set_labels = torch.tensor([label_mapping[label.item()] for label in query_set_labels])\n",
        "\n",
        "        # Compute prototypes (mean embedding for each class)\n",
        "        prototypes = torch.zeros(n_way, support_embeddings.size(1), device=support_embeddings.device)\n",
        "        for c in range(n_way):\n",
        "            prototypes[c] = support_set_embeddings[support_set_labels == c].mean(0)\n",
        "\n",
        "        # Compute distances between query examples and prototypes\n",
        "        # Using cosine similarity (higher means more similar)\n",
        "        logits = torch.matmul(query_set_embeddings, prototypes.T)\n",
        "\n",
        "        # Make predictions\n",
        "        _, predictions = torch.max(logits, dim=1)\n",
        "\n",
        "        # Compute accuracy\n",
        "        accuracy = (predictions == query_set_labels).float().mean().item()\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    mean_accuracy = np.mean(accuracies)\n",
        "    std_accuracy = np.std(accuracies)\n",
        "\n",
        "    print(f\"{n_way}-way {k_shot}-shot classification accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'mean_accuracy': mean_accuracy,\n",
        "        'std_accuracy': std_accuracy,\n",
        "        'accuracies': accuracies\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv3HuX9ff7Lv"
      },
      "source": [
        "\n",
        "\n",
        "## Embedding Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln2BWKGyf7Lv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def visualize_embeddings(embeddings, labels, class_mapping, method='tsne', title='Embedding Visualization'):\n",
        "    \"\"\"\n",
        "    Visualize embeddings using t-SNE or UMAP\n",
        "    \"\"\"\n",
        "    idx_to_class = class_mapping['idx_to_class']\n",
        "\n",
        "    # Reduce dimensionality\n",
        "    if method == 'tsne':\n",
        "        print(\"Computing t-SNE projection...\")\n",
        "        projection = TSNE(n_components=2, random_state=42).fit_transform(embeddings.numpy())\n",
        "    elif method == 'umap':\n",
        "        print(\"Computing UMAP projection...\")\n",
        "        projection = umap.UMAP(n_components=2, random_state=42).fit_transform(embeddings.numpy())\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported visualization method: {method}\")\n",
        "\n",
        "    # Create plot\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Get unique labels\n",
        "    unique_labels = torch.unique(labels).numpy()\n",
        "\n",
        "    # Create colormap\n",
        "    cmap = plt.cm.get_cmap('tab20', len(unique_labels))\n",
        "\n",
        "    # Plot each class\n",
        "    for i, label in enumerate(unique_labels):\n",
        "        mask = labels.numpy() == label\n",
        "        plt.scatter(\n",
        "            projection[mask, 0],\n",
        "            projection[mask, 1],\n",
        "            c=[cmap(i)],\n",
        "            label=idx_to_class[label],\n",
        "            alpha=0.7,\n",
        "            s=50\n",
        "        )\n",
        "\n",
        "    plt.title(title, fontsize=18)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{method}_visualization.png')\n",
        "    plt.show()\n",
        "\n",
        "    return projection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqR-tfEdf7Lv"
      },
      "source": [
        "\n",
        "\n",
        "## Grad-CAM Visualization (Bonus Implementation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q102RgcUf7Lw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def visualize_grad_cam(model, dataloader, class_mapping, num_images=5):\n",
        "    \"\"\"\n",
        "    Visualize Grad-CAM attention maps\n",
        "    \"\"\"\n",
        "    # Import GradCAM implementation\n",
        "    try:\n",
        "        from pytorch_grad_cam import GradCAM\n",
        "        from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "    except ImportError:\n",
        "        print(\"Please install pytorch-grad-cam to use this function:\")\n",
        "        print(\"!pip install grad-cam\")\n",
        "        return\n",
        "\n",
        "    # Set up GradCAM\n",
        "    target_layers = [model.backbone[-2][-1].conv2]  # Last conv layer for ResNet\n",
        "    # Initialize GradCAM without use_cuda argument\n",
        "    grad_cam = GradCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "    # Get a batch of images\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images = images[:num_images]\n",
        "    labels = labels[:num_images]\n",
        "\n",
        "    # Convert images for visualization\n",
        "    orig_images = []\n",
        "    for i in range(len(images)):\n",
        "        img = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "        img = (img * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
        "        img = np.clip(img, 0, 1)\n",
        "        orig_images.append(img)\n",
        "\n",
        "    # Generate class activation maps\n",
        "    cam_images = []\n",
        "    for i in range(len(images)):\n",
        "        input_tensor = images[i].unsqueeze(0).to(device)\n",
        "\n",
        "        # Get embeddings for target image\n",
        "        embedding = model(input_tensor)\n",
        "\n",
        "        # Generate GradCAM\n",
        "        grayscale_cam = grad_cam(input_tensor)\n",
        "        grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "        # Overlay on original image\n",
        "        cam_image = show_cam_on_image(orig_images[i], grayscale_cam, use_rgb=True)\n",
        "        cam_images.append(cam_image)\n",
        "\n",
        "    # Plot original images and their activation maps\n",
        "    plt.figure(figsize=(15, 4 * num_images))\n",
        "    for i in range(num_images):\n",
        "        # Original image\n",
        "        plt.subplot(num_images, 2, 2*i+1)\n",
        "        plt.imshow(orig_images[i])\n",
        "        plt.title(f\"Original: {class_mapping['idx_to_class'][labels[i].item()]}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # GradCAM\n",
        "        plt.subplot(num_images, 2, 2*i+2)\n",
        "        plt.imshow(cam_images[i])\n",
        "        plt.title(\"GradCAM\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('grad_cam_visualization.png')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KleEira9f7Lw"
      },
      "source": [
        "\n",
        "\n",
        "## Main Execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyXFaREWf7Lw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def main():\n",
        "    # Model parameters\n",
        "    backbone_name = 'resnet18'  # Options: 'resnet18', 'resnet50'\n",
        "    embedding_size = 128\n",
        "    batch_size = 32  # Adjust based on your GPU\n",
        "\n",
        "    # Training parameters\n",
        "    loss_type = 'triplet'  # Options: 'triplet', 'contrastive', 'arcface'\n",
        "    num_epochs = 20\n",
        "    lr = 1e-4\n",
        "\n",
        "    # --- Checkpoint Directory ---\n",
        "    # Use the drive_base_dir defined earlier\n",
        "    checkpoint_base_dir = os.path.join(drive_base_dir, f'{backbone_name}_{loss_type}')\n",
        "    os.makedirs(checkpoint_base_dir, exist_ok=True)\n",
        "    print(f\"Checkpoints will be saved in: {checkpoint_base_dir}\")\n",
        "    # --- End Checkpoint Directory ---\n",
        "\n",
        "    # Load data\n",
        "    train_val_dataset, test_dataset, eval_train_dataset = load_oxford_pets_dataset()\n",
        "    datasets_dict = prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset)\n",
        "    dataloaders = create_dataloaders(datasets_dict, batch_size=batch_size)\n",
        "\n",
        "    # Create model\n",
        "    model = EmbeddingNet(backbone_name=backbone_name, embedding_size=embedding_size)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Create optimizer and scheduler\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "    # Train model\n",
        "    model, history = train_model(\n",
        "        model=model,\n",
        "        dataloaders=dataloaders,\n",
        "        loss_type=loss_type,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        num_epochs=num_epochs,\n",
        "        embedding_size=embedding_size,\n",
        "        checkpoint_dir=checkpoint_base_dir # Pass checkpoint directory\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    # Adjust epoch range for plotting if training was resumed\n",
        "    epochs_completed = len(history['train_loss'])\n",
        "    epoch_range = range(1, epochs_completed + 1)\n",
        "    plt.plot(epoch_range, history['train_loss'], 'b-', label='Training Loss')\n",
        "    plt.plot(epoch_range, history['val_loss'], 'r-', label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'Training and Validation Loss ({loss_type} loss)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(checkpoint_base_dir, 'training_history.png')) # Save plot to drive\n",
        "    plt.show()\n",
        "\n",
        "    # Extract embeddings for evaluation\n",
        "    print(\"\\nExtracting embeddings for evaluation...\")\n",
        "    train_embeddings, train_labels = extract_embeddings(model, dataloaders['eval_train'])\n",
        "    test_embeddings, test_labels = extract_embeddings(model, dataloaders['test'])\n",
        "    few_shot_train_embeddings, few_shot_train_labels = extract_embeddings(model, dataloaders['few_shot_train'])\n",
        "    few_shot_test_embeddings, few_shot_test_labels = extract_embeddings(model, dataloaders['few_shot_test'])\n",
        "\n",
        "    # Evaluation tasks\n",
        "    print(\"\\n1. Verification Task:\")\n",
        "    verification_results = evaluate_verification(test_embeddings, test_labels)\n",
        "    # Save verification plot\n",
        "    plt.savefig(os.path.join(checkpoint_base_dir, 'verification_roc_curve.png'))\n",
        "\n",
        "    print(f\"\\nVerification Results:\")\n",
        "    print(f\"ROC AUC: {verification_results['roc_auc']:.4f}\")\n",
        "    print(f\"Equal Error Rate (EER): {verification_results['eer']:.4f}\")\n",
        "\n",
        "    print(\"\\n2. Retrieval Task:\")\n",
        "    retrieval_results = evaluate_retrieval(\n",
        "        query_embeddings=test_embeddings,\n",
        "        query_labels=test_labels,\n",
        "        gallery_embeddings=train_embeddings,\n",
        "        gallery_labels=train_labels,\n",
        "        k_values=[1, 5, 10]\n",
        "    )\n",
        "\n",
        "    print(\"\\n3. Few-shot Classification:\")\n",
        "    few_shot_results = evaluate_few_shot(\n",
        "        support_embeddings=few_shot_train_embeddings,\n",
        "        support_labels=few_shot_train_labels,\n",
        "        query_embeddings=few_shot_test_embeddings,\n",
        "        query_labels=few_shot_test_labels,\n",
        "        n_way=5,\n",
        "        k_shot=5\n",
        "    )\n",
        "\n",
        "    # Embedding visualization\n",
        "    print(\"\\n4. Embedding Visualization:\")\n",
        "    test_projection = visualize_embeddings(\n",
        "        embeddings=test_embeddings,\n",
        "        labels=test_labels,\n",
        "        class_mapping=datasets_dict['class_mapping'],\n",
        "        method='tsne',\n",
        "        title='t-SNE Visualization of Test Embeddings'\n",
        "    )\n",
        "    # Save visualization plot\n",
        "    plt.savefig(os.path.join(checkpoint_base_dir, 'tsne_visualization_test.png'))\n",
        "\n",
        "    # Visualize few-shot embeddings\n",
        "    print(\"\\nVisualizing few-shot embeddings:\")\n",
        "    # Combine few-shot train and test embeddings for visualization\n",
        "    all_few_shot_embeddings = torch.cat([few_shot_train_embeddings, few_shot_test_embeddings], dim=0)\n",
        "    all_few_shot_labels = torch.cat([few_shot_train_labels, few_shot_test_labels], dim=0)\n",
        "\n",
        "    few_shot_projection = visualize_embeddings(\n",
        "        embeddings=all_few_shot_embeddings,\n",
        "        labels=all_few_shot_labels,\n",
        "        class_mapping=datasets_dict['class_mapping'],\n",
        "        method='tsne',\n",
        "        title='t-SNE Visualization of Few-Shot Embeddings'\n",
        "    )\n",
        "    # Save visualization plot\n",
        "    plt.savefig(os.path.join(checkpoint_base_dir, 'tsne_visualization_fewshot.png'))\n",
        "\n",
        "    # Bonus: Grad-CAM Visualization\n",
        "    print(\"\\n5. Grad-CAM Visualization:\")\n",
        "    visualize_grad_cam(model, dataloaders['test'], datasets_dict['class_mapping'], num_images=3)\n",
        "    # Save Grad-CAM plot\n",
        "    plt.savefig(os.path.join(checkpoint_base_dir, 'grad_cam_visualization.png'))\n",
        "\n",
        "    # Save final model (best model is already saved by EarlyStopping in checkpoint_dir)\n",
        "    final_model_path = os.path.join(checkpoint_base_dir, f'pet_metric_learning_{backbone_name}_{loss_type}_final.pth')\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(), # Save the state after loading the best model\n",
        "        'embedding_size': embedding_size,\n",
        "        'backbone_name': backbone_name,\n",
        "        'class_mapping': datasets_dict['class_mapping']\n",
        "    }, final_model_path)\n",
        "    print(f\"Final model state saved to {final_model_path}\")\n",
        "\n",
        "    # Save evaluation results\n",
        "    eval_results = {\n",
        "        'verification': verification_results,\n",
        "        'retrieval': retrieval_results,\n",
        "        'few_shot': few_shot_results\n",
        "    }\n",
        "    eval_results_path = os.path.join(checkpoint_base_dir, 'evaluation_results.json')\n",
        "    # Make results JSON serializable (convert tensors/numpy arrays if necessary)\n",
        "    # ... (implementation depends on exact structure of results dicts)\n",
        "    # For simplicity, saving basic metrics\n",
        "    simple_eval_results = {\n",
        "        'verification_roc_auc': verification_results.get('roc_auc'),\n",
        "        'verification_eer': verification_results.get('eer'),\n",
        "        'retrieval_recall_1': retrieval_results.get('recall@1'),\n",
        "        'retrieval_precision_1': retrieval_results.get('precision@1'),\n",
        "        'few_shot_mean_accuracy': few_shot_results.get('mean_accuracy'),\n",
        "        'few_shot_std_accuracy': few_shot_results.get('std_accuracy')\n",
        "    }\n",
        "    with open(eval_results_path, 'w') as f:\n",
        "        json.dump(simple_eval_results, f, indent=2)\n",
        "    print(f\"Evaluation results saved to {eval_results_path}\")\n",
        "\n",
        "    print(\"\\nEvaluation completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAKvBhh-f7Lw"
      },
      "source": [
        "\n",
        "\n",
        "## Bonus: Multiple Loss Function Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lFfeqbUf7Lw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compare_loss_functions():\n",
        "    \"\"\"\n",
        "    Compare different loss functions for metric learning\n",
        "    \"\"\"\n",
        "    # Model parameters\n",
        "    backbone_name = 'resnet18'\n",
        "    embedding_size = 128\n",
        "    batch_size = 32\n",
        "    num_epochs = 15 # Use fewer epochs for comparison run\n",
        "    lr = 1e-4\n",
        "\n",
        "    # Loss functions to compare\n",
        "    loss_types = ['triplet', 'contrastive', 'arcface']\n",
        "\n",
        "    # Load data (only once)\n",
        "    train_val_dataset, test_dataset, eval_train_dataset = load_oxford_pets_dataset()\n",
        "    datasets_dict = prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset)\n",
        "    dataloaders = create_dataloaders(datasets_dict, batch_size=batch_size)\n",
        "\n",
        "    results = {}\n",
        "    comparison_base_dir = os.path.join(drive_base_dir, 'loss_comparison')\n",
        "    os.makedirs(comparison_base_dir, exist_ok=True)\n",
        "\n",
        "    for loss_type in loss_types:\n",
        "        print(f\"\\n{'=' * 40}\")\n",
        "        print(f\"Training with {loss_type} loss\")\n",
        "        print(f\"{'=' * 40}\")\n",
        "\n",
        "        # Define checkpoint directory for this specific run\n",
        "        checkpoint_dir = os.path.join(comparison_base_dir, f'{backbone_name}_{loss_type}')\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        # Create model\n",
        "        model = EmbeddingNet(backbone_name=backbone_name, embedding_size=embedding_size)\n",
        "        model = model.to(device)\n",
        "\n",
        "        # Create optimizer and scheduler\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "        # Train model\n",
        "        model, history = train_model(\n",
        "            model=model,\n",
        "            dataloaders=dataloaders,\n",
        "            loss_type=loss_type,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            num_epochs=num_epochs,\n",
        "            embedding_size=embedding_size,\n",
        "            checkpoint_dir=checkpoint_dir # Use specific checkpoint dir\n",
        "        )\n",
        "\n",
        "        # Extract embeddings for evaluation\n",
        "        print(\"\\nExtracting embeddings for evaluation...\")\n",
        "        train_embeddings, train_labels = extract_embeddings(model, dataloaders['eval_train'])\n",
        "        test_embeddings, test_labels = extract_embeddings(model, dataloaders['test'])\n",
        "\n",
        "        # Evaluation\n",
        "        verification_results = evaluate_verification(test_embeddings, test_labels)\n",
        "        plt.savefig(os.path.join(checkpoint_dir, 'verification_roc_curve.png')) # Save plot\n",
        "        plt.close() # Close plot to avoid displaying multiple times\n",
        "\n",
        "        retrieval_results = evaluate_retrieval(\n",
        "            query_embeddings=test_embeddings,\n",
        "            query_labels=test_labels,\n",
        "            gallery_embeddings=train_embeddings,\n",
        "            gallery_labels=train_labels,\n",
        "            k_values=[1, 5]\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        results[loss_type] = {\n",
        "            'verification': {\n",
        "                'roc_auc': verification_results['roc_auc'],\n",
        "                'eer': verification_results['eer']\n",
        "            },\n",
        "            'retrieval': {\n",
        "                'recall@1': retrieval_results['recall@1'],\n",
        "                'recall@5': retrieval_results['recall@5'],\n",
        "                'precision@1': retrieval_results['precision@1'],\n",
        "                'precision@5': retrieval_results['precision@5']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save model (optional, best is saved during training)\n",
        "        # torch.save({\n",
        "        #     'model_state_dict': model.state_dict(),\n",
        "        #     'embedding_size': embedding_size,\n",
        "        #     'backbone_name': backbone_name,\n",
        "        #     'class_mapping': datasets_dict['class_mapping']\n",
        "        # }, os.path.join(checkpoint_dir, f'pet_metric_learning_{backbone_name}_{loss_type}_comparison_final.pth'))\n",
        "\n",
        "    # Compare results\n",
        "    print(\"\\n{'='*50}\")\n",
        "    print(\"Comparison of Loss Functions\")\n",
        "    print({'='*50})\n",
        "\n",
        "    # Create a comparison table\n",
        "    comparison_data = []\n",
        "    for loss_type, metrics in results.items():\n",
        "        comparison_data.append({\n",
        "            'Loss Function': loss_type,\n",
        "            'ROC AUC': metrics['verification']['roc_auc'],\n",
        "            'EER': metrics['verification']['eer'],\n",
        "            'Recall@1': metrics['retrieval']['recall@1'],\n",
        "            'Recall@5': metrics['retrieval']['recall@5'],\n",
        "            'Precision@1': metrics['retrieval']['precision@1'],\n",
        "            'Precision@5': metrics['retrieval']['precision@5']\n",
        "        })\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "    print(comparison_df)\n",
        "    comparison_df.to_csv(os.path.join(comparison_base_dir, 'loss_comparison_results.csv'), index=False)\n",
        "    print(f\"Comparison results saved to {os.path.join(comparison_base_dir, 'loss_comparison_results.csv')}\")\n",
        "\n",
        "    # Plot comparison\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    metrics_to_plot = ['ROC AUC', 'Recall@1', 'Recall@5', 'Precision@1', 'Precision@5']\n",
        "    x = np.arange(len(metrics_to_plot))\n",
        "    width = 0.25\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(15, 10))\n",
        "    for i, loss_type in enumerate(loss_types):\n",
        "        values = [\n",
        "            results[loss_type]['verification']['roc_auc'],\n",
        "            results[loss_type]['retrieval']['recall@1'],\n",
        "            results[loss_type]['retrieval']['recall@5'],\n",
        "            results[loss_type]['retrieval']['precision@1'],\n",
        "            results[loss_type]['retrieval']['precision@5']\n",
        "        ]\n",
        "        rects = ax.bar(x + i*width - width, values, width, label=loss_type)\n",
        "        ax.bar_label(rects, padding=3, fmt='%.3f')\n",
        "\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Comparison of Loss Functions')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(metrics_to_plot)\n",
        "    ax.legend()\n",
        "    ax.grid(True, axis='y')\n",
        "    ax.set_ylim(0, 1.1) # Adjust ylim for better visualization\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.savefig(os.path.join(comparison_base_dir, 'loss_function_comparison.png'))\n",
        "    plt.show()\n",
        "\n",
        "    return results, comparison_df\n",
        "\n",
        "# Uncomment to run the comparison\n",
        "# loss_comparison_results, loss_comparison_df = compare_loss_functions()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOy7GMxYf7Lx"
      },
      "source": [
        "\n",
        "\n",
        "## Bonus: Streamlit Demo\n",
        "\n",
        "The Streamlit demo code has been moved to a separate file: `Charles_Watson_streamlit_pet_similarity_app.ipynb`.\n",
        "\n",
        "To run the demo locally:\n",
        "1. Ensure you have Streamlit installed (`pip install streamlit`).\n",
        "2. First convert the notebook to a Python script: `jupyter nbconvert --to python Charles_Watson_streamlit_pet_similarity_app.ipynb`\n",
        "3. Make sure a trained model file (e.g., `pet_metric_learning_resnet18_triplet.pth`) exists in the same directory.\n",
        "4. Run the command: `streamlit run Charles_Watson_streamlit_pet_similarity_app.py`\n",
        "\n",
        "To run the demo in Google Colab:\n",
        "1. Upload the `Charles_Watson_streamlit_pet_similarity_app.ipynb` notebook to your Colab environment.\n",
        "2. Install required packages:\n",
        "```python\n",
        "!pip install streamlit pyngrok\n",
        "```\n",
        "3. Convert the notebook to a Python script:\n",
        "```python\n",
        "!jupyter nbconvert --to python Charles_Watson_streamlit_pet_similarity_app.ipynb\n",
        "```\n",
        "4. Run Streamlit with ngrok for public access:\n",
        "```python\n",
        "from pyngrok import ngrok\n",
        "!streamlit run Charles_Watson_streamlit_pet_similarity_app.py &>/dev/null&\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app is running at: {public_url}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUxjScOcf7Lx"
      },
      "source": [
        "\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "In this notebook, we have implemented a comprehensive metric learning pipeline for pet breed classification using the Oxford-IIIT Pet Dataset. We have:\n",
        "\n",
        "1. Built a custom embedding model with a CNN backbone and projection head\n",
        "2. Implemented various loss functions for metric learning (Triplet, Contrastive, ArcFace)\n",
        "3. Developed evaluation methods for verification, retrieval, and few-shot classification\n",
        "4. Created visualization tools for embedding spaces and feature importance (Grad-CAM)\n",
        "5. Included bonus implementations for hard negative mining and loss function comparison\n",
        "6. Moved the Streamlit demo to a separate `Charles_Watson_streamlit_pet_similarity_app.ipynb` file.\n",
        "\n",
        "The code is modular and can be easily adapted for different settings and experimentation. To run the complete training and evaluation pipeline, simply execute the `main()` function."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}