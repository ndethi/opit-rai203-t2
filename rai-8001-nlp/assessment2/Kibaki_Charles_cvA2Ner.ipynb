{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MasakhaNER Project: Named Entity Recognition for African Languages\n",
    "\n",
    "## Introduction and Problem Statement\n",
    "\n",
    "This notebook explores Named Entity Recognition (NER) for low-resource African languages using the MasakhaNER dataset. NER is a fundamental NLP task that identifies and classifies named entities in text into predefined categories such as person names, organizations, locations, and dates.\n",
    "\n",
    "The MasakhaNER dataset covers 10 African languages: Amharic, Hausa, Igbo, Kinyarwanda, Luganda, Luo, Nigerian-Pidgin, Swahili, Wolof, and Yorùbá. This project addresses the challenge of developing effective NLP tools for languages with limited digital resources.\n",
    "\n",
    "### Project Goals:\n",
    "1. Explore and analyze the MasakhaNER dataset\n",
    "2. Implement preprocessing pipelines for African languages\n",
    "3. Develop and compare different NER models\n",
    "4. Evaluate model performance across languages\n",
    "5. Identify challenges and opportunities for low-resource NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preparation\n",
    "\n",
    "We'll download the MasakhaNER dataset and load it into our environment. The dataset is structured in CoNLL format, with one token per line and entity tags in BIO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_masakhaner_dataset():\n",
    "    \"\"\"\n",
    "    Download the MasakhaNER dataset from GitHub if not already present\n",
    "    \n",
    "    Returns:\n",
    "        Path to the dataset directory\n",
    "    \"\"\"\n",
    "    # Define the URL and directory\n",
    "    url = \"https://github.com/masakhane-io/masakhane-ner/archive/refs/heads/main.zip\"\n",
    "    data_dir = \"masakhane-ner-data\"\n",
    "    \n",
    "    # Check if directory already exists\n",
    "    if os.path.exists(data_dir):\n",
    "        print(f\"Dataset directory already exists at {data_dir}\")\n",
    "        return data_dir\n",
    "    \n",
    "    # Download and extract the dataset\n",
    "    print(\"Downloading MasakhaNER dataset...\")\n",
    "    response = requests.get(url)\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "        # Extract only the data directory to save space\n",
    "        for file in zip_ref.namelist():\n",
    "            if file.startswith('masakhane-ner-main/data/'):\n",
    "                zip_ref.extract(file, '.')\n",
    "    \n",
    "    # Rename the directory for easier access\n",
    "    os.rename('masakhane-ner-main/data', data_dir)\n",
    "    \n",
    "    # Clean up\n",
    "    os.rmdir('masakhane-ner-main')\n",
    "    \n",
    "    print(f\"Dataset downloaded and extracted to {data_dir}\")\n",
    "    return data_dir\n",
    "\n",
    "def load_masakhaner_data(data_dir, languages=None, sample_size=None):\n",
    "    \"\"\"\n",
    "    Load the MasakhaNER dataset for specified languages.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to the directory containing MasakhaNER data\n",
    "        languages: List of languages to load (if None, load all available)\n",
    "        sample_size: If specified, load only this many examples per language and split\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping language codes to their respective datasets\n",
    "    \"\"\"\n",
    "    if languages is None:\n",
    "        # All 10 languages in MasakhaNER\n",
    "        languages = ['amh', 'hau', 'ibo', 'kin', 'lug', 'luo', 'pcm', 'swa', 'wol', 'yor']\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    for lang in languages:\n",
    "        lang_data = []\n",
    "        \n",
    "        # Load train, dev, and test sets\n",
    "        for split in ['train', 'dev', 'test']:\n",
    "            file_path = os.path.join(data_dir, lang, f\"{split}.txt\")\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Warning: File not found: {file_path}\")\n",
    "                continue\n",
    "                \n",
    "            current_sentence = []\n",
    "            sentences = []\n",
    "            \n",
    "            # Read the CoNLL formatted file\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    \n",
    "                    if line:\n",
    "                        # Parse CoNLL format: word, tag\n",
    "                        parts = line.split()\n",
    "                        if len(parts) >= 2:\n",
    "                            word, tag = parts[0], parts[-1]  # Last column is the NER tag\n",
    "                            current_sentence.append((word, tag))\n",
    "                    elif current_sentence:\n",
    "                        # End of sentence\n",
    "                        sentences.append(current_sentence)\n",
    "                        current_sentence = []\n",
    "            \n",
    "            # Don't forget the last sentence if file doesn't end with an empty line\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "            \n",
    "            # Sample if needed\n",
    "            if sample_size and len(sentences) > sample_size:\n",
    "                sentences = sentences[:sample_size]\n",
    "            \n",
    "            # Convert to DataFrame format\n",
    "            for sentence in sentences:\n",
    "                lang_data.append({\n",
    "                    'tokens': [word for word, _ in sentence],\n",
    "                    'tags': [tag for _, tag in sentence],\n",
    "                    'split': split\n",
    "                })\n",
    "        \n",
    "        datasets[lang] = pd.DataFrame(lang_data)\n",
    "        print(f\"Loaded {len(datasets[lang])} sentences for {lang}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Download and load the dataset\n",
    "data_dir = download_masakhaner_dataset()\n",
    "\n",
    "# For quick testing, let's use a smaller sample. Remove the sample_size parameter for the full dataset.\n",
    "# For final analysis, use all data with: datasets = load_masakhaner_data(data_dir)\n",
    "datasets = load_masakhaner_data(data_dir, sample_size=100)\n",
    "\n",
    "# Display a sample from Swahili dataset\n",
    "print(\"\\nSample from Swahili (swa) dataset:\")\n",
    "sample_row = datasets['swa'].iloc[0]\n",
    "print(f\"Tokens: {sample_row['tokens']}\")\n",
    "print(f\"Tags: {sample_row['tags']}\")\n",
    "print(f\"Split: {sample_row['split']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Now, let's explore the MasakhaNER dataset to understand its characteristics across different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(datasets):\n",
    "    \"\"\"\n",
    "    Perform exploratory analysis on the MasakhaNER dataset\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dictionary mapping language codes to their respective datasets\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of DataFrames with dataset statistics and entity distribution\n",
    "    \"\"\"\n",
    "    # Overall statistics\n",
    "    print(\"Dataset Statistics:\")\n",
    "    stats = {}\n",
    "    \n",
    "    for lang, df in datasets.items():\n",
    "        # Language name mapping for better readability\n",
    "        lang_names = {\n",
    "            'amh': 'Amharic', 'hau': 'Hausa', 'ibo': 'Igbo', \n",
    "            'kin': 'Kinyarwanda', 'lug': 'Luganda', 'luo': 'Luo',\n",
    "            'pcm': 'Nigerian-Pidgin', 'swa': 'Swahili', 'wol': 'Wolof', \n",
    "            'yor': 'Yorùbá'\n",
    "        }\n",
    "        \n",
    "        # Count sentences, tokens, and entities\n",
    "        num_sentences = len(df)\n",
    "        num_tokens = sum(df['tokens'].apply(len))\n",
    "        \n",
    "        # Count entities (non-O tags)\n",
    "        all_tags = [tag for tags in df['tags'] for tag in tags]\n",
    "        entity_tags = [tag for tag in all_tags if tag != 'O']\n",
    "        num_entities = len(entity_tags)\n",
    "        \n",
    "        # Count entity types\n",
    "        entity_types = set([tag.split('-')[1] if '-' in tag else tag for tag in entity_tags if tag != 'O'])\n",
    "        \n",
    "        # Average sentence length\n",
    "        avg_sentence_len = num_tokens / num_sentences if num_sentences > 0 else 0\n",
    "        \n",
    "        # Store statistics\n",
    "        stats[lang_names.get(lang, lang)] = {\n",
    "            'sentences': num_sentences,\n",
    "            'tokens': num_tokens,\n",
    "            'entities': num_entities,\n",
    "            'entity_density': num_entities / num_tokens if num_tokens > 0 else 0,\n",
    "            'entity_types': ', '.join(sorted(entity_types)) if entity_types else 'None',\n",
    "            'avg_sentence_len': avg_sentence_len\n",
    "        }\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats).T\n",
    "    \n",
    "    # Format the columns for better readability\n",
    "    stats_df['entity_density'] = stats_df['entity_density'].map('{:.2%}'.format)\n",
    "    stats_df['avg_sentence_len'] = stats_df['avg_sentence_len'].map('{:.1f}'.format)\n",
    "    \n",
    "    print(stats_df)\n",
    "    \n",
    "    # Visualize entity type distribution\n",
    "    entity_counts = {}\n",
    "    \n",
    "    for lang, df in datasets.items():\n",
    "        lang_name = {'amh': 'Amharic', 'hau': 'Hausa', 'ibo': 'Igbo', \n",
    "                    'kin': 'Kinyarwanda', 'lug': 'Luganda', 'luo': 'Luo',\n",
    "                    'pcm': 'Nigerian-Pidgin', 'swa': 'Swahili', 'wol': 'Wolof', \n",
    "                    'yor': 'Yorùbá'}.get(lang, lang)\n",
    "        \n",
    "        # Get all entity tags and count them by type\n",
    "        all_tag_types = []\n",
    "        for tags in df['tags']:\n",
    "            for tag in tags:\n",
    "                if tag != 'O':  # Skip non-entity tags\n",
    "                    # Extract the entity type (e.g., PER from B-PER)\n",
    "                    entity_type = tag.split('-')[1] if '-' in tag else tag\n",
    "                    all_tag_types.append(entity_type)\n",
    "        \n",
    "        entity_counts[lang_name] = Counter(all_tag_types)\n",
    "    \n",
    "    # Convert to DataFrame for easier plotting\n",
    "    entity_df = pd.DataFrame(entity_counts).fillna(0)\n",
    "    \n",
    "    # Plot entity type distribution\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    entity_df.plot(kind='bar', stacked=True)\n",
    "    plt.title('Entity Type Distribution Across Languages')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Entity Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot entity density by language\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    entity_density = pd.Series({k: float(v['entity_density'].strip('%'))/100 \n",
    "                               for k, v in stats_df['entity_density'].items()})\n",
    "    entity_density.sort_values().plot(kind='bar')\n",
    "    plt.title('Entity Density by Language')\n",
    "    plt.ylabel('Entity Density (entities/token)')\n",
    "    plt.xlabel('Language')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot average sentence length by language\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    avg_sent_len = pd.Series({k: float(v) for k, v in stats_df['avg_sentence_len'].items()})\n",
    "    avg_sent_len.sort_values().plot(kind='bar')\n",
    "    plt.title('Average Sentence Length by Language')\n",
    "    plt.ylabel('Average Tokens per Sentence')\n",
    "    plt.xlabel('Language')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return stats_df, entity_df\n",
    "\n",
    "# Run exploratory data analysis\n",
    "stats_df, entity_df = explore_dataset(datasets)\n",
    "\n",
    "# Additional analysis: Look at tag distribution for a specific language\n",
    "def analyze_entity_distribution(df, lang_name):\n",
    "    \"\"\"Analyze entity distribution for a specific language\"\"\"\n",
    "    all_tags = [tag for tags in df['tags'] for tag in tags]\n",
    "    tag_counts = Counter(all_tags)\n",
    "    \n",
    "    # Create a DataFrame for better display\n",
    "    tag_df = pd.DataFrame({\n",
    "        'Tag': list(tag_counts.keys()),\n",
    "        'Count': list(tag_counts.values())\n",
    "    })\n",
    "    tag_df = tag_df.sort_values('Count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total = tag_df['Count'].sum()\n",
    "    tag_df['Percentage'] = tag_df['Count'] / total * 100\n",
    "    \n",
    "    print(f\"\\nEntity tag distribution for {lang_name}:\")\n",
    "    print(tag_df)\n",
    "    \n",
    "    # Plot the distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = tag_df.head(10).plot(kind='bar', x='Tag', y='Count')\n",
    "    plt.title(f'Top 10 Entity Tags for {lang_name}')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Tag')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return tag_df\n",
    "\n",
    "# Analyze entity distribution for Swahili\n",
    "swahili_tag_df = analyze_entity_distribution(datasets['swa'], 'Swahili')\n",
    "\n",
    "# Analyze token distribution (how many unique tokens)\n",
    "def analyze_token_distribution(datasets):\n",
    "    \"\"\"Analyze token distributions across languages\"\"\"\n",
    "    token_stats = {}\n",
    "    \n",
    "    for lang, df in datasets.items():\n",
    "        lang_name = {'amh': 'Amharic', 'hau': 'Hausa', 'ibo': 'Igbo', \n",
    "                    'kin': 'Kinyarwanda', 'lug': 'Luganda', 'luo': 'Luo',\n",
    "                    'pcm': 'Nigerian-Pidgin', 'swa': 'Swahili', 'wol': 'Wolof', \n",
    "                    'yor': 'Yorùbá'}.get(lang, lang)\n",
    "        \n",
    "        # Get all tokens\n",
    "        all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "        unique_tokens = set(all_tokens)\n",
    "        \n",
    "        token_stats[lang_name] = {\n",
    "            'total_tokens': len(all_tokens),\n",
    "            'unique_tokens': len(unique_tokens),\n",
    "            'vocabulary_ratio': len(unique_tokens) / len(all_tokens) if all_tokens else 0\n",
    "        }\n",
    "    \n",
    "    token_stats_df = pd.DataFrame(token_stats).T\n",
    "    token_stats_df['vocabulary_ratio'] = token_stats_df['vocabulary_ratio'].map('{:.2%}'.format)\n",
    "    \n",
    "    print(\"\\nToken Distribution Statistics:\")\n",
    "    print(token_stats_df)\n",
    "    \n",
    "    # Plot vocabulary ratio by language\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    vocab_ratio = pd.Series({k: float(v.strip('%'))/100 \n",
    "                            for k, v in token_stats_df['vocabulary_ratio'].items()})\n",
    "    vocab_ratio.sort_values().plot(kind='bar')\n",
    "    plt.title('Vocabulary Ratio by Language (unique tokens/total tokens)')\n",
    "    plt.ylabel('Vocabulary Ratio')\n",
    "    plt.xlabel('Language')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return token_stats_df\n",
    "\n",
    "# Analyze token distribution\n",
    "token_stats_df = analyze_token_distribution(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Now, let's preprocess the data to prepare it for model training. This involves:\n",
    "1. Converting tags to numerical indices\n",
    "2. Creating token vocabularies \n",
    "3. Preparing data loaders for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(datasets):\n",
    "    \"\"\"\n",
    "    Preprocess the MasakhaNER dataset for modeling.\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dictionary mapping language codes to their respective datasets\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed datasets ready for modeling\n",
    "    \"\"\"\n",
    "    preprocessed = {}\n",
    "    \n",
    "    for lang, df in datasets.items():\n",
    "        print(f\"Preprocessing {lang}...\")\n",
    "        \n",
    "        # Create tag vocabulary - map each tag to a unique index\n",
    "        all_tags = sorted(set([tag for tags in df['tags'] for tag in tags]))\n",
    "        tag2idx = {tag: idx for idx, tag in enumerate(all_tags)}\n",
    "        idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "        \n",
    "        # Create word vocabulary - map each word to a unique index\n",
    "        all_words = sorted(set([word.lower() for tokens in df['tokens'] for word in tokens]))\n",
    "        word2idx = {word: idx+1 for idx, word in enumerate(all_words)}  # Reserve 0 for padding\n",
    "        \n",
    "        # Add preprocessed data\n",
    "        preprocessed[lang] = {\n",
    "            'data': df,\n",
    "            'tag2idx': tag2idx,\n",
    "            'idx2tag': idx2tag,\n",
    "            'word2idx': word2idx,\n",
    "            'n_tags': len(tag2idx),\n",
    "            'n_words': len(word2idx)\n",
    "        }\n",
    "        \n",
    "        # Create train/val/test splits (MasakhaNER already has these)\n",
    "        train_df = df[df['split'] == 'train'].reset_index(drop=True)\n",
    "        val_df = df[df['split'] == 'dev'].reset_index(drop=True)\n",
    "        test_df = df[df['split'] == 'test'].reset_index(drop=True)\n",
    "        \n",
    "        preprocessed[lang]['train'] = train_df\n",
    "        preprocessed[lang]['val'] = val_df\n",
    "        preprocessed[lang]['test'] = test_df\n",
    "        \n",
    "        print(f\"  Vocabulary size: {len(word2idx)} words\")\n",
    "        print(f\"  Tag set size: {len(tag2idx)} tags\")\n",
    "        print(f\"  Train set: {len(train_df)} sentences\")\n",
    "        print(f\"  Validation set: {len(val_df)} sentences\")\n",
    "        print(f\"  Test set: {len(test_df)} sentences\")\n",
    "    \n",
    "    return preprocessed\n",
    "\n",
    "# Custom dataset class for NER\n",
    "class NERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for Named Entity Recognition\n",
    "    \n",
    "    This class handles:\n",
    "    - Converting tokens to indices\n",
    "    - Converting tags to indices  \n",
    "    - Padding sequences to a fixed length\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=128):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.word2idx = word2idx\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        words = self.sentences[idx]\n",
    "        tags = self.tags[idx]\n",
    "        \n",
    "        # Convert words and tags to indices\n",
    "        # For unknown words, use len(word2idx) as the index\n",
    "        word_ids = [self.word2idx.get(word.lower(), len(self.word2idx)) for word in words]\n",
    "        tag_ids = [self.tag2idx[tag] for tag in tags]\n",
    "        \n",
    "        # Handle sequences longer than max_len by truncating\n",
    "        if len(word_ids) > self.max_len:\n",
    "            word_ids = word_ids[:self.max_len]\n",
    "            tag_ids = tag_ids[:self.max_len]\n",
    "        \n",
    "        # Create attention mask (1 for actual tokens, 0 for padding)\n",
    "        attention_mask = [1] * len(word_ids)\n",
    "        \n",
    "        # Pad shorter sequences to max_len\n",
    "        padding_length = self.max_len - len(word_ids)\n",
    "        if padding_length > 0:\n",
    "            # Pad word_ids and attention_mask with 0s\n",
    "            word_ids = word_ids + [0] * padding_length\n",
    "            attention_mask = attention_mask + [0] * padding_length\n",
    "            \n",
    "            # Pad tag_ids with -100 (ignored by the loss function)\n",
    "            tag_ids = tag_ids + [-100] * padding_length\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(word_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(tag_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def create_dataloaders(preprocessed_data, lang, batch_size=16):\n",
    "    \"\"\"\n",
    "    Create PyTorch DataLoaders for training and evaluation\n",
    "    \n",
    "    Args:\n",
    "        preprocessed_data: Preprocessed dataset dictionary\n",
    "        lang: Language code\n",
    "        batch_size: Batch size for training\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # Extract data for the language\n",
    "    lang_data = preprocessed_data[lang]\n",
    "    word2idx = lang_data['word2idx']\n",
    "    tag2idx = lang_data['tag2idx']\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = NERDataset(\n",
    "        lang_data['train']['tokens'].tolist(),\n",
    "        lang_data['train']['tags'].tolist(),\n",
    "        word2idx, tag2idx\n",
    "    )\n",
    "    \n",
    "    val_dataset = NERDataset(\n",
    "        lang_data['val']['tokens'].tolist(),\n",
    "        lang_data['val']['tags'].tolist(),\n",
    "        word2idx, tag2idx\n",
    "    )\n",
    "    \n",
    "    test_dataset = NERDataset(\n",
    "        lang_data['test']['tokens'].tolist(),\n",
    "        lang_data['test']['tags'].tolist(),\n",
    "        word2idx, tag2idx\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Preprocess the datasets\n",
    "preprocessed_data = preprocess_data(datasets)\n",
    "\n",
    "# Choose a language for model development (e.g., Swahili)\n",
    "selected_lang = 'swa'\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    preprocessed_data, selected_lang, batch_size=16\n",
    ")\n",
    "\n",
    "# Check a batch from the dataloader\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shape - input_ids: {batch['input_ids'].shape}, labels: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "\n",
    "Now let's implement different NER models, starting with a simple BiLSTM model and then exploring more complex architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM model for NER\n",
    "class BiLSTM_NER(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM model for Named Entity Recognition\n",
    "    \n",
    "    This model consists of:\n",
    "    1. An embedding layer to convert token IDs to vectors\n",
    "    2. A bidirectional LSTM to capture context in both directions\n",
    "    3. A fully connected layer to predict tag probabilities for each token\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, tag_size, embedding_dim=100, hidden_dim=128, num_layers=2, dropout=0.5):\n",
    "        super(BiLSTM_NER, self).__init__()\n",
    "        \n",
    "        # Embedding layer - converts token IDs to vectors\n",
    "        # vocab_size + 1 to account for unknown words\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,       # Input dimension\n",
    "            hidden_dim // 2,     # Hidden dimension (divided by 2 since bidirectional will double it)\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,    # Input shape is (batch_size, seq_len, features)\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected layer for tag prediction\n",
    "        self.fc = nn.Linear(hidden_dim, tag_size)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the BiLSTM model\n",
    "        \n",
    "        Args:\n",
    "            x: Token IDs of shape (batch_size, seq_len)\n",
    "            attention_mask: Optional mask for padding tokens\n",
    "            \n",
    "        Returns:\n",
    "            Tag logits of shape (batch_size, seq_len, tag_size)\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Apply attention mask if provided (for handling padding)\n",
    "        if attention_mask is not None:\n",
    "            # Expand mask to match embedding dimensions\n",
    "            mask = attention_mask.unsqueeze(-1).expand_as(x)\n",
    "            # Apply mask (multiply by 0 to mask out padding)\n",
    "            x = x * mask\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        # Pack padded sequence is not used here for simplicity\n",
    "        lstm_out, _ = self.lstm(x)  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Project to tag space\n",
    "        logits = self.fc(lstm_out)  # (batch_size, seq_len, tag_size)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "selected_lang_data = preprocessed_data[selected_lang]\n",
    "vocab_size = len(selected_lang_data['word2idx'])\n",
    "tag_size = len(selected_lang_data['tag2idx'])\n",
    "\n",
    "model = BiLSTM_NER(\n",
    "    vocab_size=vocab_size,\n",
    "    tag_size=tag_size,\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model initialized with {vocab_size} vocabulary size and {tag_size} tag size\")\n",
    "print(model)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)  # Ignore padded positions\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=10, device=device):\n",
    "    \"\"\"\n",
    "    Train the NER model\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        optimizer: PyTorch optimizer\n",
    "        criterion: Loss function\n",
    "        num_epochs: Number of training epochs\n",
    "        device: Device to run training on ('cpu' or 'cuda')\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (trained model, training history)\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} (Training)'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Reshape outputs and labels for loss calculation\n",
    "            # From (batch_size, seq_len, num_tags) to (batch_size * seq_len, num_tags)\n",
    "            batch_size, seq_len, num_tags = outputs.shape\n",
    "            outputs = outputs.view(-1, num_tags)\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            # Calculate loss (ignoring padded positions with -100)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} (Validation)'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                \n",
    "                # Reshape for loss calculation\n",
    "                outputs = outputs.view(-1, outputs.shape[-1])\n",
    "                labels = labels.view(-1)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "            print(f\"  New best model saved! (Validation Loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    # Load the best model\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train the model (using fewer epochs for demonstration)\n",
    "# In a real scenario, you would use more epochs (10-20)\n",
    "trained_model, history = train_model(\n",
    "    model, train_loader, val_loader, optimizer, criterion, num_epochs=3\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['train_loss'], label='Training Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now, let's evaluate our model on the test set and analyze its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, idx2tag, device=device):\n",
    "    \"\"\"\n",
    "    Evaluate the NER model on test data\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        test_loader: DataLoader for test data\n",
    "        idx2tag: Dictionary mapping tag indices to tag names\n",
    "        device: Device to run evaluation on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (classification report, confusion matrix, unique tags)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Evaluating'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Get predictions (argmax)\n",
    "        _, predictions = torch.max(outputs, dim=2)\n",
    "        \n",
    "        # Convert to CPU for evaluation\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        attention_mask = attention_mask.cpu().numpy()\n",
    "        \n",
    "        # Extract predictions and true labels (ignoring padded positions)\n",
    "        for i in range(predictions.shape[0]):  # Loop through each sequence in the batch\n",
    "            for j in range(predictions.shape[1]):  # Loop through each token\n",
    "                # Only include tokens that are not padding and not labeled as -100\n",
    "                if attention_mask[i, j] == 1 and labels[i, j] != -100:\n",
    "                    all_predictions.append(idx2tag[predictions[i, j]])\n",
    "                    all_true_labels.append(idx2tag[labels[i, j]])\n",
    "\n",
    "# Compute classification report\n",
    "report = classification_report(all_true_labels, all_predictions)\n",
    "\n",
    "# Compute confusion matrix for entity types (not individual BIO tags)\n",
    "# Extract entity types (ignoring the B- or I- prefix)\n",
    "true_entity_types = [tag.split('-')[1] if '-' in tag else tag for tag in all_true_labels]\n",
    "pred_entity_types = [tag.split('-')[1] if '-' in tag else tag for tag in all_predictions]\n",
    "\n",
    "# Get unique entity types\n",
    "unique_entity_types = sorted(set(true_entity_types + pred_entity_types))\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(\n",
    "    [unique_entity_types.index(tag) for tag in true_entity_types],\n",
    "    [unique_entity_types.index(tag) for tag in pred_entity_types]\n",
    ")\n",
    "\n",
    "return report, cm, unique_entity_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2tag = preprocessed_data[selected_lang]['idx2tag']\n",
    "report, cm, unique_tags = evaluate_model(trained_model, test_loader, idx2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "xticklabels=unique_tags,\n",
    "yticklabels=unique_tags)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Entity Type')\n",
    "plt.xlabel('Predicted Entity Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Performance by Entity Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_entity_performance(report):\n",
    "\n",
    "\"\"\"Extract and visualize performance metrics by entity type\"\"\"\n",
    "\n",
    "# Parse the classification report\n",
    "lines = report.split('\\n')\n",
    "entity_metrics = {}\n",
    "for line in lines:\n",
    "    if line.strip() and not line.startswith('micro') and not line.startswith('macro') and not line.startswith('weighted') and not line.startswith('accuracy') and not line.startswith('    '):\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 5:  # Tag, precision, recall, f1-score, support\n",
    "            tag = parts[0]\n",
    "            if tag == 'O' or '-' in tag:  # Only consider actual entity tags\n",
    "                precision = float(parts[1])\n",
    "                recall = float(parts[2])\n",
    "                f1 = float(parts[3])\n",
    "                support = int(parts[4])\n",
    "                \n",
    "                # For B- and I- tags, group by entity type\n",
    "                if '-' in tag:\n",
    "                    entity_type = tag.split('-')[1]\n",
    "                    if entity_type not in entity_metrics:\n",
    "                        entity_metrics[entity_type] = {'precision': [], 'recall': [], 'f1': [], 'support': 0}\n",
    "                    entity_metrics[entity_type]['precision'].append(precision)\n",
    "                    entity_metrics[entity_type]['recall'].append(recall)\n",
    "                    entity_metrics[entity_type]['f1'].append(f1)\n",
    "                    entity_metrics[entity_type]['support'] += support\n",
    "                else:\n",
    "                    entity_metrics[tag] = {\n",
    "                        'precision': [precision],\n",
    "                        'recall': [recall],\n",
    "                        'f1': [f1],\n",
    "                        'support': support\n",
    "                    }\n",
    "\n",
    "# Average the metrics for B- and I- tags of the same entity\n",
    "for entity, metrics in entity_metrics.items():\n",
    "    metrics['precision'] = sum(metrics['precision']) / len(metrics['precision'])\n",
    "    metrics['recall'] = sum(metrics['recall']) / len(metrics['recall'])\n",
    "    metrics['f1'] = sum(metrics['f1']) / len(metrics['f1'])\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Entity': list(entity_metrics.keys()),\n",
    "    'Precision': [metrics['precision'] for metrics in entity_metrics.values()],\n",
    "    'Recall': [metrics['recall'] for metrics in entity_metrics.values()],\n",
    "    'F1 Score': [metrics['f1'] for metrics in entity_metrics.values()],\n",
    "    'Support': [metrics['support'] for metrics in entity_metrics.values()]\n",
    "})\n",
    "\n",
    "# Sort by F1 score\n",
    "metrics_df = metrics_df.sort_values('F1 Score', ascending=False)\n",
    "\n",
    "# Visualize the metrics\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot precision, recall, and F1 score\n",
    "x = np.arange(len(metrics_df))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, metrics_df['Precision'], width, label='Precision')\n",
    "plt.bar(x, metrics_df['Recall'], width, label='Recall')\n",
    "plt.bar(x + width, metrics_df['F1 Score'], width, label='F1 Score')\n",
    "\n",
    "plt.xlabel('Entity Type')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics by Entity Type')\n",
    "plt.xticks(x, metrics_df['Entity'], rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot support (number of examples)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(metrics_df['Entity'], metrics_df['Support'])\n",
    "plt.xlabel('Entity Type')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.title('Number of Examples by Entity Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "return metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Performance by Entity Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_metrics_df = analyze_entity_performance(report)\n",
    "print(\"\\nPerformance Metrics by Entity Type:\")\n",
    "print(entity_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer-Based Model\n",
    "\n",
    "Now, let's implement a more advanced model using transformers. We'll use XLM-RoBERTa, which has been pre-trained on multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transformer_data(datasets, language, model_name='xlm-roberta-base'):\n",
    "    \"\"\"\n",
    "    Prepare data for transformer-based models\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dictionary containing datasets by language\n",
    "        language: Language code to prepare data for\n",
    "        model_name: Name of the pretrained transformer model\n",
    "        \n",
    "    Returns:\n",
    "        Processed data and tag mappings\n",
    "    \"\"\"\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Get dataset for the language\n",
    "    df = datasets[language]\n",
    "    \n",
    "    # Create tag mappings\n",
    "    unique_tags = sorted(set([tag for tags_list in df['tags'] for tag in tags_list]))\n",
    "    tag2id = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "    id2tag = {i: tag for tag, i in tag2id.items()}\n",
    "    \n",
    "    print(f\"Preparing transformer data for {language}\")\n",
    "    print(f\"Using model: {model_name}\")\n",
    "    print(f\"Tag set size: {len(unique_tags)}\")\n",
    "    \n",
    "    # Process splits\n",
    "    processed_data = {}\n",
    "    \n",
    "    for split in ['train', 'dev', 'test']:\n",
    "        split_df = df[df['split'] == split].reset_index(drop=True)\n",
    "        print(f\"Processing {split} split: {len(split_df)} examples\")\n",
    "        \n",
    "        encoded_data = []\n",
    "        \n",
    "        for i in tqdm(range(len(split_df)), desc=f\"Encoding {split}\"):\n",
    "            tokens = split_df.iloc[i]['tokens']\n",
    "            tags = split_df.iloc[i]['tags']\n",
    "            \n",
    "            # Encode tokens\n",
    "            encoding = tokenizer(\n",
    "                tokens,\n",
    "                is_split_into_words=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Create labels\n",
    "            labels = torch.ones(encoding['input_ids'].shape, dtype=torch.long) * -100\n",
    "            \n",
    "            # Assign labels to each token\n",
    "            word_ids = []\n",
    "            for token_idx, offset in enumerate(encoding['offset_mapping'][0]):\n",
    "                if offset[0] == 0 and offset[1] != 0:\n",
    "                    # This token is the first subword of a word\n",
    "                    word_idx = len(word_ids)\n",
    "                    word_ids.append(word_idx)\n",
    "                    if word_idx < len(tags):\n",
    "                        labels[0, token_idx] = tag2id[tags[word_idx]]\n",
    "                else:\n",
    "                    # This token is a subword or special token\n",
    "                    word_ids.append(None)\n",
    "            \n",
    "            # Remove offset_mapping as it's no longer needed\n",
    "            encoding.pop('offset_mapping')\n",
    "            \n",
    "            # Add labels to encoding\n",
    "            encoding['labels'] = labels\n",
    "            \n",
    "            # Convert to plain Python types for easier handling\n",
    "            item = {key: val.squeeze().tolist() for key, val in encoding.items()}\n",
    "            \n",
    "            encoded_data.append(item)\n",
    "        \n",
    "        processed_data[split] = encoded_data\n",
    "    \n",
    "    return processed_data, tag2id, id2tag\n",
    "\n",
    "# Custom dataset for transformer-based models\n",
    "class TransformerNERDataset(Dataset):\n",
    "    \"\"\"Dataset for transformer-based NER models\"\"\"\n",
    "    def __init__(self, encoded_data):\n",
    "        self.encoded_data = encoded_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encoded_data[idx]['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.encoded_data[idx]['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.encoded_data[idx]['labels'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Prepare transformer data for a selected language\n",
    "transformer_data, transformer_tag2id, transformer_id2tag = prepare_transformer_data(\n",
    "    datasets, selected_lang, model_name='xlm-roberta-base'\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "transformer_train_dataset = TransformerNERDataset(transformer_data['train'])\n",
    "transformer_val_dataset = TransformerNERDataset(transformer_data['dev'])\n",
    "transformer_test_dataset = TransformerNERDataset(transformer_data['test'])\n",
    "\n",
    "transformer_train_loader = DataLoader(transformer_train_dataset, batch_size=8, shuffle=True)\n",
    "transformer_val_loader = DataLoader(transformer_val_dataset, batch_size=8)\n",
    "transformer_test_loader = DataLoader(transformer_test_dataset, batch_size=8)\n",
    "\n",
    "# Initialize the transformer model\n",
    "transformer_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'xlm-roberta-base', \n",
    "    num_labels=len(transformer_tag2id)\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "transformer_optimizer = torch.optim.AdamW(\n",
    "    transformer_model.parameters(), \n",
    "    lr=5e-5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Define learning rate scheduler\n",
    "total_steps = len(transformer_train_loader) * 3  # 3 epochs\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    transformer_optimizer,\n",
    "    max_lr=5e-5,\n",
    "    total_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training function for transformer model\n",
    "def train_transformer(model, train_loader, val_loader, optimizer, scheduler, num_epochs=3, device=device):\n",
    "    \"\"\"Train the transformer model\"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} (Training)'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} (Validation)'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                val_loss += outputs.loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "            print(f\"  New best model saved! (Validation Loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    # Load the best model\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train the transformer model (for demonstration, using only 1 epoch)\n",
    "# In a real scenario, use 3-5 epochs\n",
    "trained_transformer, transformer_history = train_transformer(\n",
    "    transformer_model, \n",
    "    transformer_train_loader, \n",
    "    transformer_val_loader,\n",
    "    transformer_optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=1  # Use more epochs for better results\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(transformer_history['train_loss'], label='Training Loss')\n",
    "plt.plot(transformer_history['val_loss'], label='Validation Loss')\n",
    "plt.title('Transformer Model Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate transformer model\n",
    "def evaluate_transformer(model, test_loader, id2tag, device=device):\n",
    "    \"\"\"Evaluate the transformer model on test data\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Get predictions (argmax)\n",
    "            predictions = torch.argmax(outputs.logits, dim=2)\n",
    "            \n",
    "            # Convert to CPU for evaluation\n",
    "            predictions = predictions.cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            attention_mask = attention_mask.cpu().numpy()\n",
    "            \n",
    "            # Extract predictions and true labels (ignoring special tokens and padded positions)\n",
    "            for i in range(predictions.shape[0]):\n",
    "                for j in range(predictions.shape[1]):\n",
    "                    # Only consider tokens with actual labels (not -100)\n",
    "                    if labels[i, j] != -100 and attention_mask[i, j] == 1:\n",
    "                        all_predictions.append(id2tag[predictions[i, j]])\n",
    "                        all_true_labels.append(id2tag[labels[i, j]])\n",
    "    \n",
    "    # Compute classification report\n",
    "    report = classification_report(all_true_labels, all_predictions)\n",
    "    \n",
    "    # Extract entity types (ignoring the B- or I- prefix)\n",
    "    true_entity_types = [tag.split('-')[1] if '-' in tag else tag for tag in all_true_labels]\n",
    "    pred_entity_types = [tag.split('-')[1] if '-' in tag else tag for tag in all_predictions]\n",
    "    \n",
    "    # Get unique entity types\n",
    "    unique_entity_types = sorted(set(true_entity_types + pred_entity_types))\n",
    "    \n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        [unique_entity_types.index(tag) for tag in true_entity_types],\n",
    "        [unique_entity_types.index(tag) for tag in pred_entity_types]\n",
    "    )\n",
    "    \n",
    "    return report, cm, unique_entity_types\n",
    "\n",
    "# Evaluate the transformer model\n",
    "transformer_report, transformer_cm, transformer_unique_tags = evaluate_transformer(\n",
    "    trained_transformer, \n",
    "    transformer_test_loader, \n",
    "    transformer_id2tag\n",
    ")\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nTransformer Model Classification Report:\")\n",
    "print(transformer_report)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(transformer_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=transformer_unique_tags, \n",
    "            yticklabels=transformer_unique_tags)\n",
    "plt.title('Transformer Model Confusion Matrix')\n",
    "plt.ylabel('True Entity Type')\n",
    "plt.xlabel('Predicted Entity Type')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze transformer model performance by entity type\n",
    "transformer_entity_metrics_df = analyze_entity_performance(transformer_report)\n",
    "print(\"\\nTransformer Performance Metrics by Entity Type:\")\n",
    "print(transformer_entity_metrics_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
