{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment 2: Metric Learning with Oxford-IIIT Pet Dataset\n",
    "\n",
    "## Introduction and Setup\n",
    "\n",
    "This notebook implements a deep metric learning approach for the Oxford-IIIT Pet Dataset, focusing on learning an embedding space where similar pet breeds are close together and dissimilar ones are far apart. We'll explore different loss functions, evaluate the model on verification, retrieval, and few-shot classification tasks, and visualize the embedding space.\n",
    "\n",
    "### Environment Setup and Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if running in Colab (to install dependencies and set up environment)\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Install required packages\n",
    "if IN_COLAB:\n",
    "    !pip install pytorch-metric-learning\n",
    "    !pip install faiss-gpu\n",
    "    !pip install umap-learn\n",
    "    !pip install matplotlib seaborn scikit-learn tqdm\n",
    "    !pip install gradio\n",
    "    !pip install grad-cam\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import itertools  # For generating pairs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import pytorch_metric_learning\n",
    "from pytorch_metric_learning import losses, miners, distances, reducers, testers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import umap\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Data Loading and Preprocessing\n",
    "\n",
    "In this section, we'll load the Oxford-IIIT Pet Dataset, perform necessary preprocessing, and create appropriate data loaders for our metric learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to load the dataset\n",
    "def load_oxford_pets_dataset(root=\"./data\", download=True):\n",
    "    train_val_dataset = datasets.OxfordIIITPet(\n",
    "        root=root, \n",
    "        split=\"trainval\", \n",
    "        transform=train_transform, \n",
    "        download=download\n",
    "    )\n",
    "    \n",
    "    test_dataset = datasets.OxfordIIITPet(\n",
    "        root=root, \n",
    "        split=\"test\", \n",
    "        transform=eval_transform, \n",
    "        download=download\n",
    "    )\n",
    "    \n",
    "    # For evaluation, create a version of the training set with eval transforms\n",
    "    eval_train_dataset = datasets.OxfordIIITPet(\n",
    "        root=root, \n",
    "        split=\"trainval\", \n",
    "        transform=eval_transform, \n",
    "        download=False\n",
    "    )\n",
    "    \n",
    "    return train_val_dataset, test_dataset, eval_train_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset Preparation for Different Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data for training, validation and few-shot evaluation\n",
    "def prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset, num_holdout_classes=5, val_ratio=0.2):\n",
    "    # Get the class names\n",
    "    class_to_idx = train_val_dataset.class_to_idx\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    num_classes = len(class_to_idx)\n",
    "    \n",
    "    # Split classes for few-shot learning (hold out some classes for testing)\n",
    "    all_class_indices = list(range(num_classes))\n",
    "    holdout_class_indices = random.sample(all_class_indices, num_holdout_classes)\n",
    "    training_class_indices = [i for i in all_class_indices if i not in holdout_class_indices]\n",
    "    \n",
    "    holdout_classes = [idx_to_class[i] for i in holdout_class_indices]\n",
    "    print(f\"Holdout classes for few-shot learning: {holdout_classes}\")\n",
    "    \n",
    "    # Create datasets excluding holdout classes for main training\n",
    "    train_val_indices = [i for i, (_, label) in enumerate(train_val_dataset) if label not in holdout_class_indices]\n",
    "    test_indices = [i for i, (_, label) in enumerate(test_dataset) if label not in holdout_class_indices]\n",
    "    eval_train_indices = [i for i, (_, label) in enumerate(eval_train_dataset) if label not in holdout_class_indices]\n",
    "    \n",
    "    # For few-shot learning, include only holdout classes\n",
    "    few_shot_train_indices = [i for i, (_, label) in enumerate(train_val_dataset) if label in holdout_class_indices]\n",
    "    few_shot_test_indices = [i for i, (_, label) in enumerate(test_dataset) if label in holdout_class_indices]\n",
    "    \n",
    "    # Split train/val\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        train_val_indices, \n",
    "        test_size=val_ratio, \n",
    "        stratify=[train_val_dataset[i][1] for i in train_val_indices],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create Subset datasets\n",
    "    train_dataset = Subset(train_val_dataset, train_indices)\n",
    "    val_dataset = Subset(train_val_dataset, val_indices)\n",
    "    test_filtered_dataset = Subset(test_dataset, test_indices)\n",
    "    eval_train_dataset = Subset(eval_train_dataset, eval_train_indices)\n",
    "    \n",
    "    # Create datasets for few-shot learning\n",
    "    few_shot_train_dataset = Subset(train_val_dataset, few_shot_train_indices)\n",
    "    few_shot_test_dataset = Subset(test_dataset, few_shot_test_indices)\n",
    "    \n",
    "    # Create dictionary for class mapping\n",
    "    class_mapping = {\n",
    "        'class_to_idx': class_to_idx,\n",
    "        'idx_to_class': idx_to_class,\n",
    "        'holdout_class_indices': holdout_class_indices,\n",
    "        'training_class_indices': training_class_indices\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset,\n",
    "        'test': test_filtered_dataset,\n",
    "        'eval_train': eval_train_dataset,\n",
    "        'few_shot_train': few_shot_train_dataset,\n",
    "        'few_shot_test': few_shot_test_dataset,\n",
    "        'class_mapping': class_mapping\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataloaders(datasets_dict, batch_size=32, num_workers=2):\n",
    "    dataloaders = {}\n",
    "    \n",
    "    for key in ['train', 'val', 'test', 'eval_train', 'few_shot_train', 'few_shot_test']:\n",
    "        if key == 'train':\n",
    "            shuffle = True\n",
    "        else:\n",
    "            shuffle = False\n",
    "            \n",
    "        dataloaders[key] = DataLoader(\n",
    "            datasets_dict[key],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    return dataloaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "train_val_dataset, test_dataset, eval_train_dataset = load_oxford_pets_dataset()\n",
    "print(f\"Train+Val size: {len(train_val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")\n",
    "\n",
    "# Prepare datasets for different tasks\n",
    "datasets_dict = prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32  # Adjust based on your GPU/memory constraints\n",
    "dataloaders = create_dataloaders(datasets_dict, batch_size=batch_size)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "for key, dataloader in dataloaders.items():\n",
    "    print(f\"{key}: {len(dataloader.dataset)} samples\")\n",
    "\n",
    "class_mapping = datasets_dict['class_mapping']\n",
    "num_classes = len(class_mapping['class_to_idx'])\n",
    "print(f\"Total number of classes: {num_classes}\")\n",
    "print(f\"Number of training classes: {len(class_mapping['training_class_indices'])}\")\n",
    "print(f\"Number of few-shot classes: {len(class_mapping['holdout_class_indices'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "In this section, we'll define our metric learning model architecture using a CNN backbone and a projection head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet18', embedding_size=128, pretrained=True):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        \n",
    "        # Get backbone and its output size\n",
    "        self.backbone, backbone_output_size = self._get_backbone(backbone_name, pretrained)\n",
    "        \n",
    "        # Projection head (MLP)\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(backbone_output_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, embedding_size)\n",
    "        )\n",
    "        \n",
    "    def _get_backbone(self, backbone_name, pretrained):\n",
    "        \"\"\"\n",
    "        Create a backbone network from various architectures\n",
    "        \"\"\"\n",
    "        if backbone_name == 'resnet18':\n",
    "            backbone = models.resnet18(pretrained=pretrained)\n",
    "            output_size = 512\n",
    "        elif backbone_name == 'resnet34':\n",
    "            backbone = models.resnet34(pretrained=pretrained)\n",
    "            output_size = 512\n",
    "        elif backbone_name == 'resnet50':\n",
    "            backbone = models.resnet50(pretrained=pretrained)\n",
    "            output_size = 2048\n",
    "        elif backbone_name == 'efficientnet_b0':\n",
    "            backbone = models.efficientnet_b0(pretrained=pretrained)\n",
    "            output_size = 1280\n",
    "        elif backbone_name == 'mobilenet_v2':\n",
    "            backbone = models.mobilenet_v2(pretrained=pretrained)\n",
    "            output_size = 1280\n",
    "        elif backbone_name == 'densenet121':\n",
    "            backbone = models.densenet121(pretrained=pretrained)\n",
    "            output_size = 1024\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
    "        \n",
    "        # For ResNet models\n",
    "        if backbone_name.startswith('resnet'):\n",
    "            # Remove the classification layer\n",
    "            backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        # For EfficientNet\n",
    "        elif backbone_name.startswith('efficientnet'):\n",
    "            backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        # For MobileNet\n",
    "        elif backbone_name.startswith('mobilenet'):\n",
    "            backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        # For DenseNet\n",
    "        elif backbone_name.startswith('densenet'):\n",
    "            backbone = nn.Sequential(\n",
    "                backbone.features,\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AdaptiveAvgPool2d((1, 1))\n",
    "            )\n",
    "        \n",
    "        return backbone, output_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        embeddings = self.projection_head(features)\n",
    "        \n",
    "        # Normalize embeddings to unit length (important for cosine distance)\n",
    "        normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return normalized_embeddings\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loss Function Implementation\n",
    "\n",
    "Here we'll implement several loss functions for metric learning including Triplet Loss, Contrastive Loss, and ArcFace. We'll also implement miners for efficient training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_loss_and_miner(loss_type, margin=0.2, embedding_size=128, num_classes=32):\n",
    "    \"\"\"\n",
    "    Create loss function and miner for metric learning\n",
    "    \"\"\"\n",
    "    if loss_type == 'triplet':\n",
    "        # Triplet loss with cosine distance\n",
    "        distance = distances.CosineSimilarity()\n",
    "        reducer = reducers.ThresholdReducer(low=0)\n",
    "        loss_func = losses.TripletMarginLoss(margin=margin, distance=distance, reducer=reducer)\n",
    "        mining_func = miners.TripletMarginMiner(margin=margin, distance=distance, type_of_triplets=\"semihard\")\n",
    "        \n",
    "    elif loss_type == 'contrastive':\n",
    "        # Contrastive loss\n",
    "        distance = distances.CosineSimilarity()\n",
    "        loss_func = losses.ContrastiveLoss(pos_margin=0.8, neg_margin=0.2, distance=distance)\n",
    "        mining_func = miners.PairMarginMiner(pos_margin=0.8, neg_margin=0.2, distance=distance)\n",
    "        \n",
    "    elif loss_type == 'arcface':\n",
    "        # ArcFace loss\n",
    "        loss_func = losses.ArcFaceLoss(embedding_size, num_classes, margin=28.6, scale=64)\n",
    "        mining_func = None\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss type: {loss_type}\")\n",
    "        \n",
    "    return loss_func, mining_func\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline with Learning Rate Scheduler and Early Stopping\n",
    "\n",
    "Here we implement learning rate scheduling and early stopping mechanisms to improve training efficiency and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_and_scheduler(model, learning_rate=1e-4, weight_decay=0.0001):\n",
    "    \"\"\"\n",
    "    Create optimizer and learning rate scheduler\n",
    "    \n",
    "    Args:\n",
    "        model: The model to optimize\n",
    "        learning_rate: Initial learning rate\n",
    "        weight_decay: L2 regularization strength\n",
    "        \n",
    "    Returns:\n",
    "        optimizer: The optimizer object\n",
    "        scheduler: The learning rate scheduler\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler that reduces LR when validation loss plateaus\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5,  # reduce LR by half when plateauing\n",
    "        patience=3,   # wait 3 epochs of no improvement before reducing\n",
    "        verbose=True,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting and save computation\"\"\"\n",
    "    def __init__(self, patience=5, verbose=True, delta=0.0001, path='best_model.pth'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last improvement\n",
    "            verbose (bool): If True, prints a message for each improvement\n",
    "            delta (float): Minimum change to qualify as an improvement\n",
    "            path (str): Path to save the checkpoint\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "            \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Save model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, loss_type, optimizer=None, scheduler=None, num_epochs=15, embedding_size=128):\n",
    "    \"\"\"\n",
    "    Train the metric learning model with enhanced monitoring\n",
    "    \"\"\"\n",
    "    # Get the number of training classes (excluding holdout classes)\n",
    "    num_training_classes = len(datasets_dict['class_mapping']['training_class_indices'])\n",
    "    \n",
    "    # Create loss function and miner\n",
    "    loss_func, mining_func = create_loss_and_miner(\n",
    "        loss_type=loss_type, \n",
    "        embedding_size=embedding_size, \n",
    "        num_classes=num_training_classes\n",
    "    )\n",
    "    \n",
    "    # If using ArcFace, we need to create a class map for the training dataset\n",
    "    if loss_type == 'arcface':\n",
    "        # Map original class indices to consecutive integers for ArcFace\n",
    "        class_map = {original: i for i, original in enumerate(datasets_dict['class_mapping']['training_class_indices'])}\n",
    "    \n",
    "    # Create optimizer and scheduler if not provided\n",
    "    if optimizer is None:\n",
    "        # Use default parameters if not provided\n",
    "        optimizer, auto_scheduler = get_optimizer_and_scheduler(model)\n",
    "        scheduler = auto_scheduler if scheduler is None else scheduler\n",
    "    \n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    history = {\n",
    "        'train_loss': [], \n",
    "        'val_loss': [],\n",
    "        'lr': [],\n",
    "        'batch_losses': [],  # Track per-batch losses for more detailed monitoring\n",
    "        'gradient_norms': []  # Track gradient norms to monitor training stability\n",
    "    }\n",
    "    \n",
    "    # Create figures for live updates\n",
    "    if IN_COLAB:  # Only for interactive environments\n",
    "        from IPython.display import clear_output\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            batch_losses = []\n",
    "            grad_norms = []\n",
    "            \n",
    "            # Iterate over data\n",
    "            for inputs, labels in tqdm(dataloaders[phase], desc=phase):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Map labels for ArcFace if needed\n",
    "                if loss_type == 'arcface':\n",
    "                    # Filter out samples from holdout classes\n",
    "                    valid_idx = torch.tensor([i for i, l in enumerate(labels) if l.item() in class_map], device=device)\n",
    "                    if len(valid_idx) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    inputs = inputs[valid_idx]\n",
    "                    arcface_labels = torch.tensor([class_map[l.item()] for l in labels[valid_idx]], device=device)\n",
    "                    labels = arcface_labels\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    embeddings = model(inputs)\n",
    "                    \n",
    "                    # Get indices for mining if using a mining function\n",
    "                    if mining_func is not None:\n",
    "                        hard_pairs = mining_func(embeddings, labels)\n",
    "                        loss = loss_func(embeddings, labels, hard_pairs)\n",
    "                    else:\n",
    "                        loss = loss_func(embeddings, labels)\n",
    "                    \n",
    "                    # Record batch loss\n",
    "                    batch_losses.append(loss.item())\n",
    "                    \n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        \n",
    "                        # Compute gradient norm\n",
    "                        total_norm = 0\n",
    "                        for p in model.parameters():\n",
    "                            if p.grad is not None:\n",
    "                                param_norm = p.grad.data.norm(2)\n",
    "                                total_norm += param_norm.item() ** 2\n",
    "                        total_norm = total_norm ** 0.5\n",
    "                        grad_norms.append(total_norm)\n",
    "                        \n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                history['lr'].append(current_lr)\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['batch_losses'].extend(batch_losses)\n",
    "                history['gradient_norms'].extend(grad_norms)\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f}, LR: {current_lr:.6f}, Grad Norm: {np.mean(grad_norms):.4f}')\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "                \n",
    "                # Update learning rate scheduler based on validation loss\n",
    "                if scheduler is not None and isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(epoch_loss)\n",
    "                \n",
    "                # Check early stopping\n",
    "                early_stopping(epoch_loss, model)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "        \n",
    "        # Update step-based schedulers at the end of each epoch\n",
    "        if scheduler is not None and not isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Check if early stopping was triggered\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Training stopped early due to no improvement in validation loss\")\n",
    "            break\n",
    "        \n",
    "        # Visualize training progress (after each epoch)\n",
    "        if IN_COLAB and (epoch % 1 == 0 or epoch == num_epochs-1):  # Update every epoch or at the end\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Plot loss curves\n",
    "            ax1.clear()\n",
    "            ax1.plot(range(1, epoch+2), history['train_loss'], 'b-', label='Training Loss')\n",
    "            ax1.plot(range(1, epoch+2), history['val_loss'], 'r-', label='Validation Loss')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.set_title(f'Training Progress - {loss_type} Loss')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True)\n",
    "            \n",
    "            # Plot learning rate\n",
    "            ax2.clear()\n",
    "            ax2.plot(range(1, epoch+2), history['lr'], 'g-')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Learning Rate')\n",
    "            ax2.set_title('Learning Rate Schedule')\n",
    "            ax2.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Plot batch losses and gradient norms\n",
    "            fig2, (ax3, ax4) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            # Batch losses\n",
    "            ax3.plot(history['batch_losses'], 'b-', alpha=0.5)\n",
    "            ax3.set_xlabel('Batch')\n",
    "            ax3.set_ylabel('Loss')\n",
    "            ax3.set_title('Batch-level Losses')\n",
    "            ax3.grid(True)\n",
    "            \n",
    "            # Gradient norms\n",
    "            ax4.plot(history['gradient_norms'], 'r-', alpha=0.5)\n",
    "            ax4.set_xlabel('Batch')\n",
    "            ax4.set_ylabel('Gradient Norm')\n",
    "            ax4.set_title('Gradient Norms')\n",
    "            ax4.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Final visualization - save figures\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(history['train_loss'])+1), history['train_loss'], 'b-', label='Training Loss')\n",
    "    plt.plot(range(1, len(history['val_loss'])+1), history['val_loss'], 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training and Validation Loss ({loss_type} loss)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_history.png')\n",
    "    \n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Main Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(run_comprehensive_eval=True):\n",
    "    # Model parameters (defaults)\n",
    "    backbone_name = 'resnet18'  # Options: 'resnet18', 'resnet34', 'resnet50', 'efficientnet_b0', 'mobilenet_v2', 'densenet121'\n",
    "    embedding_size = 128\n",
    "    batch_size = 32  # Adjust based on your GPU\n",
    "    num_workers = 2\n",
    "    \n",
    "    # Training parameters - reduced epochs with early stopping\n",
    "    loss_type = 'triplet'  # Options: 'triplet', 'contrastive', 'arcface'\n",
    "    num_epochs = 15  # Reduced from 20 since we now have early stopping\n",
    "    lr = 1e-4\n",
    "    weight_decay = 1e-5  # Added L2 regularization\n",
    "    \n",
    "    # Load data\n",
    "    train_val_dataset, test_dataset, eval_train_dataset = load_oxford_pets_dataset()\n",
    "    datasets_dict = prepare_datasets(train_val_dataset, test_dataset, eval_train_dataset)\n",
    "    dataloaders = create_dataloaders(datasets_dict, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    # Create model\n",
    "    model = EmbeddingNet(backbone_name=backbone_name, embedding_size=embedding_size)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create optimizer and scheduler with new function\n",
    "    optimizer, scheduler = get_optimizer_and_scheduler(\n",
    "        model, \n",
    "        learning_rate=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Train model with improved training loop (early stopping and better scheduler)\n",
    "    model, history = train_model(\n",
    "        model=model,\n",
    "        dataloaders=dataloaders,\n",
    "        loss_type=loss_type,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=num_epochs,\n",
    "        embedding_size=embedding_size\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(history['train_loss'])+1), history['train_loss'], 'b-', label='Training Loss')\n",
    "    plt.plot(range(1, len(history['val_loss'])+1), history['val_loss'], 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training and Validation Loss ({loss_type} loss)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'embedding_size': embedding_size,\n",
    "        'backbone_name': backbone_name,\n",
    "        'class_mapping': datasets_dict['class_mapping']\n",
    "    }, f'pet_metric_learning_{backbone_name}_{loss_type}.pth')\n",
    "    \n",
    "    if run_comprehensive_eval:\n",
    "        # Run comprehensive evaluation\n",
    "        print(\"\\nRunning comprehensive model evaluation...\")\n",
    "        results_dir = f\"./evaluation_results_{backbone_name}_{loss_type}\"\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        evaluation_results, eval_dir = comprehensive_evaluation(\n",
    "            model=model,\n",
    "            dataloaders=dataloaders,\n",
    "            class_mapping=datasets_dict['class_mapping'],\n",
    "            results_dir=results_dir\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nComprehensive evaluation completed. Results saved to {eval_dir}\")\n",
    "    else:\n",
    "        # Extract embeddings for standard evaluation\n",
    "        print(\"\\nExtracting embeddings for evaluation...\")\n",
    "        train_embeddings, train_labels = extract_embeddings(model, dataloaders['eval_train'])\n",
    "        test_embeddings, test_labels = extract_embeddings(model, dataloaders['test'])\n",
    "        few_shot_train_embeddings, few_shot_train_labels = extract_embeddings(model, dataloaders['few_shot_train'])\n",
    "        few_shot_test_embeddings, few_shot_test_labels = extract_embeddings(model, dataloaders['few_shot_test'])\n",
    "        \n",
    "        # Evaluation tasks\n",
    "        print(\"\\n1. Verification Task:\")\n",
    "        verification_results = evaluate_verification(test_embeddings, test_labels)\n",
    "        \n",
    "        print(f\"\\nVerification Results:\")\n",
    "        print(f\"ROC AUC: {verification_results['roc_auc']:.4f}\")\n",
    "        print(f\"Equal Error Rate (EER): {verification_results['eer']:.4f}\")\n",
    "        \n",
    "        print(\"\\n2. Retrieval Task:\")\n",
    "        retrieval_results = evaluate_retrieval(\n",
    "            query_embeddings=test_embeddings,\n",
    "            query_labels=test_labels,\n",
    "            gallery_embeddings=train_embeddings,\n",
    "            gallery_labels=train_labels,\n",
    "            k_values=[1, 5, 10]\n",
    "        )\n",
    "        \n",
    "        print(\"\\n3. Few-shot Classification:\")\n",
    "        # Run comprehensive few-shot evaluation across multiple settings\n",
    "        few_shot_results = evaluate_multiple_few_shot_settings(\n",
    "            support_embeddings=few_shot_train_embeddings,\n",
    "            support_labels=few_shot_train_labels,\n",
    "            query_embeddings=few_shot_test_embeddings,\n",
    "            query_labels=few_shot_test_labels\n",
    "        )\n",
    "        \n",
    "        # Embedding visualization\n",
    "        print(\"\\n4. Embedding Visualization:\")\n",
    "        test_projection = visualize_embeddings(\n",
    "            embeddings=test_embeddings,\n",
    "            labels=test_labels,\n",
    "            class_mapping=datasets_dict['class_mapping'],\n",
    "            method='tsne',\n",
    "            title='t-SNE Visualization of Test Embeddings'\n",
    "        )\n",
    "        \n",
    "        # Visualize few-shot embeddings\n",
    "        print(\"\\nVisualizing few-shot embeddings:\")\n",
    "        # Combine few-shot train and test embeddings for visualization\n",
    "        all_few_shot_embeddings = torch.cat([few_shot_train_embeddings, few_shot_test_embeddings], dim=0)\n",
    "        all_few_shot_labels = torch.cat([few_shot_train_labels, few_shot_test_labels], dim=0)\n",
    "        \n",
    "        few_shot_projection = visualize_embeddings(\n",
    "            embeddings=all_few_shot_embeddings,\n",
    "            labels=all_few_shot_labels,\n",
    "            class_mapping=datasets_dict['class_mapping'],\n",
    "            method='tsne',\n",
    "            title='t-SNE Visualization of Few-Shot Embeddings'\n",
    "        )\n",
    "        \n",
    "        # Enhanced Grad-CAM Visualization\n",
    "        print(\"\\n5. Enhanced Grad-CAM Visualization:\")\n",
    "        visualize_grad_cam_comparisons(model, dataloaders['test'], datasets_dict['class_mapping'], num_comparisons=2)\n",
    "        \n",
    "        # Original Grad-CAM Visualization\n",
    "        print(\"\\n6. Individual Grad-CAM Visualization:\")\n",
    "        visualize_grad_cam(model, dataloaders['test'], datasets_dict['class_mapping'], num_images=3)\n",
    "    \n",
    "    print(\"\\nEvaluation completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(run_comprehensive_eval=True)  # Set to False for standard evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have implemented a comprehensive metric learning pipeline for pet breed classification using the Oxford-IIIT Pet Dataset. We have:\n",
    "\n",
    "1. Built a custom embedding model with a CNN backbone and projection head that supports multiple architectures\n",
    "2. Implemented various loss functions for metric learning (Triplet, Contrastive, ArcFace)\n",
    "3. Added efficient training techniques including learning rate scheduling and early stopping\n",
    "4. Developed comprehensive evaluation methods for verification, retrieval, and few-shot classification\n",
    "5. Created detailed visualization tools for embedding spaces and feature importance (Grad-CAM)\n",
    "6. Implemented hyperparameter tuning to optimize model performance\n",
    "7. Enhanced the evaluation with advanced metrics and extensive analysis\n",
    "8. Created a user-friendly Streamlit demo for practical use cases\n",
    "\n",
    "The code is modular and can be easily adapted for different settings and experimentation. To run the complete training and evaluation pipeline, simply execute the `main()` function.\n",
    "\n",
    "This project demonstrates the power of metric learning for fine-grained visual categorization and similarity search, with applications extending beyond pet breed classification to many other domains requiring visual similarity comparison."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
