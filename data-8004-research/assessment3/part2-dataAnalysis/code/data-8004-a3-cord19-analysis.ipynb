{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2207830e",
   "metadata": {},
   "source": [
    "# CORD-19 Linguistic Diversity Analysis\n",
    "\n",
    "This notebook conducts a comprehensive analysis of linguistic diversity within the COVID-19 Open Research Dataset (CORD-19). The analysis includes:\n",
    "\n",
    "1. Language identification across the dataset\n",
    "2. Statistical analysis of language distribution patterns\n",
    "3. Content and topic analysis across languages\n",
    "4. Named entity recognition and terminology analysis\n",
    "5. Text complexity assessment\n",
    "\n",
    "The results of this analysis will provide empirical evidence of language-based disparities in access to COVID-19 scientific information and inform strategies for improving cross-lingual information access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f65e5",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "First, let's install and import all necessary libraries for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108eb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive setup for both Colab and local environments\n",
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import os\n",
    "\n",
    "# Function to check if a package is installed\n",
    "def is_package_installed(package_name):\n",
    "    try:\n",
    "        pkg_resources.get_distribution(package_name)\n",
    "        return True\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        return False\n",
    "\n",
    "# Function to install a package with proper error handling\n",
    "def install_package(package, method='pip', quiet=False):\n",
    "    \"\"\"Install a package using the specified method with error handling\"\"\"\n",
    "    result = {'success': False, 'message': ''}\n",
    "    \n",
    "    if is_package_installed(package):\n",
    "        if not quiet:\n",
    "            print(f\"{package} is already installed.\")\n",
    "        result['success'] = True\n",
    "        result['message'] = 'already installed'\n",
    "        return result\n",
    "    \n",
    "    try:\n",
    "        if method == 'pip':\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "            result['success'] = True\n",
    "            if not quiet:\n",
    "                print(f\"Successfully installed {package} via pip.\")\n",
    "        elif method == 'conda' and 'CONDA_PREFIX' in os.environ:\n",
    "            subprocess.check_call(['conda', 'install', '-y', package])\n",
    "            result['success'] = True\n",
    "            if not quiet:\n",
    "                print(f\"Successfully installed {package} via conda.\")\n",
    "        else:\n",
    "            if not quiet:\n",
    "                print(f\"Method {method} not available for installing {package}.\")\n",
    "            result['message'] = f\"Method {method} not available\"\n",
    "    except Exception as e:\n",
    "        if not quiet:\n",
    "            print(f\"Failed to install {package}: {e}\")\n",
    "        result['message'] = str(e)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Determine environment and platform\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "system_platform = platform.system().lower()  # 'windows', 'linux', or 'darwin'\n",
    "python_version = platform.python_version()\n",
    "python_version_tuple = tuple(map(int, python_version.split('.')))\n",
    "\n",
    "print(f\"Environment: {'Google Colab' if is_colab else 'Local'}\")\n",
    "print(f\"Platform: {system_platform}\")\n",
    "print(f\"Python version: {python_version}\")\n",
    "\n",
    "# Core dependencies that are needed for all environments\n",
    "core_dependencies = [\n",
    "    'numpy', 'pandas', 'matplotlib', 'seaborn', 'nltk', 'scikit-learn', 'textstat'\n",
    "]\n",
    "\n",
    "# Install core dependencies\n",
    "print(\"\\nInstalling core dependencies...\")\n",
    "for package in core_dependencies:\n",
    "    install_package(package)\n",
    "\n",
    "# Special handling for language detection tools\n",
    "print(\"\\nInstalling language detection tools...\")\n",
    "\n",
    "# 1. FastText installation\n",
    "if not is_package_installed('fasttext'):\n",
    "    print(\"Installing fastText...\")\n",
    "    try:\n",
    "        # Try pip first\n",
    "        install_result = install_package('fasttext', quiet=True)\n",
    "        if not install_result['success']:\n",
    "            print(\"Standard fastText installation failed, trying from GitHub...\")\n",
    "            !git clone https://github.com/facebookresearch/fastText.git\n",
    "            !cd fastText && pip install -e .\n",
    "    except Exception as e:\n",
    "        print(f\"Error installing fastText: {e}\")\n",
    "        print(\"Please manually install fastText after the notebook starts.\")\n",
    "else:\n",
    "    print(\"fastText is already installed.\")\n",
    "\n",
    "# 2. pycld3 installation with multiple fallbacks\n",
    "if not is_package_installed('pycld3'):\n",
    "    print(\"\\nInstalling pycld3...\")\n",
    "\n",
    "    # Install required dependencies for pycld3\n",
    "    if is_colab or system_platform == 'linux':\n",
    "        print(\"Installing protobuf dependencies for pycld3...\")\n",
    "        !apt-get update && apt-get install -y libprotobuf-dev protobuf-compiler || true\n",
    "    \n",
    "    # Method 1: Direct pip install\n",
    "    try:\n",
    "        install_result = install_package('pycld3', quiet=True)\n",
    "        if install_result['success']:\n",
    "            print(\"Successfully installed pycld3 via pip.\")\n",
    "        else:\n",
    "            # Method 2: Specific wheel based on Python version\n",
    "            py_version_str = f\"cp{python_version_tuple[0]}{python_version_tuple[1]}\"\n",
    "            wheel_urls = {\n",
    "                'cp39': 'https://files.pythonhosted.org/packages/72/75/711b4642fccb0fd496509e9601d51f0ada1b7416da987a3bdcf349970ef2/pycld3-0.22-cp39-cp39-manylinux1_x86_64.whl',\n",
    "                'cp38': 'https://files.pythonhosted.org/packages/cb/67/1482da1794d7a0fb57c234c22e4a33d751f09f228b325826ac3ee718c145/pycld3-0.22-cp38-cp38-manylinux1_x86_64.whl',\n",
    "                'cp310': 'https://files.pythonhosted.org/packages/82/51/b8eb3c3251bd9085c8ec56c3370725dc1f05e9c9d14f7ccbff5d16af1285/pycld3-0.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl'\n",
    "            }\n",
    "            \n",
    "            if py_version_str in wheel_urls:\n",
    "                print(f\"Attempting to install pycld3 from wheel for Python {py_version_str}...\")\n",
    "                !pip install {wheel_urls[py_version_str]}\n",
    "                if is_package_installed('pycld3'):\n",
    "                    print(f\"Successfully installed pycld3 from {py_version_str} wheel.\")\n",
    "                else:\n",
    "                    raise ImportError(\"Wheel installation failed\")\n",
    "            else:\n",
    "                # Method 3: Try installing from source\n",
    "                print(\"No pre-built wheel available for your Python version, trying source install...\")\n",
    "                !git clone https://github.com/bsolomon1124/pycld3.git\n",
    "                !cd pycld3 && pip install .\n",
    "    except Exception as e:\n",
    "        print(f\"Error installing pycld3: {e}\")\n",
    "        print(\"Will create a fallback implementation for pycld3\")\n",
    "else:\n",
    "    print(\"pycld3 is already installed.\")\n",
    "\n",
    "# 3. langdetect installation\n",
    "if not is_package_installed('langdetect'):\n",
    "    print(\"\\nInstalling langdetect...\")\n",
    "    try:\n",
    "        # Try pip first\n",
    "        install_result = install_package('langdetect', quiet=True)\n",
    "        if not install_result['success']:\n",
    "            print(\"Standard langdetect installation failed, trying from GitHub...\")\n",
    "            !git clone https://github.com/Mimino666/langdetect.git\n",
    "            !cd langdetect && pip install -e .\n",
    "    except Exception as e:\n",
    "        print(f\"Error installing langdetect: {e}\")\n",
    "        print(\"Please manually install langdetect after the notebook starts.\")\n",
    "else:\n",
    "    print(\"langdetect is already installed.\")\n",
    "\n",
    "# 4. spaCy installation and model download\n",
    "if not is_package_installed('spacy'):\n",
    "    print(\"\\nInstalling spaCy...\")\n",
    "    install_package('spacy')\n",
    "    # Download a smaller model by default to save space and time\n",
    "    !python -m spacy download en_core_web_sm\n",
    "else:\n",
    "    print(\"spaCy is already installed.\")\n",
    "\n",
    "# Import dependencies and create fallbacks where needed\n",
    "print(\"\\nConfiguring language detection tools...\")\n",
    "\n",
    "# Create fallback for pycld3 if it couldn't be installed\n",
    "try:\n",
    "    import pycld3\n",
    "    has_pycld3 = True\n",
    "    print(\"Successfully imported pycld3.\")\n",
    "except ImportError:\n",
    "    has_pycld3 = False\n",
    "    print(\"Creating fallback implementation for pycld3...\")\n",
    "    # Create a simple fallback that returns a fixed result\n",
    "    class PyCLD3Module:\n",
    "        def get_language(self, text):\n",
    "            print(\"Using fallback pycld3 implementation. Results may be less accurate.\")\n",
    "            # Try to use langdetect as fallback if available\n",
    "            try:\n",
    "                from langdetect import detect\n",
    "                lang = detect(text)\n",
    "                return (lang, 0.8, True)  # Confidence fixed at 0.8\n",
    "            except:\n",
    "                return ('un', 0.0, True)  # Unknown language with zero confidence\n",
    "    \n",
    "    # Replace the actual module with our fallback\n",
    "    pycld3 = PyCLD3Module()\n",
    "\n",
    "# Configure langdetect\n",
    "has_detector_factory = False\n",
    "try:\n",
    "    # Approach 1: Direct import\n",
    "    from langdetect import DetectorFactory\n",
    "    has_detector_factory = True\n",
    "    print(\"Successfully imported DetectorFactory.\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Approach 2: Import from language detection module\n",
    "        from langdetect.detector_factory import DetectorFactory\n",
    "        has_detector_factory = True\n",
    "        print(\"Successfully imported DetectorFactory from detector_factory module.\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            # Approach 3: Try to import functions individually\n",
    "            from langdetect import detect, detect_langs\n",
    "            # Create a simplified DetectorFactory class\n",
    "            class DetectorFactoryFallback:\n",
    "                seed = 0  # Default seed\n",
    "                @classmethod\n",
    "                def set_seed(cls, seed):\n",
    "                    cls.seed = seed\n",
    "            DetectorFactory = DetectorFactoryFallback\n",
    "            has_detector_factory = True\n",
    "            print(\"Created DetectorFactory fallback.\")\n",
    "        except ImportError:\n",
    "            print(\"Warning: Could not import DetectorFactory or langdetect functions.\")\n",
    "            print(\"Random seed won't be fixed for language detection.\")\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"\\nDownloading required NLTK resources...\")\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Import all the required libraries\n",
    "print(\"\\nImporting required libraries...\")\n",
    "# Standard libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# NLP tools\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import langdetect\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import textstat\n",
    "\n",
    "# Optional imports that can be skipped if not available\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "    has_transformers = True\n",
    "except ImportError:\n",
    "    has_transformers = False\n",
    "    print(\"Note: transformers library not available. Some advanced features may be disabled.\")\n",
    "\n",
    "try:\n",
    "    from gensim import corpora, models\n",
    "    has_gensim = True\n",
    "except ImportError:\n",
    "    has_gensim = False\n",
    "    print(\"Note: gensim library not available. Some topic modeling features may be limited.\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# Set langdetect seed if DetectorFactory is available\n",
    "if has_detector_factory:\n",
    "    DetectorFactory.seed = 42\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"\\nSetup complete! You're ready to run the analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01dee17",
   "metadata": {},
   "source": [
    "## Data Acquisition and Preprocessing\n",
    "\n",
    "Let's start by downloading and loading a sample of the CORD-19 dataset. For this analysis, we'll work with a subset to make processing more manageable in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bca001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download the CORD-19 dataset (metadata only for initial analysis)\n",
    "def download_cord19_data():\n",
    "    # Check if the metadata file already exists\n",
    "    if not os.path.exists('metadata.csv'):\n",
    "        print(\"Downloading CORD-19 metadata file...\")\n",
    "        !wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/latest/metadata.csv\n",
    "    else:\n",
    "        print(\"CORD-19 metadata file already exists.\")\n",
    "    \n",
    "    # For full-text analysis, we'll use a smaller subset\n",
    "    # Create a samples directory if it doesn't exist\n",
    "    if not os.path.exists('samples'):\n",
    "        os.makedirs('samples')\n",
    "        \n",
    "    # Create directory structure for the documents\n",
    "    os.makedirs('samples/document_parses/pdf_json', exist_ok=True)\n",
    "    os.makedirs('samples/document_parses/pmc_json', exist_ok=True)\n",
    "    \n",
    "    # Check if we already have documents extracted\n",
    "    pdf_files = len(os.listdir('samples/document_parses/pdf_json')) > 0\n",
    "    pmc_files = len(os.listdir('samples/document_parses/pmc_json')) > 0\n",
    "    \n",
    "    # Download a subset of full text documents for detailed analysis\n",
    "    if not (pdf_files or pmc_files):\n",
    "        print(\"Downloading sample of CORD-19 full text documents...\")\n",
    "        !wget -O samples/sample_documents.tar.gz https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/latest/document_parses.tar.gz\n",
    "        \n",
    "        # Extract a subset of the documents using wildcards but without the problematic --count option\n",
    "        print(\"Extracting sample documents (this may take a while)...\")\n",
    "        !tar -xzf samples/sample_documents.tar.gz -C samples/document_parses --strip-components=1 --wildcards \"*/pdf_json/PMC00*.json\" \"*/pmc_json/PMC00*.json\"\n",
    "        \n",
    "        print(f\"Extracted {len(os.listdir('samples/document_parses/pdf_json'))} PDF documents and {len(os.listdir('samples/document_parses/pmc_json'))} PMC documents\")\n",
    "    else:\n",
    "        print(\"CORD-19 sample documents already exist.\")\n",
    "\n",
    "# Execute the download function\n",
    "download_cord19_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff0c0c",
   "metadata": {},
   "source": [
    "Now, let's load and preprocess the metadata and full-text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c0362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "def load_metadata():\n",
    "    metadata_df = pd.read_csv('metadata.csv')\n",
    "    print(f\"Loaded metadata with {len(metadata_df)} records\")\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    # Convert date to datetime\n",
    "    metadata_df['publish_time'] = pd.to_datetime(metadata_df['publish_time'], errors='coerce')\n",
    "    \n",
    "    # Filter for COVID-19 era papers (2020 onwards)\n",
    "    covid_era_df = metadata_df[metadata_df['publish_time'] >= '2020-01-01'].copy()\n",
    "    \n",
    "    # Create a sample for analysis (adjust based on your computational resources)\n",
    "    # We'll use stratified sampling to ensure temporal representation\n",
    "    covid_era_df['year_month'] = covid_era_df['publish_time'].dt.to_period('M')\n",
    "    \n",
    "    # Take a stratified sample\n",
    "    sample_size = min(50000, len(covid_era_df))\n",
    "    sample_df = covid_era_df.groupby('year_month', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), int(sample_size/len(covid_era_df.year_month.unique()))), random_state=42)\n",
    "    )\n",
    "    \n",
    "    return sample_df\n",
    "\n",
    "# Load full text documents\n",
    "def load_fulltext_samples():\n",
    "    documents = []\n",
    "    \n",
    "    # Path to extracted documents\n",
    "    doc_path = 'samples/document_parses'\n",
    "    \n",
    "    # Check both pdf_json and pmc_json directories\n",
    "    for dir_name in ['pdf_json', 'pmc_json']:\n",
    "        full_path = os.path.join(doc_path, dir_name)\n",
    "        if os.path.exists(full_path):\n",
    "            for filename in os.listdir(full_path):\n",
    "                if filename.endswith('.json'):\n",
    "                    try:\n",
    "                        with open(os.path.join(full_path, filename), 'r') as f:\n",
    "                            doc = json.load(f)\n",
    "                            documents.append(doc)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(documents)} full text documents\")\n",
    "    return documents\n",
    "\n",
    "# Load the data\n",
    "metadata_sample = load_metadata()\n",
    "fulltext_documents = load_fulltext_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe3748",
   "metadata": {},
   "source": [
    "Let's explore the basic characteristics of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bece747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about our metadata sample\n",
    "print(\"Metadata Sample Statistics:\")\n",
    "print(f\"Number of papers: {len(metadata_sample)}\")\n",
    "print(f\"Date range: {metadata_sample['publish_time'].min()} to {metadata_sample['publish_time'].max()}\")\n",
    "print(f\"Sources: {metadata_sample['source_x'].value_counts().to_dict()}\")\n",
    "print(f\"Languages (from metadata): {metadata_sample['has_pdf_parse'].value_counts()}\")\n",
    "\n",
    "# Extract text from full-text documents\n",
    "def extract_text_from_doc(doc):\n",
    "    \"\"\"Extract title, abstract, and body text from a document\"\"\"\n",
    "    title = doc.get('metadata', {}).get('title', '')\n",
    "    abstract = ' '.join([para.get('text', '') for para in doc.get('abstract', [])])\n",
    "    \n",
    "    # Extract body text paragraphs\n",
    "    body_text = []\n",
    "    for paragraph in doc.get('body_text', []):\n",
    "        body_text.append(paragraph.get('text', ''))\n",
    "    \n",
    "    body = ' '.join(body_text)\n",
    "    \n",
    "    return {\n",
    "        'paper_id': doc.get('paper_id', ''),\n",
    "        'title': title,\n",
    "        'abstract': abstract,\n",
    "        'body_text': body,\n",
    "        'full_text': f\"{title} {abstract} {body}\"\n",
    "    }\n",
    "\n",
    "# Process the documents\n",
    "processed_docs = [extract_text_from_doc(doc) for doc in fulltext_documents]\n",
    "fulltext_df = pd.DataFrame(processed_docs)\n",
    "\n",
    "# Display basic statistics about the full text documents\n",
    "print(\"\\nFull Text Document Statistics:\")\n",
    "print(f\"Number of documents: {len(fulltext_df)}\")\n",
    "print(f\"Documents with abstracts: {fulltext_df['abstract'].str.len().gt(0).sum()}\")\n",
    "print(f\"Documents with body text: {fulltext_df['body_text'].str.len().gt(0).sum()}\")\n",
    "print(f\"Average title length: {fulltext_df['title'].str.len().mean():.1f} characters\")\n",
    "print(f\"Average abstract length: {fulltext_df['abstract'].str.len().mean():.1f} characters\")\n",
    "print(f\"Average body text length: {fulltext_df['body_text'].str.len().mean():.1f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2511da",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Language Identification\n",
    "\n",
    "Let's implement multiple language identification methods to ensure accurate detection of both high and low-resource languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba04ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the FastText language identification model\n",
    "# Method 1: Using fasttext.util to download model if available\n",
    "try:\n",
    "    # Try to download the English model as indicator that fasttext.util is working\n",
    "    import fasttext.util\n",
    "    # Check if language identification model exists, otherwise download it\n",
    "    if not os.path.exists('lid.176.bin'):\n",
    "        print(\"Downloading FastText language identification model...\")\n",
    "        !wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
    "except ImportError:\n",
    "    # If fasttext.util is not available, download directly with wget\n",
    "    print(\"fasttext.util not available, downloading model directly...\")\n",
    "    if not os.path.exists('lid.176.bin'):\n",
    "        !wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
    "\n",
    "# Load the fastText model\n",
    "try:\n",
    "    fasttext_model = fasttext.load_model('lid.176.bin')\n",
    "    print(\"FastText language model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading FastText model: {e}\")\n",
    "    print(\"Installing FastText from GitHub...\")\n",
    "    !git clone https://github.com/facebookresearch/fastText.git\n",
    "    !cd fastText && pip install -e .\n",
    "    import fasttext\n",
    "    fasttext_model = fasttext.load_model('lid.176.bin')\n",
    "    print(\"FastText language model loaded after installation\")\n",
    "\n",
    "# Function to identify language using multiple methods\n",
    "def identify_language(text, min_length=50):\n",
    "    \"\"\"\n",
    "    Identify the language of a text using multiple methods\n",
    "    Returns a dictionary with results from each method and a consensus result\n",
    "    \"\"\"\n",
    "    if not text or len(text) < min_length:\n",
    "        return {'consensus': 'unknown', 'confidence': 0}\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Use try-except blocks for each method to handle potential errors\n",
    "    \n",
    "    # FastText\n",
    "    try:\n",
    "        fasttext_pred = fasttext_model.predict(text.replace('\\n', ' '))\n",
    "        ft_lang = fasttext_pred[0][0].replace('__label__', '')\n",
    "        ft_conf = float(fasttext_pred[1][0])\n",
    "        results['fasttext'] = {'lang': ft_lang, 'confidence': ft_conf}\n",
    "    except Exception as e:\n",
    "        results['fasttext'] = {'lang': 'unknown', 'confidence': 0, 'error': str(e)}\n",
    "    \n",
    "    # langdetect - modified to handle different versions\n",
    "    try:\n",
    "        # First try the newer API\n",
    "        ld_pred = langdetect.detect_langs(text)\n",
    "        ld_lang = ld_pred[0].lang\n",
    "        ld_conf = ld_pred[0].prob\n",
    "        results['langdetect'] = {'lang': ld_lang, 'confidence': ld_conf}\n",
    "    except (AttributeError, TypeError):\n",
    "        try:\n",
    "            # Fall back to simpler detect function\n",
    "            ld_lang = langdetect.detect(text)\n",
    "            results['langdetect'] = {'lang': ld_lang, 'confidence': 0.5}  # Using fixed confidence when prob not available\n",
    "        except Exception as e:\n",
    "            results['langdetect'] = {'lang': 'unknown', 'confidence': 0, 'error': str(e)}\n",
    "    except Exception as e:\n",
    "        results['langdetect'] = {'lang': 'unknown', 'confidence': 0, 'error': str(e)}\n",
    "    \n",
    "    # CLD3\n",
    "    try:\n",
    "        cld3_pred = pycld3.get_language(text)\n",
    "        cld3_lang = cld3_pred[0]\n",
    "        cld3_conf = cld3_pred[1]\n",
    "        results['cld3'] = {'lang': cld3_lang, 'confidence': cld3_conf}\n",
    "    except Exception as e:\n",
    "        results['cld3'] = {'lang': 'unknown', 'confidence': 0, 'error': str(e)}\n",
    "    \n",
    "    # Determine consensus\n",
    "    # If at least 2 methods agree, use that language\n",
    "    languages = [results[m]['lang'] for m in results if 'error' not in results[m]]\n",
    "    if not languages:\n",
    "        consensus = 'unknown'\n",
    "        confidence = 0\n",
    "    else:\n",
    "        language_counts = Counter(languages)\n",
    "        consensus = language_counts.most_common(1)[0][0]\n",
    "        \n",
    "        # Calculate average confidence for the consensus language\n",
    "        confidence_sum = sum(results[m]['confidence'] for m in results \n",
    "                           if 'error' not in results[m] and results[m]['lang'] == consensus)\n",
    "        confidence = confidence_sum / sum(1 for m in results \n",
    "                                      if 'error' not in results[m] and results[m]['lang'] == consensus)\n",
    "    \n",
    "    results['consensus'] = consensus\n",
    "    results['confidence'] = confidence\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the language identification function\n",
    "sample_texts = {\n",
    "    \"English\": \"The COVID-19 pandemic has affected millions of people worldwide.\",\n",
    "    \"Spanish\": \"La pandemia de COVID-19 ha afectado a millones de personas en todo el mundo.\",\n",
    "    \"French\": \"La pandémie de COVID-19 a touché des millions de personnes dans le monde.\",\n",
    "    \"German\": \"Die COVID-19-Pandemie hat weltweit Millionen von Menschen betroffen.\",\n",
    "    \"Chinese\": \"COVID-19大流行已影响了全球数百万人。\",\n",
    "    \"Russian\": \"Пандемия COVID-19 затронула миллионы людей во всем мире.\",\n",
    "    \"Arabic\": \"لقد أثرت جائحة COVID-19 على ملايين الأشخاص في جميع أنحاء العالم.\",\n",
    "    \"Swahili\": \"Janga la COVID-19 limeathiri mamilioni ya watu duniani kote.\"\n",
    "}\n",
    "\n",
    "# Test the function on sample texts\n",
    "for lang, text in sample_texts.items():\n",
    "    result = identify_language(text)\n",
    "    print(f\"Expected: {lang}, Detected: {result['consensus']} (Confidence: {result['confidence']:.2f})\")\n",
    "    for method, res in result.items():\n",
    "        if method not in ['consensus', 'confidence']:\n",
    "            print(f\"  {method}: {res.get('lang', 'N/A')} (Confidence: {res.get('confidence', 0):.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6480540c",
   "metadata": {},
   "source": [
    "Now, let's identify languages for our document samples and analyze the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32501210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply language identification to our full text documents\n",
    "def identify_document_languages(df, text_column='full_text'):\n",
    "    \"\"\"Identify languages for a dataframe of documents\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    total_docs = len(df)\n",
    "    for i, (idx, row) in enumerate(df.iterrows()):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing document {i+1}/{total_docs}...\")\n",
    "        \n",
    "        text = row[text_column]\n",
    "        if pd.isna(text) or len(text) < 100:  # Skip very short texts\n",
    "            lang_result = {'consensus': 'unknown', 'confidence': 0}\n",
    "        else:\n",
    "            # For longer texts, use the first 2000 characters for faster processing\n",
    "            # This is a reasonable compromise for language detection\n",
    "            lang_result = identify_language(text[:2000])\n",
    "        \n",
    "        results.append({\n",
    "            'paper_id': row.get('paper_id', idx),\n",
    "            'language': lang_result['consensus'],\n",
    "            'confidence': lang_result['confidence']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Apply language identification to our documents\n",
    "language_results = identify_document_languages(fulltext_df)\n",
    "\n",
    "# Merge results with our full text dataframe\n",
    "fulltext_df = fulltext_df.merge(language_results, on='paper_id', how='left')\n",
    "\n",
    "# Display language distribution\n",
    "language_distribution = fulltext_df['language'].value_counts(normalize=True) * 100\n",
    "print(\"Language Distribution (%):\")\n",
    "print(language_distribution.head(10))\n",
    "\n",
    "# Create a visualization of language distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "language_distribution.head(10).plot(kind='bar', color='steelblue')\n",
    "plt.title('Top 10 Languages in CORD-19 Sample (%)', fontsize=16)\n",
    "plt.xlabel('Language', fontsize=14)\n",
    "plt.ylabel('Percentage of Documents', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.tight_layout()\n",
    "plt.savefig('language_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7e221d",
   "metadata": {},
   "source": [
    "## 2. Statistical Analysis of Language Distribution Patterns\n",
    "\n",
    "Now, let's conduct a more detailed statistical analysis of language distribution, including temporal patterns and correlations with other document characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge language information with metadata\n",
    "# First, create a mapping from paper_id to language\n",
    "language_mapping = fulltext_df[['paper_id', 'language', 'confidence']].set_index('paper_id').to_dict(orient='index')\n",
    "\n",
    "# Function to apply language detection to metadata sample\n",
    "def add_language_to_metadata(metadata_df, language_mapping):\n",
    "    \"\"\"\n",
    "    Add language information to metadata dataframe\n",
    "    For papers without full text, perform language identification on the abstract\n",
    "    \"\"\"\n",
    "    # Initialize language columns\n",
    "    metadata_df['language'] = 'unknown'\n",
    "    metadata_df['language_confidence'] = 0.0\n",
    "    \n",
    "    # Update language for papers with full text\n",
    "    for idx, row in metadata_df.iterrows():\n",
    "        paper_id = row['cord_uid']\n",
    "        if paper_id in language_mapping:\n",
    "            metadata_df.at[idx, 'language'] = language_mapping[paper_id]['language']\n",
    "            metadata_df.at[idx, 'language_confidence'] = language_mapping[paper_id]['confidence']\n",
    "        elif not pd.isna(row['abstract']) and len(row['abstract']) > 100:\n",
    "            # For papers without full text, identify language from abstract\n",
    "            lang_result = identify_language(row['abstract'])\n",
    "            metadata_df.at[idx, 'language'] = lang_result['consensus']\n",
    "            metadata_df.at[idx, 'language_confidence'] = lang_result['confidence']\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "# Apply language detection to the metadata sample\n",
    "# This would be time-consuming for the full dataset, so we'll use a smaller subset\n",
    "metadata_subset = metadata_sample.sample(1000, random_state=42)\n",
    "metadata_with_lang = add_language_to_metadata(metadata_subset, language_mapping)\n",
    "\n",
    "# Analyze temporal patterns in language distribution\n",
    "metadata_with_lang['year_month'] = metadata_with_lang['publish_time'].dt.to_period('M')\n",
    "\n",
    "# Calculate percentage of non-English documents over time\n",
    "def analyze_temporal_patterns(df):\n",
    "    \"\"\"Analyze how language distribution changes over time\"\"\"\n",
    "    # Group by year_month and calculate the percentage of non-English documents\n",
    "    temporal_lang = df.groupby('year_month').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'total_docs': len(x),\n",
    "            'english_docs': sum(x['language'] == 'en'),\n",
    "            'non_english_docs': sum(x['language'] != 'en'),\n",
    "            'non_english_percent': sum(x['language'] != 'en') / len(x) * 100\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Sort by time\n",
    "    temporal_lang = temporal_lang.sort_values('year_month')\n",
    "    \n",
    "    return temporal_lang\n",
    "\n",
    "temporal_patterns = analyze_temporal_patterns(metadata_with_lang)\n",
    "\n",
    "# Visualize temporal patterns\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(temporal_patterns['year_month'].astype(str), \n",
    "         temporal_patterns['non_english_percent'], \n",
    "         marker='o', linestyle='-', linewidth=2, markersize=8)\n",
    "plt.title('Percentage of Non-English Documents Over Time', fontsize=16)\n",
    "plt.xlabel('Year-Month', fontsize=14)\n",
    "plt.ylabel('Percentage of Non-English Documents', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.tight_layout()\n",
    "plt.savefig('temporal_language_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47fbfd3",
   "metadata": {},
   "source": [
    "Let's analyze the relationship between language and other document characteristics such as source and citation count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2943c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language distribution by source\n",
    "def analyze_language_by_source(df):\n",
    "    \"\"\"Analyze language distribution across different sources\"\"\"\n",
    "    # Group by source and calculate language percentages\n",
    "    lang_by_source = df.groupby('source_x')['language'].value_counts(normalize=True).unstack().fillna(0) * 100\n",
    "    \n",
    "    # Sort sources by percentage of English documents\n",
    "    if 'en' in lang_by_source.columns:\n",
    "        lang_by_source = lang_by_source.sort_values('en', ascending=False)\n",
    "    \n",
    "    return lang_by_source\n",
    "\n",
    "lang_by_source = analyze_language_by_source(metadata_with_lang)\n",
    "\n",
    "# Visualize language distribution by source\n",
    "plt.figure(figsize=(14, 10))\n",
    "lang_by_source.iloc[:10].plot(kind='bar', stacked=True, colormap='viridis')\n",
    "plt.title('Language Distribution by Source (Top 10 Sources)', fontsize=16)\n",
    "plt.xlabel('Source', fontsize=14)\n",
    "plt.ylabel('Percentage', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Language', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.tight_layout()\n",
    "plt.savefig('language_by_source.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e6a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical test: Chi-square test to determine if language distribution differs significantly from global speaker populations\n",
    "def chi_square_test_language_distribution(observed_counts):\n",
    "    \"\"\"\n",
    "    Perform chi-square test comparing observed language distribution\n",
    "    with expected distribution based on global speaker populations\n",
    "    \"\"\"\n",
    "    from scipy.stats import chi2_contingency\n",
    "    \n",
    "    # Approximate global speaker percentages for major languages\n",
    "    # Sources: Ethnologue, various linguistic population studies\n",
    "    global_speakers = {\n",
    "        'en': 17.0,  # English\n",
    "        'zh': 14.1,  # Chinese\n",
    "        'es': 6.9,   # Spanish\n",
    "        'hi': 6.0,   # Hindi\n",
    "        'ar': 4.5,   # Arabic\n",
    "        'bn': 3.7,   # Bengali\n",
    "        'pt': 3.1,   # Portuguese\n",
    "        'ru': 2.7,   # Russian\n",
    "        'ja': 1.8,   # Japanese\n",
    "        'fr': 1.6,   # French\n",
    "        'de': 1.3,   # German\n",
    "        'other': 37.3  # All other languages\n",
    "    }\n",
    "    \n",
    "    # Prepare observed counts\n",
    "    observed = []\n",
    "    for lang in global_speakers:\n",
    "        if lang == 'other':\n",
    "            # Sum counts for all languages not specifically listed\n",
    "            count = sum(observed_counts.get(l, 0) for l in observed_counts \n",
    "                        if l not in global_speakers or l == 'other')\n",
    "        else:\n",
    "            count = observed_counts.get(lang, 0)\n",
    "        observed.append(count)\n",
    "    \n",
    "    # Calculate expected counts based on global speaker percentages\n",
    "    total_count = sum(observed)\n",
    "    expected = [total_count * (global_speakers[lang]/100) for lang in global_speakers]\n",
    "    \n",
    "    # Perform chi-square test\n",
    "    chi2, p, dof, expected = chi2_contingency([observed, expected])\n",
    "    \n",
    "    return {\n",
    "        'chi2': chi2,\n",
    "        'p_value': p,\n",
    "        'degrees_of_freedom': dof,\n",
    "        'observed': observed,\n",
    "        'expected': expected\n",
    "    }\n",
    "\n",
    "# Get observed language counts\n",
    "observed_lang_counts = fulltext_df['language'].value_counts().to_dict()\n",
    "\n",
    "# Perform chi-square test\n",
    "chi_square_results = chi_square_test_language_distribution(observed_lang_counts)\n",
    "\n",
    "print(\"Chi-Square Test Results:\")\n",
    "print(f\"Chi-square statistic: {chi_square_results['chi2']:.2f}\")\n",
    "print(f\"p-value: {chi_square_results['p_value']:.10f}\")\n",
    "print(f\"Degrees of freedom: {chi_square_results['degrees_of_freedom']}\")\n",
    "print(\"\\nConclusion: The language distribution in the CORD-19 dataset differs \" +\n",
    "      f\"{'significantly' if chi_square_results['p_value'] < 0.05 else 'not significantly'} \" +\n",
    "      \"from the global language speaker distribution (p < 0.05).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acdb822",
   "metadata": {},
   "source": [
    "## 3. Content and Topic Analysis Across Languages\n",
    "\n",
    "Now, let's analyze the content of documents across different languages to understand topical differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a34f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and preprocess text for topic modeling\n",
    "def preprocess_text(text, language='en'):\n",
    "    \"\"\"Clean and preprocess text for topic modeling\"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords for English (for other languages, we'll keep all words)\n",
    "    if language == 'en':\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    else:\n",
    "        # For non-English, just remove very short words\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess documents\n",
    "fulltext_df['processed_text'] = fulltext_df.apply(\n",
    "    lambda row: preprocess_text(row['full_text'], row['language']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac3d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform LDA topic modeling for English documents\n",
    "def perform_topic_modeling(df, language='en', num_topics=20):\n",
    "    \"\"\"Perform LDA topic modeling on documents of a specific language\"\"\"\n",
    "    # Filter documents by language\n",
    "    lang_docs = df[df['language'] == language]\n",
    "    \n",
    "    if len(lang_docs) < 50:\n",
    "        print(f\"Not enough documents ({len(lang_docs)}) for topic modeling in language: {language}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Create a document-term matrix\n",
    "    vectorizer = TfidfVectorizer(max_features=10000, min_df=5, max_df=0.8)\n",
    "    dtm = vectorizer.fit_transform(lang_docs['processed_text'])\n",
    "    \n",
    "    # Train LDA model\n",
    "    lda_model = LatentDirichletAllocation(\n",
    "        n_components=num_topics, \n",
    "        random_state=42,\n",
    "        learning_method='online',\n",
    "        max_iter=25\n",
    "    )\n",
    "    lda_output = lda_model.fit_transform(dtm)\n",
    "    \n",
    "    # Get feature names (terms)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Create a dictionary of topics\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_words_idx = topic.argsort()[:-11:-1]  # Get indices of top 10 words\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topic_dict[f\"Topic {topic_idx+1}\"] = top_words\n",
    "    \n",
    "    return lda_model, vectorizer, topic_dict\n",
    "\n",
    "# Run topic modeling for English documents\n",
    "english_lda_model, english_vectorizer, english_topics = perform_topic_modeling(fulltext_df, 'en')\n",
    "\n",
    "# Display English topics\n",
    "print(\"Top Topics in English Documents:\")\n",
    "for topic, words in english_topics.items():\n",
    "    print(f\"{topic}: {', '.join(words)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
